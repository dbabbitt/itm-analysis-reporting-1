{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up notebook\n",
    "%pprint\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os.path as osp, os as os\n",
    "\n",
    "executable_path = sys.executable\n",
    "scripts_folder = osp.join(osp.dirname(executable_path), 'Scripts'); assert osp.exists(scripts_folder)\n",
    "py_folder = osp.abspath(osp.join(os.pardir, 'py')); assert osp.exists(py_folder), \"Create the py folder\"\n",
    "ffmpeg_folder = r'C:\\ffmpeg\\bin'; assert osp.exists(ffmpeg_folder)\n",
    "shared_folder = osp.abspath(osp.join(os.pardir, 'share')); assert osp.exists(shared_folder)\n",
    "\n",
    "if (scripts_folder not in sys.path): sys.path.insert(1, scripts_folder)\n",
    "if (py_folder not in sys.path): sys.path.insert(1, py_folder)\n",
    "if (ffmpeg_folder not in sys.path): sys.path.insert(1, ffmpeg_folder)\n",
    "if shared_folder not in sys.path: sys.path.insert(1, shared_folder)\n",
    "\n",
    "from notebook_utils import NotebookUtilities\n",
    "nu = NotebookUtilities(\n",
    "    data_folder_path=osp.abspath(osp.join(os.pardir, 'data')),\n",
    "    saves_folder_path=osp.abspath(osp.join(os.pardir, 'saves'))\n",
    ")\n",
    "nu.delete_ipynb_checkpoint_folders()\n",
    "\n",
    "# Import needed libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\daveb\\\\OneDrive\\\\Documents\\\\GitHub'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "osp.abspath(osp.join(os.pardir, os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open C:\\Users\\DaveBabbitt\\Documents\\GitHub\\itm-analysis-reporting\\ipynb\\Installs.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nu.open_path_in_notepad('../ipynb/Installs.ipynb', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize all the notebook utilities in the GitHub folder\n",
    "from itertools import combinations\n",
    "import subprocess\n",
    "\n",
    "paths_list = []\n",
    "black_list = ['.ipynb_checkpoints', '$Recycle.Bin', '.git', 'llama_index']\n",
    "for sub_directory, directories_list, files_list in walk(osp.abspath('../../')):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "        for file_name in files_list:\n",
    "            if file_name == 'notebook_utils.py':\n",
    "                file_path = osp.join(sub_directory, file_name)\n",
    "                paths_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all unique pairs (order doesn't matter)\n",
    "pairs = combinations(paths_list, 2)\n",
    "\n",
    "# Iterate through pairs and compare them\n",
    "comparator_path = \"/mnt/c/Program Files (x86)/Compare It!/wincmp3.exe\"\n",
    "for pair in pairs:\n",
    "    left_path = pair[0].replace('/mnt/c/', 'C:\\\\').replace('/', '\\\\')\n",
    "    right_path = pair[1].replace('/mnt/c/', 'C:\\\\').replace('/', '\\\\')\n",
    "    subprocess.run([\n",
    "        comparator_path, left_path, right_path\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for 20240522truncated_anova_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/20240522truncated_anova_df.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    **{'20240522truncated_anova_df': ''}\n",
    ")\n",
    "anova_df = data_frames_dict['20240522truncated_anova_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu.get_AD_KDMA_Sim\n",
      "fu.get_AD_KDMA_Text\n",
      "fu.get_PropTrust\n",
      "fu.get_ST_KDMA_Sim\n",
      "fu.get_ST_KDMA_Text\n",
      "The procedure to calculate the percentage of correct tagging within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create the tag-to-SALT data frame.\n",
      "    iii. Create the correct-count-by-tag data frame.\n",
      "    iv. Get the percentage tag correct counts for the scene.\n",
      "The procedure to calculate the number of unique patient ids in a scene is as follows:\n",
      "    i. Count the number of unique patient IDs.\n",
      "    ii. Return the calculated patient count.\n",
      "The procedure to calculate the percentage of correct injury treatment within a scene is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Get the count of all the patient injuries for the scene.\n",
      "    iii. Get the count of all correctly treated injuries for the scene.\n",
      "    iv. Compute the percentage of correctly treated injuries for the scene.\n",
      "    v. If you get a division-by-zero error, just leave it as NaN.\n",
      "The procedure to count the number of 'pulse_taken' actions in the given scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a boolean mask to filter 'PULSE_TAKEN' actions.\n",
      "    iii. Use the mask to filter the DataFrame and count the number of 'PULSE_TAKEN' actions.\n",
      "    iv. Return the count of 'PULSE_TAKEN' actions.\n",
      "The procedure to 0=all stills not visited first, 1=all stills visited first is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Extract the actual and ideal sequences of first interactions from the scene in terms of still/waver/walker.\n",
      "    iii. Truncate both sequences to the head at the stills length and compare them; they both should have all stills.\n",
      "    iv. If they are, output a 1 (All Stills visited first), if not, output a 0 (All Stills not visited first).\n",
      "The procedure to count the number of 'teleport' actions in the given scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a boolean mask to filter TELEPORT action types.\n",
      "    iii. Count the number of actions.\n",
      "The procedure to calculate the time to hemorrhage control per patient for the scene is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Iterate through patients in the scene.\n",
      "    iii. Check if the patient is hemorrhaging (defined as the injury record requires the hemorrhage control procedures) and not dead.\n",
      "    iv. Count the patient and add its hemorrhage control time to a list for averaging.\n",
      "    v. Calculate the hemorrhage control per patient by summing the control times and dividing by the patient count.\n",
      "The procedure to calculate the time (between first-in-sequence tool_hover and last-in-sequence tool_selected) that responders take to select a tool after hovering over them is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Identify scene_df indices based on TOOL_SELECTED actions.\n",
      "    iii. Split the DataFrame at TOOL_SELECTED indices.\n",
      "    iv. Calculate indecision times for each sub-dataframe.\n",
      "    v. Append time difference between TOOL_SELECTED action and first TOOL_HOVER action to the list.\n",
      "    vi. Calculate the mean indecision time.\n",
      "The procedure to calculate the triage time for a scene is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Get the scene start and end times.\n",
      "    iii. Calculate the triage time.\n",
      "The procedure to calculate the total count of the participant's player actions of strictly the pulse_taken, tool_applied, and tag_applied action types performed within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a mask to filter for specific action types.\n",
      "    iii. Calculate scene action count based on the mask.\n",
      "The procedure to calculate the total count of the participant's patient_engaged and pulse_taken action types performed within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a mask to filter for specific action types.\n",
      "    iii. Calculate assessment count based on the mask.\n",
      "The procedure to calculate the total count of the participant's \"indecision metric\" of tag_discarded and tool_discarded action types performed within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a mask to filter for specific discarded types.\n",
      "    iii. Calculate discarded count based on the mask.\n",
      "The procedure to calculate the total count of the participant's tag_applied action types performed within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a mask to filter for specific action types.\n",
      "    iii. Calculate tag_application count based on the mask.\n",
      "The procedure to calculate the total count of the participant's injury_treated action types performed within a scene dataframe is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Create a mask to filter for specific action types.\n",
      "    iii. Calculate treatment count based on the mask.\n",
      "The procedure to calculate the total number of instances where a participant treated a patient expected to die within a scene is as follows:\n",
      "    i. Ensure all needed columns are present in scene_df.\n",
      "    ii. Loop through each patient to count the instances where a participant treated a patient expected to die.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "\n",
    "for cn in anova_df.columns:\n",
    "    if any(map(lambda x: cn.startswith(x), ['mean_', 'sum_'])):\n",
    "        function_call = 'fu.get_' + '_'.join(cn.split('_')[1:])\n",
    "        try:\n",
    "            nu.describe_procedure(eval(function_call))\n",
    "        except Exception as e:\n",
    "            print(function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ipynb\n",
      "Installs.ipynb\n",
      "\n",
      "open_world\n",
      "OW Attic.ipynb\n",
      "OW Scratchpad.ipynb\n",
      "\n",
      "presentations\n",
      "Presentation Graveyard.ipynb\n",
      "\n",
      "tests\n",
      "Tests Scratchpad.ipynb\n",
      "\n",
      "visualizations\n",
      "Visualization Scratchpad.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all notebooks that are missing their FRVRS import additions\n",
    "import pyperclip\n",
    "\n",
    "functions_list = [\n",
    "    'CategoricalDtype', 'DataFrame', 'Index', 'NaT', 'Series', 'concat', 'csv', 'display', 'isna', 'isnan', 'listdir', 'makedirs',\n",
    "    'math', 'nan', 'notnull', 'np', 'osp', 'pickle', 're', 'read_csv', 'read_excel', 'read_pickle', 'remove', 'sep', 'sm', 'subprocess',\n",
    "    'sys', 'to_datetime', 'to_numeric', 'walk', 'warnings'\n",
    "]\n",
    "compile_str = r'\\b(' + '|'.join(functions_list) + r')\\b'\n",
    "pyperclip.copy(compile_str)\n",
    "functions_regex = re.compile(compile_str)\n",
    "import_regex = re.compile(r'\"from FRVRS import \\(')\n",
    "nu.delete_ipynb_checkpoint_folders()\n",
    "for sub_directory, directories_list, files_list in walk(nu.github_folder):\n",
    "    todo_list = []\n",
    "    for file_name in files_list:\n",
    "        if file_name.endswith('.ipynb'):\n",
    "            file_path = osp.join(sub_directory, file_name)\n",
    "            with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                text = f.read()\n",
    "                if (functions_regex.search(text) is not None) and (import_regex.search(text) is None):\n",
    "                    todo_list.append(file_name)\n",
    "    if todo_list:\n",
    "        print()\n",
    "        print(sub_directory.split('/')[-1])\n",
    "        print('\\n'.join(todo_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__name__', '_financial_names', 'asfortranarray', 'binary_repr', 'concatenate', 'diagonal', 'fill_diagonal', 'format_float_positional', 'isnan', 'isnat', 'linalg', 'nan', 'nan_to_num', 'nanargmax', 'nanargmin', 'nancumprod', 'nancumsum', 'nanmax', 'nanmean', 'nanmedian', 'nanmin', 'nanpercentile', 'nanprod', 'nanquantile', 'nanstd', 'nansum', 'nanvar', 'typename']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[fn for fn in dir(np) if 'na' in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/Anaconda3-2021.11-Linux-x86_64.sh C:\\Users\\DaveBabbitt\\Downloads\\Anaconda3-2021.11-Linux-x86_64.sh\n",
      "scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/cvat.notes C:\\Users\\DaveBabbitt\\Downloads\\cvat.notes\n",
      "scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/daveb.csv C:\\Users\\DaveBabbitt\\Downloads\\daveb.csv\n",
      "scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/fiftyone.yml C:\\Users\\DaveBabbitt\\Downloads\\fiftyone.yml\n",
      "scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/setup_env.txt C:\\Users\\DaveBabbitt\\Downloads\\setup_env.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print()\n",
    "for file_name in [\n",
    "    'Anaconda3-2021.11-Linux-x86_64.sh', 'cvat.notes', 'daveb.csv', 'fiftyone.yml', 'setup_env.txt'\n",
    "]:\n",
    "    print(f'scp dbabbitt@209.137.198.29:/datadrive/Research/hackathon/{file_name} C:\\\\Users\\\\DaveBabbitt\\\\Downloads\\\\{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "from datetime import timedelta\n",
    "import humanize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['os.DirEntry', 'os.O_DIRECT', 'os.O_DIRECTORY', 'os.ST_NODIRATIME', 'os.chdir', 'os.curdir', 'os.fchdir', 'os.listdir', 'os.makedirs', 'os.mkdir', 'os.pardir', 'os.removedirs', 'os.rmdir', 'os.scandir', 'os.supports_dir_fd']\n",
      "['osp.curdir', 'osp.dirname', 'osp.isdir', 'osp.pardir']\n",
      "['nu.github_folder']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print([f'os.{fn}' for fn in dir(os) if 'dir' in fn.lower()])\n",
    "print([f'osp.{fn}' for fn in dir(osp) if 'dir' in fn.lower()])\n",
    "print([f'nu.{fn}' for fn in dir(nu) if 'github' in fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['os.__name__', 'os.confstr_names', 'os.name', 'os.pathconf_names', 'os.rename', 'os.renames', 'os.sysconf_names', 'os.ttyname', 'os.uname', 'os.uname_result']\n",
      "['osp.__name__', 'osp.basename', 'osp.dirname', 'osp.supports_unicode_filenames']\n",
      "['nu.get_filename_from_url']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print([f'os.{fn}' for fn in dir(os) if 'name' in fn.lower()])\n",
    "print([f'osp.{fn}' for fn in dir(osp) if 'name' in fn.lower()])\n",
    "print([f'nu.{fn}' for fn in dir(nu) if 'name' in fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/data/xlsx/Items_Logits_in_Winsteps_Format.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spreadsheets_folder = osp.join(nu.data_folder, 'xlsx')\n",
    "for file_name in listdir(spreadsheets_folder):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = osp.join(spreadsheets_folder, file_name)\n",
    "        df = read_excel(file_path)\n",
    "        if 'scene_id' in df.columns: print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print out a section of the .gitignore\n",
    "black_list = ['.ipynb_checkpoints', '$Recycle.Bin', '.git']\n",
    "print()\n",
    "print('# exclude everything except personal directories')\n",
    "print('/*')\n",
    "print('!.gitignore')\n",
    "for sub_directory in listdir(nu.github_folder):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)) and osp.isdir(osp.join(nu.github_folder, sub_directory)):\n",
    "        print(f'!/{sub_directory}')\n",
    "print('!README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'Intel64 Family 6 Model 60 Stepping 3, GenuineIntel'\n",
    "import platform\n",
    "\n",
    "platform.processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to GitHub Desktop executable\n",
    "github_desktop_path = \"/mnt/c/Users/DaveBabbitt/AppData/Local/GitHubDesktop/GitHubDesktop.exe\"\n",
    "\n",
    "for path_str in paths_list:\n",
    "    \n",
    "    # Ensure the repository path is Windows-compliant\n",
    "    repo_path = osp.abspath(osp.dirname(osp.dirname(path_str))).replace('/mnt/c/', 'C:\\\\').replace('/', '\\\\')\n",
    "    \n",
    "    # Construct the command\n",
    "    command = [github_desktop_path, '--add-repository', repo_path]\n",
    "    # print(command)\n",
    "    \n",
    "    # Launch GitHub Desktop with the specified repository\n",
    "    subprocess.run(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

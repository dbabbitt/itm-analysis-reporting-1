{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d88d0-ee28-42bf-b2df-28ef5f83333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the notebook\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb7d17-3e6e-412c-b3f7-160342870e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import (\n",
    "    fu, nu, warnings, DataFrame, osp, listdir, Index, concat, read_excel, re, isna, to_datetime, display, NaT, read_csv, csv, to_numeric\n",
    ")\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f191acf-470f-4517-8d25-cb634011b94f",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Stats Created for Metrics Evaluation Open World\n",
    "\n",
    "Dave you should be ignoring all the files except the zip folder I sent you.\n",
    "In the zip I sent you, there are 51 folders, (51 JSON, 51 CSV).\n",
    "Zip file attached.\n",
    "All the files are named appropriated in the folder/CSV/json UUID_ParticipantID.\n",
    "Some of the internal Participants IDs might be off because the moderator forgot to enter a Participant ID or didn't enter the Participant ID correctly so we needed to figure out which participant it was.\n",
    "Please only utilize the UUID and Participant ID that is on the file name to identify and ignore the internal Participant IDs.\n",
    "Maybe that will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c41bbd-d759-42c4-839f-69f6bd450de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(metrics_evaluation_open_world_csv_stats_df='', first_responder_master_registry_file_stats_df='')\n",
    "csv_stats_df = data_frames_dict['metrics_evaluation_open_world_csv_stats_df']\n",
    "print(csv_stats_df.shape) # (171766, 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897950c-8f66-4d2d-be15-b66d14b72775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the Open World JSON Stats into one data frame\n",
    "json_stats_df = DataFrame([])\n",
    "logs_path = osp.join(fu.data_folder, 'logs', 'Human_Sim_Metrics_Data_4-12-2024')\n",
    "directories_list = listdir(logs_path)\n",
    "for dir_name in directories_list:\n",
    "    \n",
    "    # Add the JSONs to the data frame\n",
    "    folder_path = osp.join(logs_path, dir_name)\n",
    "    \n",
    "    # Iterate over the files in the current subdirectory\n",
    "    for file_name in listdir(folder_path):\n",
    "        \n",
    "        # If the file is a JSON file\n",
    "        if file_name.endswith('.json'):\n",
    "            \n",
    "            # Create a data frame from the flattened dictionary\n",
    "            json_path = osp.join(folder_path, file_name)\n",
    "            with open(json_path, 'r') as f: file_json = json.load(f)\n",
    "            row_dict = {\n",
    "                'json_file_subpath': folder_path,\n",
    "                'json_file_name': file_name\n",
    "            }\n",
    "            flattened_json_dict = nu.get_flattened_dictionary(file_json, row_dict=row_dict, key_prefix='')\n",
    "            \n",
    "            # You've got to clean the Session IDs\n",
    "            session_uuid, participant_id = dir_name.split('_')\n",
    "            flattened_json_dict['session_uuid'] = session_uuid\n",
    "            flattened_json_dict['participant_id'] = int(participant_id)\n",
    "            df = DataFrame(flattened_json_dict, index=Index([0]))\n",
    "            \n",
    "            # Append the data frame for the current subdirectory to the main data frame and break the participant ID loop\n",
    "            json_stats_df = concat([json_stats_df, df], axis='index')\n",
    "\n",
    "json_stats_df = json_stats_df.reset_index(drop=True)\n",
    "nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "print(json_stats_df.participant_id.nunique()) # 22\n",
    "print(json_stats_df.shape) # (43, 3525)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf960-9020-49be-a431-2fec62e65523",
   "metadata": {},
   "source": [
    "\n",
    "## Check for proper ingestion (duplicate file ingestion, et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a49e32-3992-433e-8680-654c174c7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check you even ingested anything\n",
    "assert len(json_stats_df.columns) > 4, \"Nothing ingested\"\n",
    "assert json_stats_df.participant_id.nunique() == 26, f\"Participant count should be 26, it's {json_stats_df.participant_id.nunique()} instead\"\n",
    "print(json_stats_df.shape) # (43, 3525)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a77265-2d8c-400f-be3d-3030a7841b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data frame\n",
    "data_frames_dict = nu.load_data_frames(metrics_evaluation_open_world_json_stats_df='')\n",
    "json_stats_df = data_frames_dict['metrics_evaluation_open_world_json_stats_df']\n",
    "\n",
    "if 'logger_version' not in json_stats_df.columns:\n",
    "    \n",
    "    # Get the columns that consistently have only one value in them per session\n",
    "    single_value_cols_set = set(csv_stats_df.columns)\n",
    "    for session_uuid, session_df in csv_stats_df.groupby('session_uuid'):\n",
    "        single_value_cols = set([col for col in session_df.columns if session_df[col].nunique() == 1])\n",
    "        single_value_cols_set = single_value_cols_set.intersection(single_value_cols)\n",
    "    print(single_value_cols_set)\n",
    "    rows_list = []\n",
    "    for session_uuid, session_df in csv_stats_df[list(single_value_cols_set)].dropna(axis='columns', how='all').groupby('session_uuid'):\n",
    "        row_dict = {cn: session_df[cn].dropna().max() for cn in session_df.columns}\n",
    "        rows_list.append(row_dict)\n",
    "    single_value_cols_df = DataFrame(rows_list)\n",
    "    print(single_value_cols_df.shape)\n",
    "    \n",
    "    on_columns = sorted(set(json_stats_df.columns).intersection(set(single_value_cols_df.columns)))\n",
    "    print(on_columns)\n",
    "    json_stats_df = json_stats_df.merge(\n",
    "        single_value_cols_df, how='left', on=on_columns\n",
    "    )\n",
    "    print(json_stats_df.shape)\n",
    "    print(single_value_cols_df.shape)\n",
    "    \n",
    "    # Save so you don't have to run it again\n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    \n",
    "    print(json_stats_df.shape) # (43, 3538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89128bdd-c10c-4f8b-92f9-949fc25e916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get column and value descriptions\n",
    "file_path = osp.join(fu.data_folder, 'xlsx', 'Metrics_Evaluation_Dataset_organization_for_BBAI.xlsx')\n",
    "dataset_organization_df = read_excel(file_path)\n",
    "\n",
    "# Fix the doubled up descriptions\n",
    "mask_series = dataset_organization_df.Labels.map(lambda x: ';' in str(x))\n",
    "for row_index, label in dataset_organization_df[mask_series].Labels.items():\n",
    "    labels_list = re.split(' *; *', str(label), 0)\n",
    "    dataset_organization_df.loc[row_index, 'Labels'] = labels_list[0]\n",
    "    \n",
    "    # Get a copy of the row\n",
    "    new_row = dataset_organization_df.loc[row_index].copy()\n",
    "    \n",
    "    # Modify the desired column value\n",
    "    new_row['Labels'] = labels_list[1]\n",
    "    \n",
    "    print(\"Append the new row to the DataFrame\")\n",
    "    dataset_organization_df = concat([dataset_organization_df, new_row], ignore_index=True)\n",
    "\n",
    "# Get a copy of the row\n",
    "mask_series = (dataset_organization_df.Variable == 'AD_Del_Omni')\n",
    "new_row = dataset_organization_df.loc[mask_series].copy()\n",
    "\n",
    "# Modify the desired column value\n",
    "new_row['Variable'] = 'AD_Del_Omni_Text'\n",
    "\n",
    "# Append the new row to the Data Frame\n",
    "dataset_organization_df = concat([dataset_organization_df, new_row], ignore_index=True)\n",
    "\n",
    "# Get the column value descriptions\n",
    "mask_series = ~dataset_organization_df.Description.isnull()\n",
    "df = dataset_organization_df[mask_series]\n",
    "value_description_dict = df.set_index('Variable').Description.to_dict()\n",
    "new_description_dict = value_description_dict.copy()\n",
    "for k, v in value_description_dict.items():\n",
    "    new_description_dict[k] = v\n",
    "    if (not k.endswith('_Text')):\n",
    "        new_key_name = f'{k}_Text'\n",
    "        new_description_dict[new_key_name] = new_description_dict.get(new_key_name, v)\n",
    "value_description_dict = new_description_dict.copy()\n",
    "\n",
    "# Create the value description function\n",
    "numeric_categories_mask_series = dataset_organization_df.Labels.map(lambda x: '=' in str(x))\n",
    "value_descriptions_columns = dataset_organization_df[numeric_categories_mask_series].Variable.unique().tolist()\n",
    "def get_value_description(column_name, column_value):\n",
    "    value_description = ''\n",
    "    if not isna(column_value):\n",
    "        mask_series = (dataset_organization_df.Variable == column_name) & ~dataset_organization_df.Labels.isnull()\n",
    "        if mask_series.any():\n",
    "            df = dataset_organization_df[mask_series]\n",
    "            mask_series = df.Labels.map(lambda label: re.split(' *= *', str(label), 0)[0] == str(int(float(column_value))))\n",
    "            if mask_series.any():\n",
    "                label = df[mask_series].Labels.squeeze()\n",
    "                value_description = re.split(' *= *', str(label), 0)[1]\n",
    "    \n",
    "    return value_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700fe83-21ea-44a3-9001-4aba4abb6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data frame\n",
    "data_frames_dict = nu.load_data_frames(metrics_evaluation_open_world_json_stats_df='')\n",
    "json_stats_df = data_frames_dict['metrics_evaluation_open_world_json_stats_df']\n",
    "\n",
    "if 'MedExp' not in json_stats_df.columns:\n",
    "    file_path = osp.join(fu.data_folder, 'xlsx', 'participant_data_0420.xlsx')\n",
    "    participant_data_df = read_excel(file_path).rename(columns={'ParticipantID': 'participant_id', 'Date': 'participation_date'})\n",
    "    participant_data_df.participation_date = to_datetime(participant_data_df.participation_date, format='%m/%d/%Y')\n",
    "    \n",
    "    # See if you can merge on participant_id\n",
    "    on_columns = sorted(set(json_stats_df.columns).intersection(set(participant_data_df.columns)))\n",
    "    print(on_columns)\n",
    "    if on_columns:\n",
    "        print(json_stats_df.shape)\n",
    "        json_stats_df = json_stats_df.merge(\n",
    "            participant_data_df, how='left', on=on_columns\n",
    "        )\n",
    "        print(participant_data_df.shape)\n",
    "        # print(sorted(set(participant_data_df.groupby('participant_id').mean().columns).intersection(set(\n",
    "        #     dataset_organization_df[~numeric_categories_mask_series].Variable\n",
    "        # ))))\n",
    "        print(json_stats_df.shape)\n",
    "    \n",
    "    # Check if the various partipant id columns are inconsistent\n",
    "    columns_list = [cn for cn in json_stats_df.columns if cn.lower().startswith('participant') and cn.lower().endswith('id')]\n",
    "    df = json_stats_df[columns_list]\n",
    "    for cn in columns_list:\n",
    "        df[cn] = df[cn].map(lambda x: str(x).strip())\n",
    "    mask_series = (df[columns_list[0]] != df[columns_list[1]])\n",
    "    if mask_series.any(): print(\"The various partipant id columns are inconsistent\")\n",
    "    \n",
    "    # Save so you don't have to run it again\n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21cd1f-f21a-47e6-bdde-71c7e85d003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove any duplicate session IDs\n",
    "mask_series = json_stats_df.duplicated(subset='session_uuid')\n",
    "if mask_series.any():\n",
    "    display(json_stats_df[mask_series].dropna(axis='columns', how='all')); raise\n",
    "    json_stats_df = json_stats_df[~mask_series]\n",
    "    \n",
    "    # Store the results and show the new data frame shape\n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    print(json_stats_df.shape) # (60, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e0e76-7e4b-4937-af10-7ba60cde9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix the null file dates\n",
    "if 'session_file_date' not in json_stats_df.columns:\n",
    "    json_stats_df['session_file_date'] = NaT\n",
    "mask_series = json_stats_df.session_file_date.isnull()\n",
    "if mask_series.any():\n",
    "    print(f'I have {mask_series.sum()} sessions in my JSON Stats data frame without file dates.')\n",
    "    for session_uuid, idx_df in json_stats_df[mask_series].groupby('session_uuid'):\n",
    "        \n",
    "        # Get the whole session history\n",
    "        mask_series = (csv_stats_df.session_uuid == session_uuid)\n",
    "        if mask_series.any():\n",
    "            session_df = csv_stats_df[mask_series]\n",
    "            \n",
    "            mask_series = ~session_df.event_time.isnull()\n",
    "            assert mask_series.any(), f\"Session ID {session_uuid} doesn't have an event times in csv_stats_df\"\n",
    "            session_file_date = session_df[mask_series].event_time.min().date()\n",
    "            json_stats_df.loc[idx_df.index, 'session_file_date'] = session_file_date\n",
    "        else: print(f\"You're missing a Session ID {session_uuid} in csv_stats_df\")\n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    mask_series = json_stats_df.session_file_date.isnull()\n",
    "    if mask_series.any():\n",
    "        print(f'I still have {mask_series.sum()} sessioms in my JSON Stats data frame without file dates.')\n",
    "        display(json_stats_df[mask_series].dropna(axis='columns', how='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d847b-f084-4e86-b377-ece008013507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add file start time\n",
    "if ('file_start_time' not in json_stats_df.columns) or ('file_stop_time' not in json_stats_df.columns):\n",
    "    if ('json_file_name' in json_stats_df.columns) and ('json_file_subpath' in json_stats_df.columns):\n",
    "        for (file_name, folder_path), idx_df in json_stats_df.groupby(['json_file_name', 'json_file_subpath']):\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = osp.join(folder_path, file_name)\n",
    "            \n",
    "            # Attempt to read CSV file using pandas; if unsuccessful, try using a reader\n",
    "            try: file_df = read_csv(file_path, header=None, index_col=False)\n",
    "            except:\n",
    "                rows_list = []\n",
    "                with open(file_path, 'r') as f:\n",
    "                    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "                    for values_list in reader:\n",
    "                        if (values_list[-1] == ''): values_list.pop(-1)\n",
    "                        rows_list.append({i: v for i, v in enumerate(values_list)})\n",
    "                file_df = DataFrame(rows_list)\n",
    "            \n",
    "            ts_series = to_datetime(file_df[2], infer_datetime_format=True, errors='coerce')\n",
    "            if ts_series.dropna().shape[0]:\n",
    "                json_stats_df.loc[idx_df.index, 'file_start_time'] = ts_series.dropna().min().to_pydatetime()\n",
    "                json_stats_df.loc[idx_df.index, 'file_stop_time'] = ts_series.dropna().max().to_pydatetime()\n",
    "        print(json_stats_df.shape) # (43, 3538)\n",
    "        nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "        nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b506d5-b30c-4c91-8783-089353e3c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a sample with a clear count of responders\n",
    "new_column_name = 'is_a_one_triage_file'\n",
    "# if (new_column_name in json_stats_df.columns):\n",
    "    # json_stats_df = json_stats_df.drop(columns=new_column_name)\n",
    "if (new_column_name not in json_stats_df.columns):\n",
    "    json_stats_df[new_column_name] = False\n",
    "    data_frames_dict = nu.load_data_frames(metrics_evaluation_open_world_scene_stats_df='')\n",
    "    scene_stats_df = data_frames_dict['metrics_evaluation_open_world_scene_stats_df']\n",
    "    if scene_stats_df is not None:\n",
    "        \n",
    "        # Assume a 1:1 correspondence between file name and UUID from the logs data frame build\n",
    "        for session_uuid, session_df in json_stats_df.groupby('session_uuid'):\n",
    "            assert session_df.shape[0] == 1, \"You've got duplicate session UUIDs\"\n",
    "            \n",
    "            # Filter in the triage files in this UUID\n",
    "            mask_series = (scene_stats_df.session_uuid == session_uuid) & (scene_stats_df.scene_type == 'Triage')\n",
    "            \n",
    "            # Get whether the file has only one triage run\n",
    "            triage_scene_count = len(scene_stats_df[mask_series].groupby('scene_id').groups)\n",
    "            is_a_one_triage_file = bool(triage_scene_count == 1)\n",
    "            \n",
    "            json_stats_df.loc[session_df.index, new_column_name] = is_a_one_triage_file\n",
    "        \n",
    "        # Store the results and show the new data frame shape\n",
    "        nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "        nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "        print(json_stats_df.shape) # (43, 3539)\n",
    "\n",
    "display(json_stats_df.groupby(new_column_name).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda8907-bbae-493d-888a-2c84e8d0f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_column_name = 'is_in_registry'\n",
    "if (new_column_name not in json_stats_df.columns):\n",
    "    json_stats_df[new_column_name] = False\n",
    "    \n",
    "    # Store the results and show the new data frame shape\n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "    print(json_stats_df.shape) # (43, 3540)\n",
    "\n",
    "display(json_stats_df.groupby(new_column_name).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a661464-fce6-4501-86e9-47dea41ae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the participant ID columns to numeric\n",
    "columns_list = [cn for cn in json_stats_df.columns if 'partici' in cn.lower()]\n",
    "for cn in columns_list: json_stats_df[cn] = to_numeric(json_stats_df[cn], errors='coerce')\n",
    "print(json_stats_df.shape) # (43, 3570)\n",
    "nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df)\n",
    "nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f517fe-1527-4a3a-86be-ad764256af14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

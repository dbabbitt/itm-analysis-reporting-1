{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1285cd-7472-4840-907c-e3b0b06ce052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733d4183-f287-4716-b4cc-13d9faa86caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up notebook\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460a2dd2-087a-46df-a2c1-610bf75ace38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "from FRVRS import nu, fu\n",
    "from numpy import nan\n",
    "from pandas import DataFrame, read_csv, read_excel, concat, get_dummies, isna\n",
    "from re import split, search, sub, MULTILINE\n",
    "from scipy.stats import f_oneway, ttest_ind, kruskal, norm\n",
    "import itertools\n",
    "import os.path as osp\n",
    "import re\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94af009-79f4-4dbd-9482-89bb6b6705b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for metrics_evaluation_open_world_csv_stats_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/metrics_evaluation_open_world_csv_stats_df.csv.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    verbose=True, metrics_evaluation_open_world_csv_stats_df=''\n",
    ")\n",
    "csv_stats_df = data_frames_dict['metrics_evaluation_open_world_csv_stats_df'].copy()\n",
    "print(sorted([cn for cn in csv_stats_df.columns if 'all' in cn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fab5ff-fd5d-4a11-80af-52221c9c1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Fearch for teleports and get the euclidean distance to the nearest patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "677a2a7c-6143-4cf2-a62b-ab19f41cdff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02d472ac-f6fe-474c-815d-6125fadfcbf7_2024211.csv;04f80090-9e61-431d-8473-dccb75fed04d_2024207.csv;05e0be07-479a-4ea4-9b90-3fc7b79ef7ff_2024225.csv;0bc62b93-ac4d-40ef-bdfe-1d7badb24f70_2024208.csv;0e8693a1-1926-4d56-b0c4-ee2dc8181fc6_2024226.csv;1995e7ef-ef02-4fc1-b1ab-f137dbf69d48_2024217.csv;21f8cb5d-f5ac-4a01-9287-43df5f6751a1_2024219.csv;220b609b-0e35-454e-9afd-c84cbfa3e3ad_2024202.csv;23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201.csv;287389c4-4c48-4483-87c0-6b363b57bde2_2024220.csv;37a554ee-fc49-4730-819c-2d97727bb0b7_2024218.csv;385032e9-9801-4dcf-a841-b3703a0d9acd_2024209.csv;3cf14e31-f416-4c78-8a69-91bf0c685448_2024215.csv;45365e18-6e38-48e7-b4a2-6b448b209034_2024218.csv;499179ba-3138-4bae-918e-ffc7fb943760_2024206.csv;4bc46c8c-66e7-463d-b3a1-2a8303af4fd1_2024203.csv;50b15e40-9860-4574-8ab8-0bd960fe27de_2024220.csv;5a909dc4-0f86-430d-a400-e37139cba69f_2024212.csv;5d8d73a3-1898-4f64-8676-73edd1b7daa0_2024211.csv;67dc0230-511d-41ac-ae9b-850900ab9e6a_2024208.csv;6db9446c-2cd4-41b4-be8d-be5ccbbc6e05_2024217.csv;703b9d0b-7786-4a4b-8a26-7c57d16b785a_2024226.csv;80f79d45-22fd-479d-b6e2-c62b5778e073_2024204.csv;84c92e21-764b-435b-9e55-0f12f60383fc_2024227.csv;8839e3b8-be5e-4878-8aaf-26c656ae2270_2024207.csv;8b6c4aed-0983-4a76-99fb-cec08dc26b92_2024224.csv;922ad146-241a-4ea6-8ff1-413d7e0d16ec_2024202.csv;a3a7727b-dd25-4e78-8d63-e66573f5077e_2024224.csv;a7ce6f7b-6466-4281-9496-92b640d9d04b_2024215.csv;a9c21d35-e8be-47fd-926a-a00dcd3e792b_2024225.csv;acf74a81-a534-44c7-9cb1-67ec381b5ee0_2024203.csv;aecfcd56-2262-40a8-9bb8-088f57d46f3f_2024206.csv;b5989edc-8348-4b84-b649-87fc4f1cca53_2024209.csv;bccb0095-5efd-4c5c-ad58-8b8624f9ab56_2024201.csv;c6d3a90f-68c0-4948-bd96-537e80973605_2024216.csv;c99de80f-15cc-45cb-aa64-5af0f2f118ca_2024222.csv;c9a8dc60-f61d-44bb-bcb5-6e2466f3c9a0_2024223.csv;cbbf410f-4657-428e-9616-8a777cc4704d_2024204.csv;d13091cc-98e4-4aba-8d02-7eca8bd1a30c_2024216.csv;d2aaf0ef-a32f-4255-b3f5-56df927ae0b4_2024214.csv;df2fcf88-874b-4cf9-9707-3fa0b30c348f_2024205.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mask_series = (csv_stats_df.injury_required_procedure == 'none')\n",
    "';'.join(csv_stats_df[mask_series].csv_file_name.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176dc954-2568-4f19-b9e2-f6d298dc25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for metrics_evaluation_open_world_anova_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/metrics_evaluation_open_world_anova_df.csv.\n",
      "['mean_PropTrust']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    verbose=True, metrics_evaluation_open_world_anova_df=''\n",
    ")\n",
    "anova_df = data_frames_dict['metrics_evaluation_open_world_anova_df'].copy()\n",
    "print(sorted([cn for cn in anova_df.columns if 'trust' in cn.lower()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024f8e14-4238-4f05-b977-a2a8797b7586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.666666667, 4.333333333, 4.0, 3.333333333, 2.333333333, 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "anova_df.mean_PropTrust.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4703671f-d5fa-4ed6-a487-aa8be006cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entitle_column_name(column_name):\n",
    "    \"\"\"\n",
    "    Entitles a column name based on specified rules.\n",
    "    \n",
    "    Parameters:\n",
    "        column_name (str):\n",
    "            The name of the column to be entitled.\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "            The entitled column name.\n",
    "    \n",
    "    Notes:\n",
    "        - If the column name starts with 'mean_' and the substring after 'mean_' exists in the value_description_dict,\n",
    "          the entitled name is obtained from the corresponding value in the dictionary, prepended with 'Average ' if necessary.\n",
    "        - Otherwise, the column name is split into parts, and each part is processed to ensure proper capitalization and word replacements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the column name starts with 'mean_' and is present in the value_description_dict\n",
    "    if column_name.startswith('mean_') and (column_name[5:] in value_description_dict):\n",
    "        entitled_name = value_description_dict[column_name[5:]]\n",
    "        \n",
    "        # Prepend 'Average ' if the entitled name does not already start with it\n",
    "        if not entitled_name.startswith('Average '):\n",
    "            entitled_name = 'Average ' + entitled_name\n",
    "    else:\n",
    "        new_parts_list = []\n",
    "        old_parts_list = [op for op in split('_', column_name, 0) if op]  # Split the column name by underscores\n",
    "        for name_part in old_parts_list:\n",
    "            \n",
    "            # Check if the part contains a capital letter followed by lowercase letters (camelCase)\n",
    "            if search('[A-Z][a-z]+', name_part):\n",
    "                humps_list = [hp for hp in split('([A-Z][a-z]+)', name_part, 0) if hp]  # Split camelCase parts\n",
    "                for i, hump_part in enumerate(humps_list):\n",
    "                    \n",
    "                    # Capitalize each part if it is all lowercase\n",
    "                    if hump_part == hump_part.lower():\n",
    "                        humps_list[i] = hump_part.title()\n",
    "                    \n",
    "                    # Replace specific abbreviations with full words\n",
    "                    elif hump_part == 'Sim':\n",
    "                        humps_list[i] = 'Simulation'\n",
    "                    elif hump_part == 'Yrs':\n",
    "                        humps_list[i] = 'Years of'\n",
    "                    elif hump_part == 'Mil':\n",
    "                        humps_list[i] = 'Military'\n",
    "                    elif hump_part == 'Exp':\n",
    "                        humps_list[i] = 'Experience'\n",
    "                new_parts_list.extend(humps_list)\n",
    "            else:\n",
    "                \n",
    "                # Process parts that are not in camelCase\n",
    "                if name_part == name_part.lower():\n",
    "                    \n",
    "                    # Capitalize if the part is all lowercase and meets certain conditions\n",
    "                    if (len(name_part) > 2) and (name_part != 'uuid'):\n",
    "                        name_part = name_part.title()\n",
    "                    \n",
    "                    # Convert certain parts to uppercase\n",
    "                    elif name_part not in ['to', 'of', 'per']:\n",
    "                        name_part = name_part.upper()\n",
    "                new_parts_list.append(name_part)\n",
    "        \n",
    "        # Replace 'Mean' with 'Average' if it is the first part\n",
    "        if new_parts_list[0] == 'Mean':\n",
    "            new_parts_list[0] = 'Average'\n",
    "        entitled_name = ' '.join(new_parts_list)\n",
    "    \n",
    "    return entitled_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f7af94-4c22-4d7d-9f1a-e5c0431aa701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get column and value descriptions\n",
      "\n",
      "Fix the doubled up descriptions\n",
      "\n",
      "Get the column value descriptions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nGet column and value descriptions\")\n",
    "file_path = osp.join(nu.data_folder, 'xlsx', 'Metrics_Evaluation_Dataset_organization_for_BBAI.xlsx')\n",
    "dataset_organization_df = read_excel(file_path)\n",
    "\n",
    "print(\"\\nFix the doubled up descriptions\")\n",
    "mask_series = dataset_organization_df.Labels.map(lambda x: ';' in str(x))\n",
    "for row_index, label in dataset_organization_df[mask_series].Labels.items():\n",
    "    labels_list = split(' *; *', str(label), 0)\n",
    "    dataset_organization_df.loc[row_index, 'Labels'] = labels_list[0]\n",
    "    \n",
    "    # Append the new row to the DataFrame\n",
    "    new_row = dataset_organization_df.loc[row_index].copy()\n",
    "    new_row['Labels'] = labels_list[1]\n",
    "    dataset_organization_df = concat([dataset_organization_df, new_row], ignore_index=True)\n",
    "\n",
    "# Append the AD_Del_Omni_Text row to the DataFrame\n",
    "mask_series = (dataset_organization_df.Variable == 'AD_Del_Omni')\n",
    "new_row = dataset_organization_df.loc[mask_series].copy()\n",
    "new_row['Variable'] = 'AD_Del_Omni_Text'\n",
    "dataset_organization_df = concat([dataset_organization_df, new_row], ignore_index=True)\n",
    "\n",
    "print(\"\\nGet the column value descriptions\")\n",
    "mask_series = ~dataset_organization_df.Description.isnull()\n",
    "df = dataset_organization_df[mask_series]\n",
    "value_description_dict = df.set_index('Variable').Description.to_dict()\n",
    "new_description_dict = value_description_dict.copy()\n",
    "for k, v in value_description_dict.items():\n",
    "    new_description_dict[k] = v\n",
    "    if (not k.endswith('_Text')):\n",
    "        new_key_name = f'{k}_Text'\n",
    "        new_description_dict[new_key_name] = new_description_dict.get(new_key_name, v)\n",
    "value_description_dict = new_description_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559d7a6f-14b1-49e0-8a3f-41f961c1f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write up the steps to do the ANOVA stats columns calculations\n",
      "\n",
      "participant_id (Participant ID)\n",
      "Steps Needed to do Calculations:\n",
      "1. The participant_id is found in both the CSV and the JSON data in the \"Human_Sim_Metrics_Data 4-12-2024.zip\" file provided by CACI..\n",
      "\n",
      "scene_id (Scene ID)\n",
      "Steps Needed to do Calculations:\n",
      "1. The scene_id is derived from the CSV SESSION_START and SESSION_END entries..\n",
      "\n",
      "session_uuid (Session UUID)\n",
      "Steps Needed to do Calculations:\n",
      "1. The session_uuid is found in both the CSV and the JSON data in the \"Human_Sim_Metrics_Data 4-12-2024.zip\" file provided by CACI..\n",
      "\n",
      "mean_AD_KDMA_Sim (Average KDMA measurement from simulator probe responses)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the AD_KDMA_Sim column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The AD_KDMA_Sim value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_AD_KDMA_Text (Average KDMA measurement from text probe responses)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the AD_KDMA_Text column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The AD_KDMA_Text value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_PropTrust (Average rating on 3-item propensity to trust measure; higher is higher propensity to trust)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the PropTrust column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The PropTrust value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_ST_KDMA_Sim (Average KDMA measurement from simulator probe responses)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the ST_KDMA_Sim column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The ST_KDMA_Sim value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_ST_KDMA_Text (Average KDMA measurement from text probe responses)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the ST_KDMA_Text column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The ST_KDMA_Text value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_YrsMilExp (Average Years serving in a medical role in the military)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Find the YrsMilExp column in the participant_data_0420 spreadsheet provided by CACI for that participant.\n",
      "3. The YrsMilExp value is semi-continously numeric, and you can average it for whatever grouping you need.\n",
      "\n",
      "mean_actual_engagement_distance (Average Actual Engagement Distance)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter out all the non-locations of the non-engaged.\n",
      "3. Add the Euclidean distances between the successive engagment locations of a chronologically-ordered list.\n",
      "\n",
      "mean_first_engagement (Average First Engagement)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter for actions with the type \"PATIENT_ENGAGED\".\n",
      "3. Get the action tick of the first 'PATIENT_ENGAGED' action.\n",
      "4. Return the action tick of the first 'PATIENT_ENGAGED' action.\n",
      "\n",
      "mean_first_treatment (Average First Treatment)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter for actions with the type \"INJURY_TREATED\".\n",
      "3. Get the action tick of the first 'INJURY_TREATED' action.\n",
      "4. Return the action tick of the first 'INJURY_TREATED' action.\n",
      "\n",
      "mean_injury_correctly_treated_count (Average Injury Correctly Treated Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Loop through each injury ID and make a determination if it's treated or not.\n",
      "3. Return the count of records where injuries were correctly treated.\n",
      "\n",
      "mean_injury_not_treated_count (Average Injury Not Treated Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Loop through each injury and make a determination if it's treated or not.\n",
      "3. Return the count of records where injuries were not treated.\n",
      "\n",
      "mean_injury_wrongly_treated_count (Average Injury Wrongly Treated Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the count of all the patient injuries.\n",
      "3. Get the count of all correctly treated injuries.\n",
      "4. Get the count of all untreated injuries.\n",
      "5. Count the number of patients whose injuries have been incorrectly treated.\n",
      "\n",
      "mean_last_engagement (Average Last Engagement)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the mask for the PATIENT_ENGAGED actions.\n",
      "3. Find the maximum elapsed time among rows satisfying the action mask.\n",
      "4. Return the last engagement time.\n",
      "\n",
      "mean_last_still_engagement (Average Last Still Engagement)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the chronological order of engagement starts for each patient in the scene.\n",
      "3. Filter out only the still patients.\n",
      "4. Get the maximum engagement start from that subset.\n",
      "\n",
      "mean_measure_of_right_ordering (Average Measure of Right Ordering)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Extract the actual and ideal sequences of first interactions from the scene in terms of still/waver/walker.\n",
      "3. Calculate the R-squared adjusted value as a measure of right ordering.\n",
      "\n",
      "mean_patient_count (Average Patient Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Count the number of unique patient IDs.\n",
      "3. Return the calculated patient count.\n",
      "\n",
      "mean_percent_hemorrhage_controlled (Average Percent Hemorrhage Controlled)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Loop through each injury, examining its required procedures and wrong treatments.\n",
      "3. Check if an injury record or treatment exists for a hemorrhage-related procedure.\n",
      "4. Count any injuries requiring hemorrhage control procedures.\n",
      "5. Check if the injury was treated correctly.\n",
      "6. See if there are any tools applied that are associated with the hemorrhage injuries.\n",
      "7. Count any hemorrhage-related injuries that have been treated, and not wrong, and not counted twice.\n",
      "8. Calculate the percentage of controlled hemorrhage-related injuries.\n",
      "9. Return the percentage of hemorrhage cases controlled.\n",
      "\n",
      "mean_prioritize_high_injury_severity_patients (Average Prioritize High Injury Severity Patients)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the chronological order of engagement starts for each patient in the scene.\n",
      "3. Get the cluster ID, if available.\n",
      "4. Get the predicted priority.\n",
      "5. Get the maximum injury severity.\n",
      "6. Check if the responder even interacted with this patient.\n",
      "7. Get the first engagement start that has a location.\n",
      "8. Add engagement information to the list.\n",
      "9. Add engagement information to the list.\n",
      "10. Sort the starts list chronologically.\n",
      "11. Merge the distance delta dataset with the original dataset.\n",
      "12. Break up the metadata columns into their own columns.\n",
      "13. Split the pipe-delimited values into a DataFrame.\n",
      "14. Change the column names to reflect the content.\n",
      "15. Make engagement_start an integer.\n",
      "16. Add the split columns to the original DataFrame.\n",
      "17. Drop the original column and the empty predicted priority column.\n",
      "18. Add the prioritize patients column to the original dataset.\n",
      "19. Initialize variables.\n",
      "20. Loop through columns looking for injury severity columns.\n",
      "21. Check if any injury severity was found.\n",
      "22. Check if engaged patient has the maximum severity.\n",
      "23. Apply the function to each row (axis=1).\n",
      "\n",
      "mean_pulse_taken_count (Average Pulse Taken Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Create a boolean mask to filter 'PULSE_TAKEN' actions.\n",
      "3. Use the mask to filter the DataFrame and count the number of 'PULSE_TAKEN' actions.\n",
      "4. Return the count of 'PULSE_TAKEN' actions.\n",
      "\n",
      "mean_stills_value (Average Stills Value)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Extract the actual and ideal sequences of first interactions from the scene in terms of still/waver/walker.\n",
      "3. Truncate both sequences to the head at the stills length and compare them; they both should have all stills.\n",
      "4. If they are, output a 1 (All Stills visited first), if not, output a 0 (All Stills not visited first).\n",
      "\n",
      "mean_teleport_count (Average Teleport Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Create a boolean mask to filter TELEPORT action types.\n",
      "3. Count the number of actions.\n",
      "\n",
      "mean_time_to_hemorrhage_control_per_patient (Average Time to Hemorrhage Control Per Patient)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Iterate through patients in the scene.\n",
      "3. Check if the patient is hemorrhaging and not dead.\n",
      "4. Calculate the hemorrhage control per patient.\n",
      "\n",
      "mean_time_to_last_hemorrhage_controlled (Average Time to Last Hemorrhage Controlled)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the start time of the scene.\n",
      "3. Initialize the last controlled time to 0.\n",
      "4. Iterate through patients in the scene.\n",
      "5. Check if the patient is hemorrhaging and not dead.\n",
      "6. Get the time to hemorrhage control for the patient.\n",
      "7. Update the last controlled time if the current controlled time is greater.\n",
      "8. Return the time to the last hemorrhage controlled event.\n",
      "\n",
      "mean_total_actions_count (Average Total Actions Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Create a boolean mask to filter action types that are user-initiated (TELEPORT, S_A_L_T_WALK_IF_CAN, TRIAGE_LEVEL_WALK_IF_CAN, S_A_L_T_WAVE_IF_CAN, TRIAGE_LEVEL_WAVE_IF_CAN, PATIENT_ENGAGED, PULSE_TAKEN, BAG_ACCESS, TOOL_HOVER, TOOL_SELECTED, INJURY_TREATED, TOOL_APPLIED, TAG_SELECTED, TAG_APPLIED, BAG_CLOSED, TAG_DISCARDED, and TOOL_DISCARDED).\n",
      "3. Include VOICE_COMMAND actions with specific user-initiated messages in the mask (walk to the safe area, wave if you can, are you hurt, reveal injury, lay down, where are you, can you hear, anywhere else, what is your name, hold still, sit up/down, stand up, can you breathe, show me, stand, walk, and wave).\n",
      "4. Count the number of user actions for the current group.\n",
      "5. Return the total number of user actions.\n",
      "\n",
      "mean_triage_time (Average Triage Time)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Get the scene start and end times.\n",
      "3. Calculate the triage time.\n",
      "\n",
      "mean_voice_capture_count (Average Voice Capture Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter for actions with the type \"VOICE_CAPTURE\".\n",
      "3. Count the number of \"VOICE_CAPTURE\" actions.\n",
      "4. Return the count of 'VOICE_CAPTURE' actions.\n",
      "\n",
      "mean_walk_command_count (Average Walk Command Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter for voice commands with the message \"walk to the safe area\".\n",
      "3. Count the number of \"walk to the safe area\" voice commands.\n",
      "4. Return the count of 'walk to the safe area' voice command events.\n",
      "\n",
      "mean_walk_value (Average Walk Value)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Check in the scene if there are any WALK_IF_CAN actions.\n",
      "3. If there are, output a 1 (Walk Command issued), if not, output a 0 (No Walk Command issued).\n",
      "\n",
      "mean_walkers_value (Average Walkers Value)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Extract the actual and ideal sequences of first interactions from the scene in terms of still/waver/walker.\n",
      "3. Truncate both sequences to the tail at the walkers length and compare them; they both should have all walkers.\n",
      "4. If they are, output a 1 (All Walkers visited last), if not, output a 0 (All Walkers not visited last).\n",
      "\n",
      "mean_wave_command_count (Average Wave Command Count)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Filter for voice commands with the message \"wave if you can\".\n",
      "3. Count the number of \"wave if you can\" voice commands.\n",
      "4. Return the count of 'wave if you can' voice command events.\n",
      "\n",
      "mean_wave_value (Average Wave Value)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Check in the scene if there are any WAVE_IF_CAN actions.\n",
      "3. If there are, output a 1 (Wave Command issued), if not, output a 0 (No Wave Command issued).\n",
      "\n",
      "medical_role (Medical Role)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Use the \"MedRole\" key from the JSON stats to determine the integer value.\n",
      "3. Determine the intersection of JSON stats columns and the columns in your dataframe to merge on (usually participant_id and session_uuid).\n",
      "4. Filter only those \"merge on\" columns and the \"MedRole\" column on the right side of the merge in order for your dataframe to do a left outer join with the JSON stats dataframe.\n",
      "5. Decode the integer value by means of the Labels column in the Metrics_Evaluation_Dataset_organization_for_BBAI spreadsheet provided by CACI and map that to the new column.\n",
      "\n",
      "encounter_layout (Encounter Layout)\n",
      "Steps Needed to do Calculations:\n",
      "1. Group your dataset by participant_id, session_uuid, and scene_id..\n",
      "2. Print the shape of the CSV stats DataFrame.\n",
      "3. Use the patients lists from the March 25th ITM BBAI Exploratory analysis email.\n",
      "4. Loop through each session and scene in the CSV stats dataset.\n",
      "5. Print the unique patient IDs for each scene.\n",
      "6. Loop through each environment and get the patients list for that environment.\n",
      "7. Check if all patients are in that scene.\n",
      "8. If so, find the corresponding session in the JSON stats dataset and add that environment to it as a new column.\n",
      "9. Print the shape of the JSON stats DataFrame.\n",
      "10. Display the count of records for each environment.\n",
      "Split the patients metadata ({nu.conjunctify_nounts(engaged_patient_columns_list)}) from the patients list\n",
      "['fu.get_actual_engagement_distance', 'fu.get_first_engagement', 'fu.get_first_treatment', 'fu.get_injury_correctly_treated_count', 'fu.get_injury_not_treated_count', 'fu.get_injury_wrongly_treated_count', 'fu.get_last_engagement', 'fu.get_last_still_engagement', 'fu.get_measure_of_right_ordering', 'fu.get_patient_count', 'fu.get_percent_hemorrhage_controlled', 'fu.add_prioritize_severity_column', 'fu.get_pulse_taken_count', 'fu.get_stills_value', 'fu.get_teleport_count', 'fu.get_time_to_hemorrhage_control_per_patient', 'fu.get_time_to_last_hemorrhage_controlled', 'fu.get_total_actions_count', 'fu.get_triage_time', 'fu.get_voice_capture_count', 'fu.get_walk_command_count', 'fu.get_walk_value', 'fu.get_walkers_value', 'fu.get_wave_command_count', 'fu.get_wave_value', 'fu.add_medical_role_column', 'fu.add_encounter_layout_column']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "\n",
    "print('\\nWrite up the steps to do the ANOVA stats columns calculations')\n",
    "comment_regex = re.compile('^ *# ([^\\r\\n]+)', MULTILINE)\n",
    "function_call_dict = {\n",
    "    'encounter_layout': 'fu.add_encounter_layout_column', 'medical_role': 'fu.add_medical_role_column',\n",
    "    'mean_prioritize_high_injury_severity_patients': 'fu.add_prioritize_severity_column'\n",
    "}\n",
    "survey_columns = ['AD_KDMA_Sim', 'AD_KDMA_Text', 'PropTrust', 'ST_KDMA_Sim', 'ST_KDMA_Text', 'YrsMilExp']\n",
    "mean_survey_columns = ['mean_' + cn for cn in survey_columns]\n",
    "engaged_patient_columns_list = []\n",
    "function_calls_list = []\n",
    "for cn in anova_df.columns:\n",
    "    if 'engaged_patient' in cn:\n",
    "        engaged_patient_columns_list.append(cn)\n",
    "        continue\n",
    "    print('')\n",
    "    print(f'{cn} ({entitle_column_name(cn)})')\n",
    "    print('Steps Needed to do Calculations:')\n",
    "    comments_list = []\n",
    "    if cn in ['participant_id', 'scene_id', 'session_uuid']:\n",
    "        if cn == 'scene_id':\n",
    "            comments_list.append('The scene_id is derived from the CSV SESSION_START and SESSION_END entries.')\n",
    "        else:\n",
    "            comments_list.append(\n",
    "                f'The {cn} is found in both the CSV and the JSON data in the'\n",
    "                + ' \"Human_Sim_Metrics_Data 4-12-2024.zip\" file provided by CACI.'\n",
    "            )\n",
    "    else:\n",
    "        comments_list.append('Group your dataset by participant_id, session_uuid, and scene_id.')\n",
    "        if cn in mean_survey_columns:\n",
    "            comments_list.extend([\n",
    "                f'Find the {cn.replace(\"mean_\", \"\")} column in the participant_data_0420 spreadsheet provided by CACI for that participant',\n",
    "                f'The {cn.replace(\"mean_\", \"\")} value is semi-continously numeric, and you can average it for whatever grouping you need'\n",
    "            ])\n",
    "        else:\n",
    "            if cn in function_call_dict:\n",
    "                function_call = function_call_dict[cn]\n",
    "            else:\n",
    "                function_call = cn.replace('mean_', 'fu.get_')\n",
    "            function_calls_list.append(function_call)\n",
    "            source_code = inspect.getsource(eval(function_call))\n",
    "            comments_list.extend([comment_str for comment_str in comment_regex.findall(source_code) if comment_str and ('verbose' not in comment_str)])\n",
    "    for i, comment_str in enumerate(comments_list):\n",
    "        print(f'{i+1}. {comment_str}.')\n",
    "if engaged_patient_columns_list:\n",
    "    print('Split the patients metadata ({nu.conjunctify_nounts(engaged_patient_columns_list)}) from the patients list')\n",
    "print(function_calls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "092caaf5-cd4b-415c-8cd9-d4bef967b0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mean_prioritize_high_injury_severity_patients', 'fu.get_prioritize_high_injury_severity_patients')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cn, function_calls_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bada6956-bed0-4bc7-b6a5-a6ffcdd5792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for metrics_evaluation_open_world_scene_stats_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/metrics_evaluation_open_world_scene_stats_df.csv.\n",
      "['all_patient_injuries_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    verbose=True, metrics_evaluation_open_world_scene_stats_df=''\n",
    ")\n",
    "scene_stats_df = data_frames_dict['metrics_evaluation_open_world_scene_stats_df'].copy()\n",
    "print(sorted([cn for cn in scene_stats_df.columns if 'all' in cn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d48b06-0fe7-473d-97a4-f36b0aec0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groupby_columns = ['participant_id', 'session_uuid', 'scene_id']\n",
    "rows_list = []\n",
    "for (participant_id, session_uuid, scene_id), scene_df in scene_stats_df.groupby(groupby_columns):\n",
    "    row_dict = {}\n",
    "    for cn in groupby_columns: row_dict[cn] = eval(cn)\n",
    "    for cn in scene_df.columns:\n",
    "        if 'injur' in cn:\n",
    "            row_dict[cn] = scene_df[cn].squeeze()\n",
    "    rows_list.append(row_dict)\n",
    "injury_counts_df = DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6a8561-03a2-4d8e-af6a-fd3ab8745855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_patient_injuries_count</th>\n",
       "      <th>injury_correctly_treated_count</th>\n",
       "      <th>injury_not_treated_count</th>\n",
       "      <th>injury_wrongly_treated_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    all_patient_injuries_count  injury_correctly_treated_count  \\\n",
       "0                           31                              15   \n",
       "1                           18                               0   \n",
       "2                            0                               0   \n",
       "3                            0                               0   \n",
       "4                           23                              11   \n",
       "..                         ...                             ...   \n",
       "63                          19                               5   \n",
       "64                          23                              12   \n",
       "65                          31                              23   \n",
       "66                          31                              10   \n",
       "67                          23                               5   \n",
       "\n",
       "    injury_not_treated_count  injury_wrongly_treated_count  \n",
       "0                         10                             6  \n",
       "1                         18                             0  \n",
       "2                          0                             0  \n",
       "3                          0                             0  \n",
       "4                          8                             4  \n",
       "..                       ...                           ...  \n",
       "63                         9                             5  \n",
       "64                         7                             4  \n",
       "65                         2                             6  \n",
       "66                        20                             1  \n",
       "67                        15                             3  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = [cn for cn in injury_counts_df.columns if 'injur' in cn]\n",
    "injury_counts_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cf11fb9-5cf1-412f-9724-748c96f66e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert (\n",
    "    scene_stats_df.all_patient_injuries_count == scene_stats_df[\n",
    "        ['injury_correctly_treated_count', 'injury_not_treated_count', 'injury_wrongly_treated_count']\n",
    "    ].sum(axis='columns')\n",
    ").all(), \"The sum of the various injury counts does not match the total injury count for all rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269310da-3df9-4369-850d-9e7d674d5991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        # Create a mock DataFrame with all injury columns\n",
      "        self.injury_counts_df = DataFrame({\n",
      "            \"all_patient_injuries_count\": [20, 32, 25, 0, 18]\n",
      "            \"injury_correctly_treated_count\": [3, 19, 9, 0, 0]\n",
      "            \"injury_not_treated_count\": [13, 11, 11, 0, 18]\n",
      "            \"injury_wrongly_treated_count\": [4, 2, 5, 0, 0]\n",
      "        })\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = injury_counts_df.sample(5)\n",
    "print(f\"\"\"\n",
    "        # Create a mock DataFrame with all injury columns\n",
    "        self.injury_counts_df = DataFrame({{\n",
    "            \"all_patient_injuries_count\": {df.all_patient_injuries_count.tolist()}\n",
    "            \"injury_correctly_treated_count\": {df.injury_correctly_treated_count.tolist()}\n",
    "            \"injury_not_treated_count\": {df.injury_not_treated_count.tolist()}\n",
    "            \"injury_wrongly_treated_count\": {df.injury_wrongly_treated_count.tolist()}\n",
    "        }})\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36eef4b7-a041-4571-ae9b-34a1a826c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groupby_columns = ['participant_id', 'session_uuid', 'scene_id']\n",
    "millisecond_threshold = 0\n",
    "for (participant_id, session_uuid, scene_id), scene_df in csv_stats_df.groupby(groupby_columns):\n",
    "    \n",
    "    # Loop through each injury and make a determination if it's treated or not\n",
    "    injury_correctly_treated_count = 0\n",
    "    for patient_id, patient_df in scene_df.groupby('patient_id'):\n",
    "        for injury_id, injury_df in patient_df.groupby('injury_id'):\n",
    "            is_correctly_treated = fu.get_is_injury_correctly_treated(injury_df, patient_df)\n",
    "            if is_correctly_treated: injury_correctly_treated_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a56d1c5-656a-4f95-b4a0-4a4432afcb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask_series = (injury_df.action_type == 'INJURY_TREATED')\n",
    "action_ticks_list = sorted(injury_df[mask_series].action_tick.unique())\n",
    "mask_series = ~injury_df.injury_required_procedure.isnull()\n",
    "required_procedure = injury_df[mask_series].injury_required_procedure.mode().squeeze()\n",
    "print(required_procedure)\n",
    "for action_tick in action_ticks_list:\n",
    "    mask_series = patient_df.action_tick.map(\n",
    "        lambda ts: abs(ts - action_tick) < millisecond_threshold\n",
    "    ) & patient_df.action_type.isin(['TOOL_APPLIED'])\n",
    "    if mask_series.any():\n",
    "        print([(required_procedure == self.tool_type_to_required_procedure_dict.get(tool_type)) for tool_type in patient_df[mask_series].tool_applied_type])\n",
    "        is_correctly_treated = any(\n",
    "            [(required_procedure == self.tool_type_to_required_procedure_dict.get(tool_type)) for tool_type in patient_df[mask_series].tool_applied_type]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a30f5f09-c21b-4da7-a9a5-be2a878fa6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>139341</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>action_type</th>\n",
       "      <td>INJURY_RECORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action_tick</th>\n",
       "      <td>19200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <td>2024-04-05 12:10:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_uuid</th>\n",
       "      <td>9f3bb117-3a4b-4bc7-9237-e2d9920d5333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csv_file_subpath</th>\n",
       "      <td>Human_Sim_Metrics_Data_4-12-2024/9f3bb117-3a4b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_id</th>\n",
       "      <td>L Shoulder Broken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_patient_id</th>\n",
       "      <td>Patient X Root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_required_procedure</th>\n",
       "      <td>splint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_severity</th>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_body_region</th>\n",
       "      <td>leftArm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_injury_treated</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_injury_treated_with_wrong_treatment</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_record_injury_injury_locator</th>\n",
       "      <td>(-1.3, 2.4, -20.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene_id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <td>2024227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csv_file_name</th>\n",
       "      <td>9f3bb117-3a4b-4bc7-9237-e2d9920d5333_2024227.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <td>Patient X Root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_id</th>\n",
       "      <td>L Shoulder Broken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_id</th>\n",
       "      <td>(-1.3, 2.4, -20.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_severity</th>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injury_body_region</th>\n",
       "      <td>leftArm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              139341\n",
       "action_type                                                                            INJURY_RECORD\n",
       "action_tick                                                                                    19200\n",
       "event_time                                                                       2024-04-05 12:10:51\n",
       "session_uuid                                                    9f3bb117-3a4b-4bc7-9237-e2d9920d5333\n",
       "csv_file_subpath                                   Human_Sim_Metrics_Data_4-12-2024/9f3bb117-3a4b...\n",
       "injury_record_id                                                                   L Shoulder Broken\n",
       "injury_record_patient_id                                                              Patient X Root\n",
       "injury_record_required_procedure                                                              splint\n",
       "injury_record_severity                                                                        medium\n",
       "injury_record_body_region                                                                    leftArm\n",
       "injury_record_injury_treated                                                                   False\n",
       "injury_record_injury_treated_with_wrong_treatment                                              False\n",
       "injury_record_injury_injury_locator                                               (-1.3, 2.4, -20.9)\n",
       "scene_id                                                                                           0\n",
       "participant_id                                                                               2024227\n",
       "csv_file_name                                       9f3bb117-3a4b-4bc7-9237-e2d9920d5333_2024227.csv\n",
       "patient_id                                                                            Patient X Root\n",
       "injury_id                                                                          L Shoulder Broken\n",
       "location_id                                                                       (-1.3, 2.4, -20.9)\n",
       "injury_severity                                                                               medium\n",
       "injury_body_region                                                                           leftArm"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "injury_df.dropna(axis='columns', how='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f477c558-c837-4be9-9790-c72219ca4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type</th>\n",
       "      <th>action_tick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42381</th>\n",
       "      <td>PATIENT_ENGAGED</td>\n",
       "      <td>1262495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42407</th>\n",
       "      <td>BREATHING_CHECKED</td>\n",
       "      <td>1266435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42408</th>\n",
       "      <td>INJURY_TREATED</td>\n",
       "      <td>1267526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42409</th>\n",
       "      <td>TOOL_APPLIED</td>\n",
       "      <td>1267526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42420</th>\n",
       "      <td>INJURY_TREATED</td>\n",
       "      <td>1273515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42422</th>\n",
       "      <td>PULSE_TAKEN</td>\n",
       "      <td>1273567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42483</th>\n",
       "      <td>INJURY_TREATED</td>\n",
       "      <td>1281386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42485</th>\n",
       "      <td>PULSE_TAKEN</td>\n",
       "      <td>1281897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42508</th>\n",
       "      <td>INJURY_TREATED</td>\n",
       "      <td>1286988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42626</th>\n",
       "      <td>PATIENT_DEMOTED</td>\n",
       "      <td>192305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42647</th>\n",
       "      <td>PATIENT_RECORD</td>\n",
       "      <td>193949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42648</th>\n",
       "      <td>INJURY_RECORD</td>\n",
       "      <td>193949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42649</th>\n",
       "      <td>INJURY_RECORD</td>\n",
       "      <td>193949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42650</th>\n",
       "      <td>INJURY_RECORD</td>\n",
       "      <td>193949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             action_type  action_tick\n",
       "42381    PATIENT_ENGAGED      1262495\n",
       "42407  BREATHING_CHECKED      1266435\n",
       "42408     INJURY_TREATED      1267526\n",
       "42409       TOOL_APPLIED      1267526\n",
       "42420     INJURY_TREATED      1273515\n",
       "42422        PULSE_TAKEN      1273567\n",
       "42483     INJURY_TREATED      1281386\n",
       "42485        PULSE_TAKEN      1281897\n",
       "42508     INJURY_TREATED      1286988\n",
       "42626    PATIENT_DEMOTED       192305\n",
       "42647     PATIENT_RECORD       193949\n",
       "42648      INJURY_RECORD       193949\n",
       "42649      INJURY_RECORD       193949\n",
       "42650      INJURY_RECORD       193949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['action_type', 'action_tick']\n",
    "patient_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f33b25f-d66d-40c1-96f0-14f8ab8a9385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adept Shooter Root has a R Bicep Puncture which requires a tourniquet and it was correctly treated\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6817/3824210221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;34mf'{patient_id} has a {injury_id} which requires a {required_procedure} and it {\"was\" if is_correctly_treated else \"was not\"} correctly treated'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 )\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "\n",
    "from pandas import Series\n",
    "\n",
    "groupby_columns = ['participant_id', 'session_uuid', 'scene_id', 'patient_id']\n",
    "for (participant_id, session_uuid, scene_id, patient_id), patient_df in csv_stats_df.groupby(groupby_columns):\n",
    "    for injury_id, injury_df in patient_df.groupby('injury_id'):\n",
    "        action_ticks_list = sorted(injury_df.action_tick.unique())\n",
    "        mask_series = ~patient_df.tool_type.isnull() & patient_df.action_tick.isin(action_ticks_list)\n",
    "        if mask_series.any():\n",
    "            df = patient_df[mask_series]\n",
    "            mask_series = ~injury_df.injury_record_required_procedure.isnull()\n",
    "            required_procedure = injury_df[mask_series].injury_record_required_procedure.mode().squeeze()\n",
    "            if isinstance(required_procedure, Series):\n",
    "                required_procedure = '(who knows?)'\n",
    "                raise\n",
    "            for tool_type in df.tool_type:\n",
    "                is_correctly_treated = (required_procedure == fu.tool_type_to_required_procedure_dict.get(tool_type))\n",
    "                print(\n",
    "                    f'{patient_id} has a {injury_id} which requires a {required_procedure} and it {\"was\" if is_correctly_treated else \"was not\"} correctly treated'\n",
    "                )\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ebb44af-16ef-4148-87e5-f4d047f7e979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>injury_record_required_procedure</th>\n",
       "      <th>injury_treated_required_procedure</th>\n",
       "      <th>injury_required_procedure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42658</th>\n",
       "      <td>tourniquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tourniquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>tourniquet</td>\n",
       "      <td>tourniquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      injury_record_required_procedure injury_treated_required_procedure  \\\n",
       "42658                       tourniquet                               NaN   \n",
       "44199                              NaN                        tourniquet   \n",
       "\n",
       "      injury_required_procedure  \n",
       "42658                tourniquet  \n",
       "44199                tourniquet  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = [cn for cn in injury_df.columns if 'required_procedure' in cn]\n",
    "injury_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d1f69bf-ce48-4258-b1aa-8f7879258a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04f80090-9e61-431d-8473-dccb75fed04d_2024207.csv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "osp.basename(osp.join('../data/logs', df.csv_file_subpath.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4855a-ed51-4f87-8fdd-c91686285b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

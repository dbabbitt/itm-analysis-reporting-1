{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456af56e-0ca1-4d3a-9918-b171362593b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e1ffe-922e-4f82-b6e0-cdc7b87b358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groupby_columns = fu.scene_groupby_columns\n",
    "error_types_df = csv_stats_df.copy()\n",
    "for groupby_tuple, responder_categories_df in error_types_df.groupby(groupby_columns):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb334f0-50dc-4d35-9789-56601e5f7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{column_name: column_value for column_name, column_value in zip(groupby_columns, groupby_tuple)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87f853-6cbf-42b5-bd29-99bf20557722",
   "metadata": {},
   "source": [
    "\n",
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1837c6-bef3-4736-bd0f-5e59a721c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = ~scene_df.patient_id.isnull()\n",
    "df = scene_df[mask_series]\n",
    "if df.shape[0]: display(df.sample(min(df.shape[0], 5)).dropna(axis='columns', how='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d5c33-748f-48e0-a086-7d6ca458f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get them set up as if we were going to run triage accuracy and time to hemorrhage control stats on them\n",
    "# I know they are small samples, I primarily want you to let us know what information is missing for you to look at those things.\n",
    "# I am also interested in variability around each patient. Are you able to segregate the files by patient and tell me something about\n",
    "# how different participants acted (assess, treat, tag) in response to each patient?\n",
    "# I also want to extract from each csv a list of the actions the responder did (engage, assess, treat, and tag actions, not movement or others)\n",
    "# so that I can see a list of what they did in the scenario (this will eventually form the basis of delegation decisions).\n",
    "# Basically I would like you to play with this small sample in anticipation of receiving a similar, larger dataset from these new scenarios.\n",
    "# Let me know if you have questions but also please be patient as this is low priority compared to evaluation preparation this week.\n",
    "import os\n",
    "import glob\n",
    "import os.path as osp\n",
    "\n",
    "data_frames_dict = nu.load_data_frames(verbose=False, metrics_evaluation_open_world_scene_stats_df='', metrics_evaluation_open_world_json_stats_df='')\n",
    "json_stats_df = data_frames_dict['metrics_evaluation_open_world_json_stats_df']\n",
    "logs_folder = '../data/logs'\n",
    "pattern = osp.join(logs_folder, 'Disaster Day 3.6.2024 ITM Files 405*')\n",
    "csv_count = 0\n",
    "for logs_path in glob.glob(pattern):\n",
    "    csv_count += len([fn for sub_directory, directories_list, files_list in os.walk(logs_path) for fn in files_list if fn.lower().endswith('.csv')])\n",
    "uuids_count = json_stats_df.session_uuid.unique().shape[0]\n",
    "elevens_mask = (scene_stats_df.patient_count >= 11) & (scene_stats_df.is_scene_aborted == False)\n",
    "scene_type_dict = scene_stats_df[elevens_mask].groupby('scene_type').size().to_dict()\n",
    "orientation_count = scene_type_dict['Orientation'] if 'Orientation' in scene_type_dict else 0\n",
    "triage_count = scene_type_dict['Triage'] if 'Triage' in scene_type_dict else 0\n",
    "aborted_count = scene_stats_df[scene_stats_df.is_scene_aborted].shape[0]\n",
    "aborted_suffix = 's' if (aborted_count != 1) else ''\n",
    "one_triage_dict = json_stats_df.groupby('is_a_one_triage_file').size().to_dict()\n",
    "print(f'''Okay, I have ingested Disaster day 3.6.2024 ITM files 405F.zip and Disaster Day 3.6.2024 ITM Files 405E.zip and have verified that:\n",
    "•\tThere are {csv_count} CSV files in the zip file.\n",
    "•\tThere are {uuids_count} unique session UUIDs in there.\n",
    "•\tThere are {orientation_count} 11-patient-or-above Orientation scenes and {triage_count} Triage scenes in there.\n",
    "•\tThe time difference between when the scene starts and the last engagement for {aborted_count} scene{aborted_suffix} is longer than 16 minutes.\n",
    "•\tThere are {one_triage_dict.get(True, 0)} files that have one and only one triage scene in them. (The other {one_triage_dict.get(False, 0)}''')\n",
    "print(''' are not one-triage-scene files).\n",
    "If any of these is unexpected, please explain why.\n",
    "''')\n",
    "mask_series = (scene_stats_df.scene_type == 'Orientation') & elevens_mask\n",
    "uuids_list = sorted(scene_stats_df[mask_series].session_uuid.unique())\n",
    "mask_series = json_stats_df.session_uuid.isin(uuids_list)\n",
    "if mask_series.any():\n",
    "    print(\n",
    "        f'P.S. The files with orientation scenes in them are:'\n",
    "    )\n",
    "    for file_name in sorted(json_stats_df[mask_series].session_file_name): print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edab44-2162-4c9a-8dbd-1e18f73e07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "if 'is_in_registry' in json_stats_df:\n",
    "    registry_uuids = sorted(json_stats_df[json_stats_df.is_in_registry].session_uuid.unique())\n",
    "    print(f'Of the {len(registry_uuids)} files in the registry, ', end='')\n",
    "    mask_series = scene_stats_df.session_uuid.isin(registry_uuids) & (scene_stats_df.scene_type == 'Triage')\n",
    "    print(f'of the {scene_stats_df[mask_series].shape[0]} triage scenes in those files, this is the counts of the values of responder type:')\n",
    "    display(HTML(json_stats_df.responder_type.value_counts().to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cf836-adb1-411f-904e-7dfc224b46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(HTML(scene_stats_df.groupby(['patient_count', 'scene_type']).size().to_frame().rename(columns={0: 'record_count'}).reset_index(drop=False).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfe1c3-a484-4b03-8964-b6f1c342f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = (scene_stats_df.scene_type == 'Orientation')\n",
    "df = scene_stats_df[mask_series].dropna(axis='columns', how='all')\n",
    "display(df.sample(10).T)\n",
    "if 'session_uuid' in df.columns:\n",
    "    session_uuids_list = sorted(df.session_uuid.unique())\n",
    "    mask_series = json_stats_df.session_uuid.isin(session_uuids_list)\n",
    "    df = json_stats_df[mask_series].dropna(axis='columns', how='all')\n",
    "    display(df.sample(9).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c035ed-78fe-443c-a016-47ef307ebe87",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fc3d0-2135-4569-8e85-6567523a3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "is_scalar_lambda = lambda value: isinstance(value, (int, float, str, bool, np.number))\n",
    "for (session_uuid, scene_id), idx_df in distance_delta_df.groupby(fu.scene_groupby_columns):\n",
    "    \n",
    "    # Get the whole scene history\n",
    "    mask_series = True\n",
    "    for cn in fu.scene_groupby_columns: mask_series &= (csv_stats_df[cn] == eval(cn))\n",
    "    scene_df = csv_stats_df[mask_series]\n",
    "    \n",
    "    if is_scalar_lambda(fu.get_scene_start(scene_df)): print(\n",
    "        \"    row_dict['scene_start'] = fu.get_scene_start(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_last_engagement(scene_df)): print(\n",
    "        \"    row_dict['last_engagement'] = fu.get_last_engagement(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_player_location(scene_df, action_tick=0)): print(\n",
    "        \"    row_dict['player_location'] = fu.get_player_location(scene_df, action_tick)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_scene_type(scene_df)): print(\n",
    "        \"    row_dict['scene_type'] = fu.get_scene_type(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_scene_end(scene_df)): print(\n",
    "        \"    row_dict['scene_end'] = fu.get_scene_end(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_patient_count(scene_df)): print(\n",
    "        \"    row_dict['patient_count'] = fu.get_patient_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_injury_treatments_count(scene_df)): print(\n",
    "        \"    row_dict['injury_treatments_count'] = fu.get_injury_treatments_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_injury_not_treated_count(scene_df)): print(\n",
    "        \"    row_dict['injury_not_treated_count'] = fu.get_injury_not_treated_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_injury_correctly_treated_count(scene_df)): print(\n",
    "        \"    row_dict['injury_correctly_treated_count'] = fu.get_injury_correctly_treated_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_injury_wrongly_treated_count(scene_df)): print(\n",
    "        \"    row_dict['injury_wrongly_treated_count'] = fu.get_injury_wrongly_treated_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_pulse_taken_count(scene_df)): print(\n",
    "        \"    row_dict['pulse_taken_count'] = fu.get_pulse_taken_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_teleport_count(scene_df)): print(\n",
    "        \"    row_dict['teleport_count'] = fu.get_teleport_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_voice_capture_count(scene_df)): print(\n",
    "        \"    row_dict['voice_capture_count'] = fu.get_voice_capture_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_walk_command_count(scene_df)): print(\n",
    "        \"    row_dict['walk_command_count'] = fu.get_walk_command_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_wave_command_count(scene_df)): print(\n",
    "        \"    row_dict['wave_command_count'] = fu.get_wave_command_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_first_engagement(scene_df)): print(\n",
    "        \"    row_dict['first_engagement'] = fu.get_first_engagement(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_first_treatment(scene_df)): print(\n",
    "        \"    row_dict['first_treatment'] = fu.get_first_treatment(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_ideal_engagement_order(scene_df, tuples_list=None)): print(\n",
    "        \"    row_dict['ideal_engagement_order'] = fu.get_ideal_engagement_order(scene_df, tuples_list=None)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_is_scene_aborted(scene_df)): print(\n",
    "        \"    row_dict['is_scene_aborted'] = fu.get_is_scene_aborted(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_triage_time(scene_df)): print(\n",
    "        \"    row_dict['triage_time'] = fu.get_triage_time(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_dead_patients(scene_df)): print(\n",
    "        \"    row_dict['dead_patients'] = fu.get_dead_patients(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_still_patients(scene_df)): print(\n",
    "        \"    row_dict['still_patients'] = fu.get_still_patients(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_total_actions_count(scene_df)): print(\n",
    "        \"    row_dict['total_actions'] = fu.get_total_actions_count(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_actual_and_ideal_patient_sort_sequences(scene_df)): print(\n",
    "        \"    row_dict['actual_and_ideal_sequences'] = fu.get_actual_and_ideal_patient_sort_sequences(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_measure_of_right_ordering(scene_df)): print(\n",
    "        \"    row_dict['measure_of_right_ordering'] = fu.get_measure_of_right_ordering(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_percent_hemorrhage_controlled(scene_df)): print(\n",
    "        \"    row_dict['percent_hemorrhage_controlled'] = fu.get_percent_hemorrhage_controlled(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_time_to_last_hemorrhage_controlled(scene_df)): print(\n",
    "        \"    row_dict['time_to_last_hemorrhage_controlled'] = fu.get_time_to_last_hemorrhage_controlled(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_time_to_hemorrhage_control_per_patient(scene_df)): print(\n",
    "        \"    row_dict['time_to_hemorrhage_control_per_patient'] = fu.get_time_to_hemorrhage_control_per_patient(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_triage_priority_data_frame(scene_df)): print(\n",
    "        \"    row_dict['triage_priority_data_frame'] = fu.get_triage_priority_data_frame(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_actual_engagement_order(scene_df)): print(\n",
    "        \"    row_dict['actual_engagement_order'] = fu.get_actual_engagement_order(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_distracted_engagement_order(scene_df, tuples_list=None)): print(\n",
    "        \"    row_dict['distracted_engagement_order'] = fu.get_distracted_engagement_order(scene_df, tuples_list=None)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_stills_value(scene_df)): print(\n",
    "        \"    row_dict['stills_value'] = fu.get_stills_value(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_walkers_value(scene_df)): print(\n",
    "        \"    row_dict['walkers_value'] = fu.get_walkers_value(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_wave_value(scene_df)): print(\n",
    "        \"    row_dict['wave_value'] = fu.get_wave_value(scene_df)\"\n",
    "    )\n",
    "    if is_scalar_lambda(fu.get_walk_value(scene_df)): print(\n",
    "        \"    row_dict['walk_value'] = fu.get_walk_value(scene_df)\"\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea63d19-2dab-46d6-be95-fac6a5a22f26",
   "metadata": {},
   "source": [
    "\n",
    "Please use the data attached for exploratory analyses in our open world scenarios. The document attached defines each of the variables (by column) and each row represents one human.\n",
    "\n",
    "You should have TWO simulation data files for each participant (row) and they should be matched up by the UUIDs in columns 3 and 4 (they should be two different environments in case you have those in separate folders).\n",
    "\n",
    "I would like you to look for any patterns in these data, especially related to the variables “ST_KDMA_Text” “ST_KDMA_Sim” “AD_KDMA_Text” “AD_KDMA_Sim”\n",
    "Let me know if you have any questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d08c41-989d-4d5d-8513-174b3050e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add file start time\n",
    "if ('in_sim1' not in json_stats_df.columns) or ('in_sim2' not in json_stats_df.columns):\n",
    "    json_stats_df['in_sim1'] = False\n",
    "    json_stats_df['in_sim2'] = False\n",
    "    \n",
    "    from pandas import read_excel\n",
    "    file_path = '../data/xlsx/Metrics_Eval_Participant_data_for_BBAI.xlsx'\n",
    "    participant_data_df = read_excel(file_path)\n",
    "    \n",
    "    mask_series = ~participant_data_df.Sim1.isnull() & (participant_data_df.Sim1 != '')\n",
    "    sim1_session_uuids_list = [u[:-1] if u.endswith('_') else u for u in sorted(participant_data_df[mask_series].Sim1)]\n",
    "    mask_series = ~participant_data_df.Sim2.isnull() & (participant_data_df.Sim2 != '')\n",
    "    sim2_session_uuids_list = [u[:-1] if u.endswith('_') else u for u in sorted(participant_data_df[mask_series].Sim2)]\n",
    "    assert set(sim1_session_uuids_list).intersection(set(sim2_session_uuids_list)) == set(), \"The participant ID groupings are not distinct\"\n",
    "    \n",
    "    mask_series = json_stats_df.session_uuid.isin(sim1_session_uuids_list)\n",
    "    json_stats_df.loc[mask_series, 'in_sim1'] = True\n",
    "    mask_series = json_stats_df.session_uuid.isin(sim2_session_uuids_list)\n",
    "    json_stats_df.loc[mask_series, 'in_sim2'] = True\n",
    "    \n",
    "    nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df, verbose=True)\n",
    "    nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df, verbose=True)\n",
    "display(json_stats_df.groupby('in_sim1', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))\n",
    "display(json_stats_df.groupby('in_sim2', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447d237-6e2f-4389-be27-fb43e11700a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f102db-c03d-4526-b4a9-b6a22d67a0bc",
   "metadata": {},
   "source": [
    "\n",
    "# Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40812c06-886b-4e38-992f-fabeee0b507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# See if you can merge on Sim1's session_uuid\n",
    "sim1_df = json_stats_df.merge(participant_data_df, left_on='session_uuid', right_on='Sim1', how='inner')\n",
    "print(sim1_df.shape)\n",
    "mask_series = (sim1_df.participant_id_x != sim1_df.participant_id_y)\n",
    "assert mask_series.sum() == 0, f\"{mask_series.sum()} files have mismatched Participant IDs in Sim1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954816e-8076-4bf5-8469-b323d55bf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# See if you can merge on Sim2's session_uuid\n",
    "sim2_df = json_stats_df.merge(participant_data_df, left_on='session_uuid', right_on='Sim2', how='inner')\n",
    "print(sim2_df.shape)\n",
    "mask_series = (sim2_df.participant_id_x != sim2_df.participant_id_y)\n",
    "assert mask_series.sum() == 0, f\"{mask_series.sum()} files have mismatched Participant IDs in Sim2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a6a8-e094-4f50-8066-07816980fac9",
   "metadata": {},
   "source": [
    "\n",
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68318a90-6310-42a4-974a-956f441e13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(json_stats_df.participant_id.nunique()) # 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bc561-6b04-4ded-bacf-059c553e083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show numeric columns\n",
    "column_descriptions_df = nu.get_column_descriptions(participant_data_df)\n",
    "mask_series = column_descriptions_df.dtype.isin(['float64', 'int64'])\n",
    "columns_set = set(column_descriptions_df[mask_series].column_name).difference(set(['ParticipantID']))\n",
    "list(columns_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a4085-53e1-48c4-8525-e2490fdcdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Walk through the logs, getting only file names and session_uuids\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "import os\n",
    "from os import path as osp\n",
    "\n",
    "important_columns_list = ['file_name', 'session_uuid']\n",
    "file_names_and_session_uuids_df = DataFrame([], columns=important_columns_list)\n",
    "\n",
    "# Iterate over the subdirectories, directories, and files in the logs folder\n",
    "logs_folder = '../data/logs'\n",
    "for sub_directory, directories_list, files_list in os.walk(logs_folder):\n",
    "    \n",
    "    # Create a data frame to store the data for the current subdirectory\n",
    "    sub_directory_df = DataFrame([], columns=important_columns_list)\n",
    "    \n",
    "    # Iterate over the files in the current subdirectory\n",
    "    for file_name in files_list:\n",
    "        \n",
    "        # If the file is a CSV file, merge it into the subdirectory data frame\n",
    "        if file_name.endswith('.csv'):\n",
    "            # sub_directory_df = fu.process_files(sub_directory_df, sub_directory, file_name, verbose=verbose)\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = osp.join(sub_directory, file_name)\n",
    "            \n",
    "            # Attempt to read CSV file using pandas\n",
    "            try: file_df = read_csv(file_path, header=None, index_col=False)\n",
    "            \n",
    "            # If unsuccessful, try using a reader\n",
    "            except:\n",
    "                rows_list = []\n",
    "                with open(file_path, 'r') as f:\n",
    "                    import csv\n",
    "                    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "                    for values_list in reader:\n",
    "                        if (values_list[-1] == ''): values_list.pop(-1)\n",
    "                        rows_list.append({i: v for i, v in enumerate(values_list)})\n",
    "                file_df = DataFrame(rows_list)\n",
    "            \n",
    "            # Ignore small files and return the subdirectory data frame unharmed\n",
    "            if (file_df.shape[1] >= 16):\n",
    "                \n",
    "                # Add file name  to the data frame\n",
    "                file_dir_suffix = osp.abspath(sub_directory).replace(osp.abspath(logs_folder) + os.sep, '')\n",
    "                file_df['file_name'] = '/'.join(file_dir_suffix.split(os.sep)) + '/' + file_name\n",
    "                \n",
    "                # Name the global columns\n",
    "                columns_list = ['action_type', 'action_tick', 'event_time', 'session_uuid']\n",
    "                file_df.columns = columns_list + file_df.columns.tolist()[len(columns_list):]\n",
    "\n",
    "                # Remove all but the file name and session columns\n",
    "                file_df = file_df[important_columns_list].drop_duplicates()\n",
    "                \n",
    "                # Append the data frame for the current file to the data frame for the current subdirectory\n",
    "                sub_directory_df = concat([sub_directory_df, file_df], axis='index')\n",
    "    \n",
    "    # Append the data frame for the current subdirectory to the main data frame\n",
    "    file_names_and_session_uuids_df = concat([file_names_and_session_uuids_df, sub_directory_df], axis='index')\n",
    "    \n",
    "file_names_and_session_uuids_df = file_names_and_session_uuids_df.reset_index(drop=True)\n",
    "mask_series = file_names_and_session_uuids_df.session_uuid.isin(sim_session_uuids_list)\n",
    "df = file_names_and_session_uuids_df[mask_series]\n",
    "print(df.shape) # (43, 3)\n",
    "display(df.sample(5).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa1adf-23eb-40eb-8036-a4d817171780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "search_regex = re.compile('Urban', re.IGNORECASE)\n",
    "columns_list = nu.get_regexed_columns(json_stats_df, search_regex=search_regex)\n",
    "df = nu.get_regexed_dataframe(json_stats_df, columns_list, search_regex=search_regex)\n",
    "df.sample(4)[columns_list].dropna(axis='columns', how='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20978a-b8c0-446e-bf23-bcae82781269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = [\n",
    "    'encounter_layout', 'configData_narrative_narrativeDescription', 'configData_scene', 'configData_scenarioData_name', 'configData_scenarioData_description'\n",
    "]\n",
    "for cn in columns_list: display(json_stats_df.groupby(cn, dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1123d32-297d-4c8e-b083-756c8124453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_subgraph(sub_graph, suptitle='Within-function Function Calls', nodes_list_list=None, node_color='b', verbose=False):\n",
    "    \n",
    "    # Vertically separate the labels for easier readability\n",
    "    layout_items = nx.spring_layout(sub_graph).items()\n",
    "    left_lim, right_lim = -1500, 1500\n",
    "    bottom_lim, top_lim = left_lim * nu.twitter_aspect_ratio, right_lim * nu.twitter_aspect_ratio\n",
    "    rows_list = [{'node_name': node_name, 'layout_x': pos_array[0], 'layout_y': pos_array[1]} for node_name, pos_array in layout_items]\n",
    "    df = DataFrame(rows_list).sort_values('layout_x')\n",
    "    df['x_tick'] = [int(round(el)) for el in pd.cut(np.array([left_lim, right_lim]), len(sub_graph.nodes)+1, retbins=True)[1]][1:-1]\n",
    "    df = df.sort_values('layout_y')\n",
    "    df['y_tick'] = [int(round(el)) for el in pd.cut(np.array([bottom_lim, top_lim]), len(sub_graph.nodes)+1, retbins=True)[1]][1:-1]\n",
    "    \n",
    "    # Create the layout dictionary\n",
    "    layout_dict = {}\n",
    "    for row_index, row_series in df.iterrows():\n",
    "        node_name = row_series.node_name\n",
    "        layout_x = row_series.x_tick\n",
    "        layout_y = row_series.y_tick\n",
    "        layout_dict[node_name] = np.array([float(layout_x), float(layout_y)])\n",
    "    \n",
    "    # Draw the graph using the layout\n",
    "    fig = plt.figure(figsize=(18, 7), facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.axis('off')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    fig.suptitle(suptitle, fontsize=24)\n",
    "    \n",
    "    # Make the nodes the node_color\n",
    "    if nodes_list_list is None:\n",
    "        node_collection = nx.draw_networkx_nodes(\n",
    "            G=sub_graph, pos=layout_dict, alpha=0.33, node_color=node_color.reshape(1, -1), node_size=150\n",
    "        )\n",
    "        edge_collection = nx.draw_networkx_edges(\n",
    "            G=sub_graph, pos=layout_dict, alpha=0.25, width=[edge_tuple[2]['weight'] for edge_tuple in sub_graph.edges(data=True)]\n",
    "        )\n",
    "        labels_collection = nx.draw_networkx_labels(G=sub_graph, pos=layout_dict, font_size=10)\n",
    "    \n",
    "    # Color each nodes list differently\n",
    "    else:\n",
    "        if verbose: display(nodes_list_list)\n",
    "        color_cycler = nu.get_color_cycler(len(nodes_list_list))\n",
    "        for nodes_list, fcd in zip(nodes_list_list, color_cycler()):\n",
    "            if verbose: display(fcd['color'])\n",
    "            node_color = fcd['color'].reshape(1, -1)\n",
    "            sub_subgraph = nx.subgraph(sub_graph, nodes_list)\n",
    "            node_collection = nx.draw_networkx_nodes(G=sub_subgraph, pos=layout_dict, alpha=0.33, node_color=node_color, node_size=150)\n",
    "            edge_collection = nx.draw_networkx_edges(G=sub_subgraph, pos=layout_dict, alpha=0.25)\n",
    "            labels_collection = nx.draw_networkx_labels(G=sub_subgraph, pos=layout_dict, font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2fc50-49d9-4d4a-8eee-1d4374d4677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "# Create data structures to tally the nodes, edges, and weights\n",
    "edge_tuple_dict = {}\n",
    "for pair in itertools.combinations(columns_list, 2):\n",
    "    df = json_stats_df[list(pair)].drop_duplicates().sort_values(list(pair))\n",
    "    # print()\n",
    "    # print(*pair)\n",
    "    eval_list = []\n",
    "    for record_dict in df.dropna(axis='index', how='any').to_dict(orient='records'):\n",
    "        values_list = list(record_dict.values())\n",
    "        name_similarities_df = nu.check_for_typos(*[[v] for v in values_list])\n",
    "        # print(*values_list)\n",
    "        eval_list.append(1.0 if (values_list[0] in values_list[1]) else name_similarities_df.max_similarity.squeeze())\n",
    "    similarity_measure = np.min(eval_list)\n",
    "    edge_tuple_dict[pair] = similarity_measure#if similarity_measure > 0.08: \n",
    "\n",
    "# Create the directed graph\n",
    "dg = nx.DiGraph()\n",
    "dg.add_nodes_from(columns_list)\n",
    "dg.add_weighted_edges_from([(k[0], k[1], 10*v) for k, v in edge_tuple_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef4222-dfc9-4af4-ad74-9f7bd333d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "show_subgraph(dg, suptitle='Encouter Layout Features', nodes_list_list=None, node_color=np.array([0.4, 0.4, 0.4, 1.0]), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eca7f-14d3-43fe-8966-7375a7c29225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = ['configData_scene', 'configData_scenarioData_name', 'configData_scenarioData_description']\n",
    "display(json_stats_df[columns_list].drop_duplicates().sort_values(columns_list))\n",
    "display(json_stats_df.groupby(columns_list, dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74f725-4dc7-4df4-8b92-01064762dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_stats_df.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e0f04-3f32-49ea-b02c-6ca3d3815365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the unique count of each column\n",
    "for cn in sorted(json_stats_df.columns): print(cn, json_stats_df[cn].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61059cd7-0496-4da6-99c6-c389c9d0628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted(json_stats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60e35f-5321-47d2-9548-79f0855c6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(json_stats_df.groupby('responder_type').size().to_frame().rename(columns={0: 'record_count'}).sort_values(\n",
    "    'record_count', ascending=False\n",
    ").head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ed2a7-62dc-4593-9861-7113b3e8a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show all patient names\n",
    "old_patient_names_set = set([\n",
    "    'Adept Shooter Root', 'Adept Victim Root', 'Broken Bob Root', 'Broken Gloria Root', 'Broken Helga Root', 'Civilian 1 Female Root', 'Civilian 1 Root',\n",
    "    'Civilian 2 Root', 'Intelligence Officer Root', 'Local Civilian with Internal Bleeding Root', 'Local Soldier 1 Root', 'Marine 1 Male Root', 'Marine 2 Male Root',\n",
    "    'Marine 3 Male Root', 'Marine 4 Male Root', 'Marine with Leg Amputation Root', 'Marine with Narrative Root', 'NPC 1 Root', 'NPC 2 Root', 'NPC 3 Root',\n",
    "    'NPC 4 Root', 'NPC Root', 'Navy Soldier 1 Male Root', 'Navy Soldier 2 Male Root', 'Navy Soldier 3 Male Root', 'Navy Soldier 4 Female Root',\n",
    "    'Navy Solider 4 Female Root', 'Open World Civilian 1 Male Root', 'Open World Civilian 2 Female Root', 'Open World Marine 1 Female Root',\n",
    "    'Open World Marine 1 Male Root', 'Open World Marine 2 Female Root', 'Open World Marine 2 Male Root', 'Open World Marine 3 Male Root',\n",
    "    'Open World Marine 4 Male Root', 'Patient U Root', 'Patient V Root', 'Patient W Root', 'Patient X Root', 'Simulation Root', 'Tutorial Military Marine Root',\n",
    "    'US Soldier 1 Root', 'bystander Root', 'electrician Root', 'patient U Root', 'patient V Root', 'patient W Root', 'patient X Root'\n",
    "])\n",
    "mask_series = ~csv_stats_df.patient_id.isnull()\n",
    "all_patient_names = sorted(csv_stats_df[mask_series].patient_id.unique())\n",
    "old_patient_names_set.symmetric_difference(set(all_patient_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49a048-7157-4af1-bab1-1e6bef31250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show all the responder types in the registry\n",
    "df = data_frames_dict['first_responder_master_registry_file_stats_df']\n",
    "mask_series = ~df.responder_type.isnull()\n",
    "sorted(df[mask_series].responder_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89baae73-bad6-4a03-929d-e82461c9bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show if you have any \"file\" columns in the logs data frame\n",
    "sorted([cn for cn in csv_stats_df.columns if 'file' in cn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92f182-2bde-4369-9950-cb5a7a238bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to base the CACI data on the sessions in their spreadsheet\n",
    "print(logs_df.shape)\n",
    "file_path = '../data/xlsx/Metrics_Eval_Participant_data_for_BBAI.xlsx'\n",
    "participant_data_df = read_excel(file_path)\n",
    "uuid_columns = [f'Sim{i}' for i in range(1, 3)]\n",
    "uuid_fn = lambda x: str(x)[:-1] if str(x).endwith('_') else x\n",
    "json_stats_uuids = []\n",
    "for participant_id in range(2_024_201, 2_024_223+1):\n",
    "    mask_series = (participant_data_df.ParticipantID == participant_id)\n",
    "    if mask_series.any():\n",
    "        participant_df = participant_data_df[mask_series]\n",
    "        on_columns = set(json_stats_df.columns).intersection(set(participant_df.columns))\n",
    "        session_uuids_list = participant_df[uuid_columns].applymap(uuid_fn).values.ravel().tolist()\n",
    "        mask_series = json_stats_df.session_uuid.isin(session_uuids_list)\n",
    "        if mask_series.any(): json_stats_uuids.extend(json_stats_df[mask_series].session_uuid.tolist())\n",
    "    else: print(f'You are missing Participant ID #{participant_id} from the Metrics Eval Participant data for BBAI')\n",
    "print(logs_df.shape)\n",
    "# nu.store_objects(metrics_evaluation_open_world_json_stats_df=json_stats_df, verbose=True)\n",
    "# nu.save_data_frames(metrics_evaluation_open_world_json_stats_df=json_stats_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efa510-d21b-45ec-9c2d-0f0a6bac1dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "# Filter rows with non-null 'Date' values\n",
    "mask_series = ~json_stats_df.Date.isnull()\n",
    "filtered_df = json_stats_df[mask_series]\n",
    "\n",
    "# Get minimum and maximum dates as datetime objects\n",
    "min_date = filtered_df['Date'].min()\n",
    "max_date = filtered_df['Date'].max()\n",
    "\n",
    "# Format the datetime objects for human-readable output\n",
    "min_date_str = datetime.datetime.strptime(min_date, '%m/%d/%Y').strftime('%B %d, %Y')\n",
    "max_date_str = datetime.datetime.strptime(max_date, '%m/%d/%Y').strftime('%B %d, %Y')\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    f'A total of 22 participants, representing diverse medical roles, completed the ITM scenarios between {min_date_str} and {max_date_str}.'\n",
    "    ' Each participant engaged with two separate open world environments.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e509c-b934-46d6-a92f-4c0bff7cfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the open world action types\n",
    "import re\n",
    "import os\n",
    "\n",
    "action_type_regex = re.compile(\"^([^,\\r\\n]+),\", re.MULTILINE)\n",
    "file_ending = '.csv'\n",
    "logs_path = '../data/logs/Metrics Evaluation Open World'\n",
    "all_actions_set = set()\n",
    "for sub_directory, directories_list, files_list in os.walk(logs_path):\n",
    "    for file_name in files_list:\n",
    "        if file_name.endswith(file_ending):\n",
    "            file_path = osp.join(sub_directory, file_name)\n",
    "            with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                text = f.read()\n",
    "                actions_set = set(action_type_regex.findall(text))\n",
    "                all_actions_set.update(actions_set)\n",
    "\n",
    "old_actions_list = [\n",
    "    'BAG_ACCESS', 'BAG_CLOSED', 'INJURY_RECORD', 'INJURY_TREATED', 'PATIENT_DEMOTED', 'PATIENT_ENGAGED', 'PATIENT_RECORD', 'PULSE_TAKEN',\n",
    "    'S_A_L_T_WALKED', 'S_A_L_T_WALK_IF_CAN', 'S_A_L_T_WAVED', 'S_A_L_T_WAVE_IF_CAN', 'TAG_APPLIED', 'TAG_DISCARDED', 'TAG_SELECTED', 'TELEPORT',\n",
    "    'TOOL_APPLIED', 'TOOL_DISCARDED', 'TOOL_HOVER', 'TOOL_SELECTED', 'VOICE_CAPTURE', 'VOICE_COMMAND', 'PLAYER_LOCATION', 'PLAYER_GAZE',\n",
    "    'SESSION_END', 'SESSION_START'\n",
    "]\n",
    "new_actions_list = sorted(all_actions_set.difference(set(old_actions_list)))\n",
    "print(new_actions_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1341223-5c53-4423-9118-0a502cd2b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up notebook\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df1a687-f72f-4765-b68d-fe154761a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "from FRVRS import nu, fu\n",
    "from datetime import date, timedelta\n",
    "from numpy import nan, isnan\n",
    "from os import listdir as listdir, makedirs as makedirs, path as osp, remove as remove, sep as sep, walk as walk\n",
    "from pandas import (\n",
    "    CategoricalDtype, DataFrame, Index, NaT, Series, concat, get_dummies, isna, notnull, read_csv, read_excel,\n",
    "    to_datetime, to_numeric\n",
    ")\n",
    "from re import split, search, sub, MULTILINE\n",
    "from scipy.stats import f_oneway, ttest_ind, kruskal, norm\n",
    "import itertools\n",
    "from matplotlib import colors as colors, pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163ee110-4921-474e-884b-907bd36007c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for 20240521truncated_csv_stats_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/20240521truncated_csv_stats_df.csv.\n",
      "(158663, 117)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    **{'verbose': True, '20240521truncated_csv_stats_df': ''}\n",
    ")\n",
    "csv_stats_df = data_frames_dict['20240521truncated_csv_stats_df'].copy()\n",
    "print(csv_stats_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b52b98-b35b-4664-91e2-ddafbbab4885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encounter_layout</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desert</th>\n",
       "      <td>40678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>25582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submarine</th>\n",
       "      <td>50198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urban</th>\n",
       "      <td>42150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  record_count\n",
       "encounter_layout              \n",
       "Desert                   40678\n",
       "Jungle                   25582\n",
       "Submarine                50198\n",
       "Urban                    42150\n",
       "NaN                         55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Add the sim environment column\n",
    "new_column_name = 'encounter_layout'\n",
    "encounter_layouts_list = ['Desert', 'Jungle', 'Submarine', 'Urban']\n",
    "for (session_uuid, scene_id), scene_df in csv_stats_df.groupby(fu.scene_groupby_columns):\n",
    "    mask_series = ~scene_df.patient_id.isnull()\n",
    "    spl = sorted(scene_df[mask_series].patient_id.unique())\n",
    "    for env_str in encounter_layouts_list:\n",
    "\n",
    "        # Assume no Unity suffix\n",
    "        patients_list = eval(f'fu.{env_str.lower()}_patients_list')\n",
    "        if all(map(lambda p: p in spl, patients_list)):\n",
    "            \n",
    "            csv_stats_df.loc[scene_df.index, new_column_name] = env_str\n",
    "display(csv_stats_df.groupby([new_column_name], dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b32534-c3f8-4eab-8005-c1956b16faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_stats_df\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for df_name in dir():\n",
    "    if df_name.endswith('_df'):\n",
    "        if 'encounter_layout' in eval(f'{df_name}.columns'):\n",
    "            print(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d70e0756-7380-41d3-9b6a-7105e041051e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35520    Desert\n",
       "35521    Desert\n",
       "35522    Desert\n",
       "35523    Desert\n",
       "35524    Desert\n",
       "          ...  \n",
       "35592    Desert\n",
       "35593    Desert\n",
       "35594    Desert\n",
       "35595    Desert\n",
       "35597    Desert\n",
       "Name: encounter_layout, Length: 72, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mask_series = ~session_df.encounter_layout.isnull()\n",
    "session_df[mask_series].encounter_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22111a58-3cd8-4dcd-bd13-5abd9148785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.131, 0, -28.682)\n",
      "[(0.3, 0.0, -7.3)]\n",
      "(0.3, 0.0, -7.3)\n",
      "22.770913134962328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(base_point)\n",
    "print(neighbors_list)\n",
    "print(nearest_neighbor)\n",
    "print(euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892da058-dfb3-40aa-93dd-afd7a92dfc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type</th>\n",
       "      <th>action_tick</th>\n",
       "      <th>location_id</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>encounter_layout</th>\n",
       "      <th>session_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35563</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1369817</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35564</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1370395</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35565</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1372069</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35566</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1377061</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35567</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1378242</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35568</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1382824</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35582</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1443694</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35583</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1451294</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35585</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1461607</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35597</th>\n",
       "      <td>TELEPORT</td>\n",
       "      <td>1497960</td>\n",
       "      <td>(0.3, 0.0, -7.3)</td>\n",
       "      <td>2024201</td>\n",
       "      <td>Desert</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      action_type  action_tick       location_id  participant_id  \\\n",
       "35563    TELEPORT      1369817  (0.3, 0.0, -7.3)         2024201   \n",
       "35564    TELEPORT      1370395  (0.3, 0.0, -7.3)         2024201   \n",
       "35565    TELEPORT      1372069  (0.3, 0.0, -7.3)         2024201   \n",
       "35566    TELEPORT      1377061  (0.3, 0.0, -7.3)         2024201   \n",
       "35567    TELEPORT      1378242  (0.3, 0.0, -7.3)         2024201   \n",
       "35568    TELEPORT      1382824  (0.3, 0.0, -7.3)         2024201   \n",
       "35582    TELEPORT      1443694  (0.3, 0.0, -7.3)         2024201   \n",
       "35583    TELEPORT      1451294  (0.3, 0.0, -7.3)         2024201   \n",
       "35585    TELEPORT      1461607  (0.3, 0.0, -7.3)         2024201   \n",
       "35597    TELEPORT      1497960  (0.3, 0.0, -7.3)         2024201   \n",
       "\n",
       "      encounter_layout                          session_uuid  \n",
       "35563           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35564           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35565           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35566           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35567           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35568           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35582           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35583           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35585           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  \n",
       "35597           Desert  23081f6e-875e-44f5-8bd0-edc3905f5c2c  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['action_type', 'action_tick', 'location_id', 'participant_id', 'encounter_layout', 'session_uuid']\n",
    "session_df[mask_series][columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b42f3d-cea5-4fe7-80c8-733d465fd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the Open World JSON Stats into one data frame\n",
    "logs_path = osp.join(nu.data_folder, 'logs', 'Human_Sim_Metrics_Data_4-12-2024')\n",
    "directories_list = listdir(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e58513-1c14-4ae9-9f82-bb766e912bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_stats_df = DataFrame([])\n",
    "for dir_name in directories_list:\n",
    "    \n",
    "    # Add the JSONs to the data frame\n",
    "    folder_path = osp.join(logs_path, dir_name)\n",
    "    \n",
    "    # Iterate over the files in the current subdirectory\n",
    "    for file_name in listdir(folder_path):\n",
    "        \n",
    "        # If the file is a JSON file\n",
    "        if file_name.endswith('.json'):\n",
    "            \n",
    "            # Create a data frame from the flattened dictionary\n",
    "            json_path = osp.join(folder_path, file_name)\n",
    "            with open(json_path, 'r') as f: file_json = json.load(f)\n",
    "            row_dict = {\n",
    "                'json_file_subpath': folder_path,\n",
    "                'json_file_name': file_name\n",
    "            }\n",
    "            flattened_json_dict = nu.get_row_dictionary(file_json, row_dict=row_dict, key_prefix='')\n",
    "            \n",
    "            # You've got to clean the Session IDs\n",
    "            session_uuid, participant_id = dir_name.split('_')\n",
    "            flattened_json_dict['session_uuid'] = session_uuid\n",
    "            flattened_json_dict['participant_id'] = int(participant_id)\n",
    "            df = DataFrame(flattened_json_dict, index=Index([0]))\n",
    "            \n",
    "            # Append the data frame for the current subdirectory to the main data frame and break the participant ID loop\n",
    "            json_stats_df = concat([json_stats_df, df], axis='index')\n",
    "            \n",
    "json_stats_df = json_stats_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6d72a-6e96-4635-be46-099740aec1ad",
   "metadata": {},
   "source": [
    "\n",
    "### Drop boring columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6c750-1e92-4440-8108-79d91eea964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for proper ingestion (duplicate file ingestion, et al)\n",
    "assert len(json_stats_df.columns) > 4, \"Nothing ingested\"\n",
    "assert json_stats_df.participant_id.nunique() == 26, f\"Participant count should be 26, it's {json_stats_df.participant_id.nunique()} instead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4abc9-e6a2-49f5-90d5-c1a71a3d96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the json stats dataset\n",
    "nu.save_data_frames(**{f'{today.year}{today.month:02d}{today.day:02d}json_stats_df': json_stats_df}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df46ce-ba21-49fd-bdf0-2d1c56fd71c7",
   "metadata": {},
   "source": [
    "\n",
    "### Get the nearest neighbor location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315660a3-e9ea-4e64-9924-cc19bbf4afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_action_tick_by_position(csv_stats_df, json_stats_df, participant_id, session_uuid, include_y=False):\n",
    "    on_columns = ['participant_id', 'session_uuid']\n",
    "    \n",
    "    # Get the location (CSV) mask\n",
    "    location_columns = on_columns + ['player_location_location']\n",
    "    assert set(location_columns).issubset(\n",
    "        set(csv_stats_df.columns)\n",
    "    ), f\"csv_stats_df is missing these location columns: {set(location_columns).difference(set(csv_stats_df.columns))}\"\n",
    "    location_mask_series = ~csv_stats_df.player_location_location.isnull()\n",
    "    assert location_mask_series.any(), f\"You don't have any player location data in csv_stats_df\"\n",
    "    location_session_mask = (csv_stats_df.participant_id == participant_id) & (csv_stats_df.session_uuid == session_uuid)\n",
    "    assert location_session_mask.any(), f\"You don't have any CSV data for participant #{participant_id} session #{session_uuid}\"\n",
    "    location_mask_series &= location_session_mask\n",
    "    assert location_mask_series.any(), f\"You don't have any player location data for participant #{participant_id} session #{session_uuid}\"\n",
    "    \n",
    "    # Get the position (JSON) mask\n",
    "    position_prefix_str = 'configData_playerData_startPosition_'\n",
    "    if include_y:\n",
    "        xyz_list = ['x', 'y', 'z']\n",
    "    else:\n",
    "        xyz_list = ['x', 'z']\n",
    "    position_columns = on_columns + [f'{position_prefix_str}{d}' for d in xyz_list]\n",
    "    assert set(position_columns).issubset(\n",
    "        set(json_stats_df.columns)\n",
    "    ), f\"json_stats_df is missing these columns: {set(position_columns).difference(set(json_stats_df.columns))}\"\n",
    "    position_session_mask = (json_stats_df.participant_id == participant_id) & (json_stats_df.session_uuid == session_uuid)\n",
    "    assert position_session_mask.any(), f\"You don't have any JSON data for participant #{participant_id} session #{session_uuid}\"\n",
    "    \n",
    "    # Get the base point (JSON)\n",
    "    mask_series = False\n",
    "    mask_series |= json_stats_df.configData_narrative_narrativeSections11_teleportPointsToActivate.isin(['Freeform'])\n",
    "    mask_series |= json_stats_df.configData_narrative_narrativeSections12_teleportPointsToActivate.isin(['Freeform'])\n",
    "    mask_series |= json_stats_df.configData_narrative_narrativeSections13_teleportPointsToActivate.isin(['Freeform'])\n",
    "    df = json_stats_df[position_session_mask]\n",
    "    position_x = df[mask_series][f'{position_prefix_str}x'].squeeze()\n",
    "    if include_y:\n",
    "        position_y = df[mask_series][f'{position_prefix_str}y'].squeeze()\n",
    "    position_z = df[mask_series][f'{position_prefix_str}z'].squeeze()\n",
    "    if include_y:\n",
    "        base_point = (position_x, position_y, position_z)\n",
    "    else:\n",
    "        base_point = (position_x, position_z)\n",
    "    \n",
    "    # Create the left side of the merge\n",
    "    csv_locations_df = csv_stats_df[location_mask_series][location_columns]\n",
    "    for i, d in enumerate(xyz_list):\n",
    "        csv_locations_df[f'location_{d}'] = csv_locations_df.player_location_location.map(lambda coord_str: eval(coord_str)[i])\n",
    "    csv_locations_df = csv_locations_df.drop(columns=['player_location_location']).drop_duplicates()\n",
    "    \n",
    "    # Get the right side of the merge\n",
    "    mask_series = ~json_stats_df[f'{position_prefix_str}x'].isnull()\n",
    "    if include_y:\n",
    "        mask_series &= ~json_stats_df[f'{position_prefix_str}y'].isnull()\n",
    "    mask_series &= ~json_stats_df[f'{position_prefix_str}z'].isnull()\n",
    "    assert set(position_columns).issubset(\n",
    "        set(json_stats_df.columns)\n",
    "    ), f\"You're missing {set(position_columns).intersection(set(json_stats_df.columns))} as position columns\"\n",
    "    prefix_str = 'configData_playerData_startP'\n",
    "    json_positions_df = json_stats_df[mask_series][position_columns].rename(\n",
    "        columns={cn: cn.replace(prefix_str, 'p') for cn in position_columns if cn.startswith(prefix_str)}\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # Merge the two coordinate datasets\n",
    "    on_columns = sorted(set(csv_locations_df.columns).intersection(set(json_positions_df.columns)))\n",
    "    print(csv_locations_df.shape)\n",
    "    print(json_positions_df.shape)\n",
    "    merge_df = csv_locations_df.merge(json_positions_df, on=on_columns, how='inner').drop_duplicates()\n",
    "    print(merge_df.shape)\n",
    "    \n",
    "    # Get the neighbors list\n",
    "    neighbors_list = [(x, y, z) for (x, y, z), _ in csv_locations_df.groupby(['location_x', 'location_y', 'location_z'])]\n",
    "    nearest_neighbor = nu.get_nearest_neighbor(base_point, neighbors_list)\n",
    "    euclidean_distance = nu.get_euclidean_distance(base_point, nearest_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790c6db-b30e-42e6-9e4a-2a72fa1c0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_json_df = DataFrame([])\n",
    "folder_path = osp.join(fu.data_folder, 'json')\n",
    "\n",
    "# Iterate over the files in the current subdirectory\n",
    "for file_name in listdir(folder_path):\n",
    "    \n",
    "    # If the file is a JSON file\n",
    "    if file_name.endswith('.json'):\n",
    "        \n",
    "        # Create a data frame from the flattened dictionary\n",
    "        json_path = osp.join(folder_path, file_name)\n",
    "        with open(json_path, 'r') as f: file_json = json.load(f)\n",
    "        row_dict = {\n",
    "            'json_file_name': file_name\n",
    "        }\n",
    "        flattened_json_dict = nu.get_row_dictionary(file_json, row_dict=row_dict, key_prefix='')\n",
    "        df = DataFrame(flattened_json_dict, index=Index([0]))\n",
    "        \n",
    "        # Append the data frame for the current subdirectory to the main data frame and break the participant ID loop\n",
    "        eval_json_df = concat([eval_json_df, df], axis='index')\n",
    "        \n",
    "eval_json_df = eval_json_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5dc53-d0f1-4125-baf2-e893c94673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_json_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9425a-ca98-4478-8e28-3f2d13195d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the teleport columns\n",
    "mask_series = eval_json_df.columns.map(lambda cn: '_playerTeleportation_teleportPosition_' in cn)\n",
    "teleport_columns = eval_json_df.columns[mask_series]\n",
    "if len(teleport_columns) > 2:\n",
    "    for column_name in sorted(teleport_columns):\n",
    "        print(column_name)\n",
    "elif len(teleport_columns):\n",
    "    display(eval_json_df[teleport_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43269b87-87fb-4f03-a530-87329b5f661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the teleport columns\n",
    "mask_series = eval_json_df.columns.map(lambda cn: 'teleportPointsToActivate' in cn)\n",
    "teleport_columns = eval_json_df.columns[mask_series]\n",
    "if len(teleport_columns) > 2:\n",
    "    for column_name in sorted(teleport_columns):\n",
    "        print(column_name)\n",
    "elif len(teleport_columns):\n",
    "    display(eval_json_df[teleport_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b60296-110f-4b1b-bbcf-261065fea505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the teleport point names\n",
    "for i in range(14):\n",
    "    column_name = f'narrative_narrativeSections{i:02d}_teleportPointsToActivate'\n",
    "    if column_name in eval_json_df:\n",
    "        mask_series = ~eval_json_df[column_name].isnull()\n",
    "        print( column_name, sorted(eval_json_df[mask_series][column_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344089d8-edea-4cf9-94dc-dfd59cfb7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (participant_id, session_uuid), session_df in csv_stats_df.groupby(['participant_id', 'session_uuid']):\n",
    "    mask_series = (session_df.action_type == 'TELEPORT') & ~session_df.location_id.isnull()\n",
    "    neighbors_list = [eval(location_id) for location_id in session_df[mask_series].location_id]\n",
    "    nearest_neighbor = nu.get_nearest_neighbor(base_point, neighbors_list)\n",
    "    euclidean_distance = nu.get_euclidean_distance(base_point, nearest_neighbor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473bc55-8abf-4d72-8d4d-dd9f0f7da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b308fdd-abc2-488a-b0e5-28e9fc727947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = session_df.location_id.isin([str(nearest_neighbor)])\n",
    "df = session_df[mask_series]\n",
    "print(sorted(df.action_tick.unique()))\n",
    "print(sorted(df.action_type.unique()))\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b475a-c007-42b6-bf28-27650fd8d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Submarine: (0.02, 0, -13.5)\n",
    "# Urban: (13.126, 0, 21.61)\n",
    "# Jungle: (0.7, 0, 5.45)\n",
    "# Desert: (8.131, 0, -28.682)\n",
    "activate_columns = eval_json_df.columns[eval_json_df.columns.map(\n",
    "    lambda cn: bool(re.search(r'narrative_narrativeSections\\d+_teleportPointsToActivate', cn))\n",
    ")]\n",
    "mask_series = False\n",
    "for cn in activate_columns:\n",
    "    mask_series |= eval_json_df[cn].isin(['Freeform'])\n",
    "teleport_columns = eval_json_df.columns[eval_json_df.columns.map(\n",
    "    lambda cn: bool(re.search(r'narrative_narrativeSections\\d+_playerTeleportation_teleportPosition_(x|y|z)', cn))\n",
    ")]\n",
    "columns_list = sorted(eval_json_df[mask_series][teleport_columns].dropna(axis='columns', how='all').columns)\n",
    "description_columns = [cn for cn in sorted(eval_json_df.columns[eval_json_df.columns.map(\n",
    "    lambda cn: bool(re.search(r'narrative_narrativeSections\\d+_sectionDescription', cn))\n",
    ")]) if cn.replace('_sectionDescription', '_playerTeleportation_teleportPosition_x') in columns_list]\n",
    "activate_columns = [cn for cn in sorted(eval_json_df.columns[eval_json_df.columns.map(\n",
    "    lambda cn: bool(re.search(r'narrative_narrativeSections\\d+_teleportPointsToActivate', cn))\n",
    ")]) if cn.replace('_teleportPointsToActivate', '_playerTeleportation_teleportPosition_x') in columns_list]\n",
    "columns_list = sorted(columns_list + description_columns + activate_columns) + ['json_file_name']\n",
    "eval_json_df[mask_series][columns_list].dropna(axis='index', how='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33038e-994c-4fd6-867c-540266f7421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_json_df[mask_series].narrative_narrativeSections14_sectionDescription.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb17f3-e4bc-4ad5-a9d0-3a0921208df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the teleport point names\n",
    "for i in range(14):\n",
    "    column_name = f'configData_narrative_narrativeSections{i:02d}_teleportPointsToActivate'\n",
    "    if column_name in json_stats_df:\n",
    "        mask_series = ~json_stats_df[column_name].isnull()\n",
    "        print( column_name, sorted(json_stats_df[mask_series][column_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775e6c0-d4eb-470d-8418-2be06e9e8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get list of all start position x, y, and z coordinates\n",
    "prefix_str = 'configData_playerData_startPosition_'\n",
    "print(f\"\\nThese are the values I can see for {' '.join(prefix_str.split('_')).strip()}:\")\n",
    "xyz_list = ['x', 'y', 'z']\n",
    "for cn in [f'{prefix_str}{dim}' for dim in xyz_list]:\n",
    "    position_mask_series = ~json_stats_df[cn].isnull()\n",
    "    print(cn.split('_')[-1], set(json_stats_df[position_mask_series][cn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26015461-564d-4542-b6dc-5c3f51165b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = ['configData_teleportPointOverride']\n",
    "display(json_stats_df[columns_list].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d7dd4-be87-414c-97c8-aaad9d9f4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the narrativeConfig columns\n",
    "mask_series = json_stats_df.columns.map(lambda cn: 'narrative' in cn)\n",
    "config_columns = json_stats_df.columns[mask_series]\n",
    "if len(config_columns) > 2:\n",
    "    for column_name in sorted(config_columns):\n",
    "        print(column_name)\n",
    "elif len(config_columns):\n",
    "    display(json_stats_df[config_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436399d8-ee3e-4190-9085-36b06439eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get columns that contain an x or z coordinate\n",
    "xyz_list = ['x', 'y', 'z']\n",
    "mask_series = json_stats_df.columns.map(lambda cn: any(map(lambda dim: cn.endswith(f'_{dim}'), xyz_list)))\n",
    "xyz_columns = json_stats_df.columns[mask_series]\n",
    "# print(xyz_columns)\n",
    "location_regex = re.compile(r'-?\\d+\\.\\d*')\n",
    "location_columns = sorted(nu.get_regexed_columns(json_stats_df[xyz_columns], search_regex=location_regex))\n",
    "if len(location_columns) > 2:\n",
    "    for column_name in sorted(location_columns):\n",
    "        print(column_name)\n",
    "elif len(location_columns):\n",
    "    display(json_stats_df[location_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0308a-5574-4853-ba34-bebb7165918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "location_mask_series = ~csv_stats_df.player_location_location.isnull()\n",
    "location_columns = on_columns + ['player_location_location']\n",
    "assert set(location_columns).issubset(set(csv_stats_df.columns)), \"You've got the wrong location columns\"\n",
    "left_df = csv_stats_df[location_mask_series][location_columns]\n",
    "for i, d in enumerate(xyz_list):\n",
    "    left_df[f'location_{d}'] = left_df.player_location_location.map(lambda coord_str: eval(coord_str)[i])\n",
    "left_df = left_df.drop(columns=['player_location_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3922ee-75a0-458f-ade0-b3687da15011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "right_gb = right_df.groupby(on_columns)\n",
    "left_gb = left_df.groupby(on_columns)\n",
    "tuples_list = sorted(set(right_gb.groups.keys()).intersection(set(left_gb.groups.keys())))\n",
    "location_groupby_columns = [f'location_{d}' for d in xyz_list]\n",
    "rows_list = []\n",
    "for key_tuple in tuples_list:\n",
    "    position_mask_series = (right_df.participant_id == key_tuple[0]) & (right_df.session_uuid == key_tuple[1])\n",
    "    position_x = right_df[position_mask_series].position_x.squeeze()\n",
    "    position_y = right_df[position_mask_series].position_y.squeeze()\n",
    "    position_z = right_df[position_mask_series].position_z.squeeze()\n",
    "    # display(right_df[position_mask_series].drop_duplicates())\n",
    "    base_point = (position_x, position_y, position_z)\n",
    "    \n",
    "    location_mask_series = (left_df.participant_id == key_tuple[0]) & (left_df.session_uuid == key_tuple[1])\n",
    "    neighbors_list = [(x, y, z) for (x, y, z), _ in left_df[location_mask_series].groupby(location_groupby_columns)]\n",
    "    nearest_neighbor = nu.get_nearest_neighbor(base_point, neighbors_list)\n",
    "    euclidean_distance = nu.get_euclidean_distance(base_point, nearest_neighbor)\n",
    "    row_dict = {\n",
    "        'base_point': base_point,\n",
    "        'nearest_neighbor': nearest_neighbor,\n",
    "        'euclidean_distance': euclidean_distance,\n",
    "    }\n",
    "    base_point = (position_x, position_z)\n",
    "    neighbors_list = [(x, z) for (x, y, z), _ in left_df[location_mask_series].groupby(location_groupby_columns)]\n",
    "    nearest_neighbor = nu.get_nearest_neighbor(base_point, neighbors_list)\n",
    "    euclidean_distance = nu.get_euclidean_distance(base_point, nearest_neighbor)\n",
    "    row_dict['euclidean_distance_xz'] = euclidean_distance\n",
    "    rows_list.append(row_dict)\n",
    "euclidean_distance_df = DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e6692-8bf0-4981-ab19-8fb68910a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [cn for cn in df.columns if 'position' in cn.lower()]\n",
    "mask_series = (df.configData_narrative_narrativeSections00_sectionDescription == 'Initialization')\n",
    "# df.configData_scene.unique() == ['sim-urban-sanitized', 'sim-desert', 'sim-jungle']\n",
    "df = df[mask_series].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b60be-fa98-49dd-af29-1c2d39e48bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the standard deviation for each column\n",
    "std_dev = df.std()\n",
    "\n",
    "# Get the indices of columns with zero variance\n",
    "zero_variance_cols = std_dev[std_dev == 0].index\n",
    "\n",
    "# Remove columns with zero variance\n",
    "column_count = df.shape[1]\n",
    "df.drop(columns=zero_variance_cols, inplace=True)\n",
    "print(f\"Dropped {int(column_count - df.shape[1])} JSON columns with zero variance\")\n",
    "\n",
    "# Assess the number of unique values (excluding NaNs) in each column\n",
    "unique_values = df.nunique(dropna=True)\n",
    "\n",
    "# Isolate the indices of columns with fewer than two unique values\n",
    "few_unique_cols = unique_values[unique_values < 2].index\n",
    "\n",
    "# Remove columns with fewer than two unique values\n",
    "column_count = df.shape[1]\n",
    "df.drop(columns=few_unique_cols, inplace=True)\n",
    "print(f\"Dropped {int(column_count - df.shape[1])} JSON columns with fewer than two unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c21739-96ac-4520-a2cb-0dc269dfe332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468a729-233e-4987-b508-b5ca7cd71bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "free_regex = re.compile('free', re.IGNORECASE)\n",
    "free_columns = sorted(nu.get_regexed_columns(json_stats_df, search_regex=free_regex))\n",
    "if len(free_columns) > 2:\n",
    "    for column_name in sorted(free_columns):\n",
    "        print(column_name)\n",
    "else:\n",
    "    display(json_stats_df[free_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621535c-bd1b-4850-ab92-9421eaa266e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adept_regex = re.compile('adept', re.IGNORECASE)\n",
    "adept_columns = sorted(nu.get_regexed_columns(json_stats_df, search_regex=adept_regex))\n",
    "if len(adept_columns) > 2:\n",
    "    for column_name in sorted(adept_columns):\n",
    "        print(column_name)\n",
    "else:\n",
    "    display(json_stats_df[adept_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03446ff9-e84d-4ee2-bb84-081bfcd85a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soartech_regex = re.compile('soartech', re.IGNORECASE)\n",
    "soartech_columns = sorted(nu.get_regexed_columns(json_stats_df, search_regex=soartech_regex))\n",
    "if len(soartech_columns) > 2:\n",
    "    for column_name in sorted(soartech_columns):\n",
    "        print(column_name)\n",
    "else:\n",
    "    display(json_stats_df[soartech_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155c18c-b283-4d3e-866f-cc0b1e98a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column_name in sorted(set(adept_columns).intersection(set(soartech_columns))):\n",
    "    print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df0cbe-0b3d-4a19-a263-214f0ecd4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(14):\n",
    "    column_name = f'configData_narrative_narrativeSections{i:02d}_sectionDescription'\n",
    "    if column_name in json_stats_df:\n",
    "        mask_series = ~json_stats_df[column_name].isnull()\n",
    "        print(column_name, sorted(json_stats_df[mask_series][column_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336269d-60a0-43f3-b941-68d8f3d30db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column_name in sorted([cn for cn in json_stats_df.columns if any(map(lambda x: x in cn.lower(), ['start', 'intro', 'free', 'form']))]):\n",
    "    print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b273c68-c035-45a1-8c79-e230c8aaba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column_name in sorted([cn for cn in json_stats_df.columns if any(map(lambda x: x in cn.lower(), ['position']))]):\n",
    "    print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae7ebc-c03c-42d7-8b43-57a8158faa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted(fu.location_id_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3cf81-55a3-437c-91fe-1f010ff31e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_dict = nu.load_data_frames(\n",
    "    verbose=True, metrics_evaluation_open_world_csv_stats_df=''\n",
    ")\n",
    "csv_stats_df = data_frames_dict['metrics_evaluation_open_world_csv_stats_df'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60c669-b278-491a-a2ab-0931c8409442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location_regex = re.compile(r'\\((?:-?\\d{1,2}\\.\\d, ){2}-?\\d{1,2}\\.\\d\\)')\n",
    "location_columns = sorted(nu.get_regexed_columns(csv_stats_df, search_regex=location_regex))\n",
    "if len(location_columns) > 2:\n",
    "    for column_name in sorted(location_columns):\n",
    "        print(column_name)\n",
    "elif len(location_columns):\n",
    "    display(json_stats_df[location_columns].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae1ee4-5a99-404f-b020-40af27405997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns to merge the JSON stats dataset with the CSV stats on\n",
    "on_columns = sorted(set(csv_stats_df.columns).intersection(set(json_stats_df.columns)))\n",
    "print(on_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c36063-6b97-4f1f-ad4b-55994d50158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted(right_df.position_y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c697e2-f641-47dc-911e-d4d08fc31aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted(left_df.location_y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b54117-692b-40e9-b746-3b8b7642b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "# Create a box plot of all the columns\n",
    "columns_list = ['euclidean_distance_xz']\n",
    "sns.boxplot(euclidean_distance_df[columns_list], ax=ax)\n",
    "\n",
    "# Label the y-axis\n",
    "ax.set_ylabel('Nearest Horizontal Distance (Euclidean)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca2478-9c30-4f02-a765-00ce983a6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "# Create a box plot of all the columns\n",
    "mask_series = (euclidean_distance_df.euclidean_distance_xz < 20)\n",
    "columns_list = ['euclidean_distance_xz']\n",
    "sns.boxplot(euclidean_distance_df[mask_series][columns_list], ax=ax)\n",
    "\n",
    "# Label the y-axis\n",
    "ax.set_ylabel('Nearest Horizontal Distance (Euclidean)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ab37c-b909-4a2f-9e20-165a4ccbb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "euclidean_distance_df.sort_values('euclidean_distance_xz', ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

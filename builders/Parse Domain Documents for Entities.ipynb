{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51ab3c6-9bc9-4299-86d6-64763b66e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c96aaf-34eb-4e78-b477-d2e6bd721e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import nu\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=nu,\n",
    "    secrets_json_path=osp.abspath(osp.join(nu.data_folder, 'secrets', 'itm_secrets.json'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ffe8e-6573-470f-aef5-348522a0de15",
   "metadata": {},
   "source": [
    "\n",
    "# Parse Domain Documents for Entities\n",
    "\n",
    "Downloaded all documents from https://nextcentury.atlassian.net/wiki/spaces/ITMC/pages/2991849482/Domain+Documents and converted them all to PDF files and stored them in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea43e5c-ac11-4453-accc-78c7f84e4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "pdf_folder = '../data/Domain_Knowledge'\n",
    "black_list = ['.ipynb_checkpoints', '$Recycle.Bin', '.git']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa97bb-59d8-41c5-8bed-22df8c53840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display all the files we ingested\n",
    "for sub_directory, directories_list, files_list in os.walk(pdf_folder):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "        if any(map(lambda x: x.endswith('.txt'), files_list)):\n",
    "            for file_name in files_list:\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = osp.join(sub_directory, file_name)\n",
    "                    print(file_path.replace('../data/Domain_Knowledge/', ''))\n",
    "        else:\n",
    "            for file_name in files_list:\n",
    "                if file_name.endswith('.pdf'):\n",
    "                    file_path = osp.join(sub_directory, file_name)\n",
    "                    print(file_path.replace('../data/Domain_Knowledge/', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053cef1-4571-4abd-9868-2b0de4bf37de",
   "metadata": {},
   "source": [
    "\n",
    "## Option 1: Use a Hugging Face NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da6aee2-cb75-4121-9285-7dd8fa91887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9988381, 'index': 1, 'word': 'Barack', 'start': 0, 'end': 6}, {'entity': 'I-PER', 'score': 0.9994398, 'index': 2, 'word': 'Obama', 'start': 7, 'end': 12}, {'entity': 'I-LOC', 'score': 0.9983613, 'index': 10, 'word': 'United', 'start': 43, 'end': 49}, {'entity': 'I-LOC', 'score': 0.9920671, 'index': 11, 'word': 'States', 'start': 50, 'end': 56}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Named entity recognition pipeline, passing in a specific model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "token_classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "sentence = 'Barack Obama was the 44th President of the United States.'\n",
    "tokens = token_classifier(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2547e-6a16-4772-b317-eacac8a443c9",
   "metadata": {},
   "source": [
    "\n",
    "## Option 2: Use SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "003f66f0-f67f-47f2-88eb-b2bbb6d33791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Barack', 'tag_': 'NNP', 'ent_type_': 'PERSON', 'pos_': 'PROPN'}, {'text': 'Obama', 'tag_': 'NNP', 'ent_type_': 'PERSON', 'pos_': 'PROPN'}, {'text': 'was', 'tag_': 'VBD', 'ent_type_': '', 'pos_': 'AUX'}, {'text': 'the', 'tag_': 'DT', 'ent_type_': '', 'pos_': 'DET'}, {'text': '44th', 'tag_': 'JJ', 'ent_type_': 'ORDINAL', 'pos_': 'ADJ'}, {'text': 'President', 'tag_': 'NNP', 'ent_type_': '', 'pos_': 'PROPN'}, {'text': 'of', 'tag_': 'IN', 'ent_type_': '', 'pos_': 'ADP'}, {'text': 'the', 'tag_': 'DT', 'ent_type_': 'GPE', 'pos_': 'DET'}, {'text': 'United', 'tag_': 'NNP', 'ent_type_': 'GPE', 'pos_': 'PROPN'}, {'text': 'States', 'tag_': 'NNP', 'ent_type_': 'GPE', 'pos_': 'PROPN'}, {'text': '.', 'tag_': '.', 'ent_type_': '', 'pos_': 'PUNCT'}]\n",
      "[{'text': 'Barack Obama', 'label_': 'PERSON'}, {'text': '44th', 'label_': 'ORDINAL'}, {'text': 'the United States', 'label_': 'GPE'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "try: nlp = spacy.load('en_core_web_sm')\n",
    "except OSError as e:\n",
    "    print(str(e).strip())\n",
    "    command_str = f'{sys.executable} -m spacy download en_core_web_sm --quiet'\n",
    "    print(command_str)\n",
    "    !{command_str}\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Example usage\n",
    "sentence = 'Barack Obama was the 44th President of the United States.'\n",
    "doc = nlp(sentence)\n",
    "print([{'text': word.text, 'tag_': word.tag_, 'ent_type_': word.ent_type_, 'pos_': word.pos_} for word in doc])\n",
    "print([{'text': ent.text, 'label_': ent.label_} for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba76ba4-1188-4912-80b4-c9cb20ec01f0",
   "metadata": {},
   "source": [
    "\n",
    "## Extract the text from PDFs and load it into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3655159d-289f-4306-b6ac-6c70ee1d02c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/domain_knowledge_sentences_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get text from PDFs\n",
    "if nu.pickle_exists('domain_knowledge_sentences_dict'):\n",
    "    domain_knowledge_sentences_dict = nu.load_object('domain_knowledge_sentences_dict')\n",
    "else:\n",
    "    from PyPDF2 import PdfReader\n",
    "    def convert(file_path, verbose=False):\n",
    "        \"\"\"\n",
    "        Convert PDF, return its text content as a string\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            for page_number in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_number]\n",
    "                text += page.extract_text()\n",
    "        if verbose: print(f'Text length for {file_path} is {len(text):,} characters.')\n",
    "\n",
    "        return text\n",
    "    domain_knowledge_sentences_dict = {}\n",
    "    for sub_directory, directories_list, files_list in os.walk(pdf_folder):\n",
    "        if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "            if any(map(lambda x: x.endswith('.txt'), files_list)):\n",
    "                for file_name in files_list:\n",
    "                    if file_name.endswith('.txt'):\n",
    "                        file_path = osp.join(sub_directory, file_name)\n",
    "                        with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                            text = f.read()\n",
    "                            domain_knowledge_sentences_dict[file_path] = text\n",
    "            else:\n",
    "                for file_name in files_list:\n",
    "                    if file_name.endswith('.pdf'):\n",
    "                        file_path = osp.join(sub_directory, file_name)\n",
    "                        text = convert(file_path, verbose=True)\n",
    "                        domain_knowledge_sentences_dict[file_path] = text\n",
    "    nu.store_objects(domain_knowledge_sentences_dict=domain_knowledge_sentences_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e3819a-c3e1-4fc2-a4b7-4f9daeae85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assert that you got all the hyphenated word wrappings out\n",
    "for file_path, text in domain_knowledge_sentences_dict.items():\n",
    "    assert not ('effec-' in text), f'{file_path} still has hyphenated word wrappings.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e474dff1-2f02-4ab6-b5f6-61b8eda7af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load documents\n",
    "if nu.csv_exists('domain_doc_ners_df'): domain_doc_ners_df = nu.load_data_frames(domain_doc_ners_df='domain_doc_ners_df')['domain_doc_ners_df']\n",
    "else:\n",
    "    entities = []\n",
    "    for file_path, text in domain_knowledge_sentences_dict.items():\n",
    "        text_length = len(text)\n",
    "        # print(f'Text length for {file_path} is {text_length:,} characters.')\n",
    "        \n",
    "        # Prepare to join subword tokens back together and keep track of entity and score\n",
    "        output_words = []\n",
    "        current_word = ''\n",
    "        current_entities = []\n",
    "        current_scores = []\n",
    "        current_starts = []\n",
    "        current_ends = []\n",
    "        \n",
    "        # Extract metadata from entity recognition pipeline and add it as a row dictionary to the entities rows list\n",
    "        tokens = token_classifier(text)\n",
    "        for metadata_dict in tokens:\n",
    "            current_entities.append(metadata_dict['entity'])\n",
    "            current_scores.append(metadata_dict['score'])\n",
    "            current_starts.append(metadata_dict['start'])\n",
    "            current_ends.append(metadata_dict['end'])\n",
    "            if metadata_dict['word'].startswith('##'): current_word += metadata_dict['word'][2:]\n",
    "            else:\n",
    "                \n",
    "                # Take the mode of entities and average of scores for the current_word\n",
    "                if current_word:\n",
    "                    if len(current_entities) > 1: current_entities = current_entities[:-1]\n",
    "                    if len(current_scores) > 1: current_scores = current_scores[:-1]\n",
    "                    if len(current_ends) > 2: current_ends = current_ends[:-1]\n",
    "                    mode_entity = pd.Series(current_entities).mode().tolist()[-1]\n",
    "                    mean_score = pd.Series(current_scores).mean()\n",
    "                    start_idx = current_starts[0]\n",
    "                    end_idx = current_ends[-1]\n",
    "                    entity_tuple = (current_word, mode_entity, mean_score, start_idx, end_idx)\n",
    "                    output_words.append(entity_tuple)\n",
    "                    current_word = ''\n",
    "                    current_entities = []\n",
    "                    current_scores = []\n",
    "                    current_starts = []\n",
    "                    current_ends = []\n",
    "                else:\n",
    "                    current_word = metadata_dict['word']\n",
    "                    current_entities = [metadata_dict['entity']]\n",
    "                    current_scores = [metadata_dict['score']]\n",
    "                    current_starts = [metadata_dict['start']]\n",
    "                    current_ends = [metadata_dict['end']]\n",
    "        \n",
    "        # Take the mode of entities for the last current_word\n",
    "        if current_word:\n",
    "            mode_entity = pd.Series(current_entities).mode().tolist()[-1]\n",
    "            mean_score = pd.Series(current_scores).mean()\n",
    "            start_idx = current_starts[0]\n",
    "            end_idx = current_ends[-1]\n",
    "            entity_tuple = (current_word, mode_entity, mean_score, start_idx, end_idx)\n",
    "            output_words.append(entity_tuple)\n",
    "        \n",
    "        for word, entity, score, start, end in output_words:\n",
    "            metadata_dict = {'bert_word': word, 'bert_entity': entity, 'bert_score': score, 'bert_start': start, 'bert_end': end, 'file_path': file_path}\n",
    "            entities.append(metadata_dict)\n",
    "\n",
    "        # Extract SpaCy named entities and add them as a row dictionary to the entities rows list\n",
    "        if text_length <= nlp.max_length:\n",
    "            doc = nlp(text)\n",
    "            entities.extend([\n",
    "                {'file_path': file_path, 'nlp_word': word.text, 'nlp_tag': word.tag_, 'nlp_type': word.ent_type_, 'nlp_pofs': word.pos_}\n",
    "                for word in doc\n",
    "            ])\n",
    "            entities.extend([\n",
    "                {'file_path': file_path, 'ent_phrase': ent.text, 'ent_type': ent.label_, 'ent_start': ent.start_char, 'ent_end': ent.end_char}\n",
    "                for ent in doc.ents\n",
    "            ])\n",
    "    domain_doc_ners_df = DataFrame(entities)\n",
    "    nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af4fbc1-a318-44b5-a730-a4d977276b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert_word', 'bert_entity', 'bert_score', 'bert_start', 'bert_end', 'file_path', 'nlp_word', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_phrase', 'ent_type', 'ent_start', 'ent_end']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assert that all the subword tokens are gone\n",
    "print(domain_doc_ners_df.columns.tolist())\n",
    "mask_series = domain_doc_ners_df.bert_word.map(lambda x: str(x).startswith('##'))\n",
    "df = domain_doc_ners_df[mask_series]\n",
    "assert (df.shape[0] == 0), 'There still exist subword tokens.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fc27787-e268-4c03-b810-8e9eacbdc812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check that you indeed have only strings among the BERT entities\n",
    "mask_series = domain_doc_ners_df.bert_entity.isnull()\n",
    "sorted(domain_doc_ners_df[~mask_series].bert_entity.tolist(), key=lambda x: len(str(x)), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b311d7-0d52-4ccd-bb6e-07d5dcc69257",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the entity type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67525ee5-93a2-45ca-870d-5581fa9fafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'QUANTITY', 'ORG', 'EVENT', 'GPE', 'NORP', 'ORDINAL', 'PRODUCT', 'TIME', 'PERCENT', 'CARDINAL', 'LOC', 'DATE', 'MONEY', 'FAC', 'LAW', 'LANGUAGE', 'WORK_OF_ART', 'nan']\n",
      "\n",
      "CARDINAL ['2015;180(11):1178–1183', '446,484', '22,28,41', '4-6', '700', '2M/', '362', '825', '3000', '905', '4,500', '1–3', '1981–1982', '2006;171(9):826', '3–48', '1-95', '3–8', '2d 553', '284', '9–22']\n",
      "\n",
      "DATE ['1-7', 'the second half of the 20th century', '15 September 1983', 'August 1862', 'the next \\nhour', '62', 'May 29–June 1, 1996', 'April 24, 1863', 'May 12, 2018', 'Nurs Adm. 1995;25:60–62', 'a single day', '78234', '15 December 1996', 'the end of the sixteenth\\ncentury', 'as many years', 'fourteen days', '7-8)', 'the seventies', '5525', '1994 to 1995']\n",
      "\n",
      "EVENT ['the Civil War Doctor Who Pioneered Battlefield Care', 'Operation Iraqi Freedom/Operation Enduring Freedom\\nWW', 'Total War', 'the Military\\nChapter 38', 'The Gulf War', 'World War I:', 'Nazi War on Cancer', 'Revolution', 'Gulf War\\nOEF', 'the Vietnam\\nWar', 'World Health Organization', 'Operation Enduring Freedom\\nOIF', 'the Civil\\nWar', 'Hague Convention for the Protection of Cultural Property', 'the Thirty Years War', 'World War I and', 'Treaty Series', 'The American Revolution', 'Korean War', 'Waging War']\n",
      "\n",
      "FAC ['Line 2', 'the Common\\nRule', 'Candura\\n1978\\nRefusal', 'the Zhong Ma Castle', 'Milvian Bridge', 'Background', 'the Prolonged Field Care Setting', 'MacDill Air Force Base', 'Henderson Hall', 'the Presidential Palace, Port', 'Pearl Harbor', 'Ebolalike', 'PO Able', 'Role 1', 'the “Great Society”', 'the Corpus Hippocraticum', 'C-141', 'Role\\nPhysiatrist\\nBoard', 'Wit Stwosz Street', 'the Naval \\nSpecial Warfare Command']\n",
      "\n",
      "GPE ['Md:', 'Kalisch', 'Calif', 'pancytopenia', 'health….and', 'Newton', 'SOUTHCOM', 'Mathis JE', 'Ft Belvoir', 'Sydney', 'future39', 'Israel', 'Chile', 'typhomalaria', 'Tonghso', 'Pennsylvania', 'Washington, DC:', 'Normandy', 'the United States Armed Forces', 'Cannon']\n",
      "\n",
      "LANGUAGE ['Taiwanese', 'Hebrew', 'French', 'stress35', 'English', 'Arabic', 'Spanish', 'Chinese', 'Filipino']\n",
      "\n",
      "LAW ['the First Amendment', 'Article\\n5', 'article 40', 'Article 6\\nWounded', 'the ANA Code for Nurses', 'CHAPTER 12', 'Order 13139', 'the CIOMS Conference', 'CCW Protocol II', 'Article 134', 'The Universal Declaration of Human Rights', 'Article 10, 1977', 'Article 102', 'article 35', 'Article 27', 'Roe v Wade', 'Common Article 2 of the Geneva Conventions', 'the Limited Test Ban Treaty', 'Affordable Care Act', 'the “Quadruple Aim.']\n",
      "\n",
      "LOC ['Alps', 'the Barbary Coast', 'information62', 'Outbreak', 'the Role 3', 'earth', 'Green', 'Fitzgerald AE', 'the Northwest Africa Strategic', 'South America', 'Platoon', 'the Persian Gulf\\nWar', 'the Role 2', 'Potomac', 'Savo Island', 'the Pacific Islands', 'North Main Street', 'Matthews LJ', 'McChrystal', 'Biological']\n",
      "\n",
      "MONEY ['millions of dollars', '220', '450 mL', '475 US 503', '10,000', 'hundreds of billions', '2) US Department', 'additional $150', 'D-238', '6) US Navy Bureau of Medicine', '$210 million', '12) Department of Defense', '5) US Navy', '702,000', '4', '276', '588 NE2d 1232', '100', '750,000', '4) US \\n']\n",
      "\n",
      "NORP ['Marine', 'Longmans', 'Vietnamese', 'Complex', 'Pyridostigmine', 'Protestants', 'Iliad', 'Bosnian', 'poles', 'WAPS', 'Venezuelan', 'Kimsey', 'Somalis', 'Hippke', 'Bliese', 'Pentagonists', 'Baliki', 'Christians', 'Prussians', 'Extremis']\n",
      "\n",
      "ORDINAL ['1946–49', 'Third', '22nd', '35th', '14(pp199–211),63', '73rd', '18th', 'second', '85th', 'Second', 'fifth', 'Seventh', '5ml', 'DoD', 'First', '19th', '24th', 'Fifth', '28th', '12th']\n",
      "\n",
      "ORG ['Plans/Recommendations', 'the Prehospital \\nTrauma Life Support (PHTLS', 'Hickling EJ', 'Lecturer of Global Affairs', 'ANALYSIS', 'VETERANS’ HEALTHCARE ISSUES', 'The Family Journal', 'the Committee on Medical Research', 'the\\nMedical Department of the Navy', 'ULTRA', 'the Nuremberg\\nTribunal', 'Miller WL', 'Medical Department Standards of Nursing Practice', 'Kerr ST', 'NSN', 'Garfinkel L', 'Gottleib', 'the Draper Committee\\nReport', 'us-israel.org/jsource/Holocaust/nurmlaw4.html', 'the World\\nMedical Association']\n",
      "\n",
      "PERCENT ['73%', '6%', '70 percent', '7 Dr Beecher', '47%', '76%', '14-8', '80 percent', 'the remaining 5%', '52%', '55%', 'as low as 30%', 'up to 10%', '03%', 'about 30%', 'nearly 75%', '6.', '77%', '50%', '90 SALT83']\n",
      "\n",
      "PERSON ['1994;159(8):541–547', 'STEPHEN D. GIEBNER', 'Khyler', 'Roof WC', 'Mefloquine\\nMefloquinesensitive', '• Crawling', 'Kim JS', 'Ramsey21(pp143–147,428–432', 'Adv \\nNeuroimmunol', 'Rich NM', 'Taeger G', 'Mil Psychol', '21,22 Kennedy', 'Frankel H', 'Jay Johannigman', 'Timothy Sprunger', 'Sullivan KA', 'Marty Schreiber', 'McCart GM', 'Smith B.']\n",
      "\n",
      "PRODUCT ['• \\nImpaired', '•\\nCourage', 'Un \\nSouvenir de Solferino', 'the MACE 2', '•\\n', 'FSW\\nFeet', '• Ask', 'flashbacks', '•\\nPeacekeeping', 'J Am Coll Surg', 'BW', 'DD1380 TCCC Card', '• \\nPrevious', 'DIVAD', '1MAJ Schauer', '• \\nOveruse', 'M-16', 'Appendix F', 'RCM 916(e', 'J6']\n",
      "\n",
      "QUANTITY ['20 kg', '85 MMOs', '25-inch', '3 miles', '45 kg', '5 miles', '1mm', 'two foot', '1 miles', 'more than 25 kilograms', '3mm', 'between 160 km (99 miles', '179 Cal App', '40 tons', '2,438 m/8,000 ft above', '10 ml of', '100mg per minute', 'approximately 6 square kilometers', '3 mg', '20 IU/kg']\n",
      "\n",
      "TIME ['9-11', '30 minutes', 'each minute', 'a couple of hours', 'about 4 hours ago', '1 minute', '80-hour', 'all hours of the day', '2004;108:17–27', 'up to 3 hours', '1 to 3 hours', 'approximately 12 hours', 'the same hour', '3 to 4 hours', '3:45', 'the first minute', '2-L', 'work hours', '1 Hour', '5-20 minutes']\n",
      "\n",
      "WORK_OF_ART ['Understand the Impact of Religion and Culture\\non Patient Autonomy\\nReligious', 'Un Souvenir de Solférino', 'Medical Rules of Engagement', 'The Human Volunteer in Military\\nBiomedical Research', 'Document Mental Status', 'Combat Stress \\nDespite', 'Tactical Medicine\\nTABLE', 'National Child Traumatic \\nStress Network', 'MRAP', 'Demographics\\nAge', 'Naval Ophthalmic Support and Training Activity', 'Additional Protocol I”)(June 8, 1977', 'The Principles of International War-Conduct Law', 'Lieutenant General\\nKitano Masaji', 'Warning Before Attack \\n ', 'The Brocade Banner: The Story of Japanese Nationalism', 'Nation Building After World War II', 'Doctor’s Trial', 'The Extreme Situation: Leaving the Wounded', 'Claims for Negligent Medical Care Provided Members of the Armed Forces']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = sorted([str(w) for w in domain_doc_ners_df.ent_type.unique()])\n",
    "words_list = random.sample(words_list, min(len(words_list), 20))\n",
    "print(words_list)\n",
    "mask_series = domain_doc_ners_df.ent_type.isin(words_list)\n",
    "for type, type_df in domain_doc_ners_df[mask_series].groupby('ent_type'):\n",
    "    mask_series = type_df.ent_phrase.isnull()\n",
    "    texts_list = sorted(type_df[~mask_series].ent_phrase.unique())\n",
    "    print()\n",
    "    print(type, random.sample(texts_list, min(len(texts_list), 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303fbcc3-6738-4029-899c-82865d46a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVENT', 'PRODUCT', 'ORDINAL', 'ORG', 'FAC', 'LAW', 'GPE', '', 'DATE', 'MONEY', 'LANGUAGE', 'PERCENT', 'PERSON', 'nan', 'CARDINAL', 'LOC', 'WORK_OF_ART', 'QUANTITY', 'TIME', 'NORP']\n",
      "\n",
      " ['radiologic', 'issn/0026', 'scored', 'philosophically', '54,55', 'http://www.army.mil/cmh-pg/art/A&I/Vietnam/p_3_4_67.jpg', 'RHW', 'Shinseki', 'battlefront', 'CRITICAL', 'Upper', 'corn', 'POWs', 'jurisdictions', 'forced', 'Senior', 'dicentrics', 'FOLLOW', 'WOUNDS', 'exemption']\n",
      "\n",
      "CARDINAL ['2006;2', '1968;78:269–279', '496', '1995;66(3):260–263', 'See', '358–389', \"4'11\", 'odd', '10(–5', '733', '2005;90(1):53–76', '2012;4(1):22', '380', '691', '1975;31(4):49–65', 'V1', '764', '1988;153(1):7–11', '2014;189(12):1479–1486', '1987;17:545–548']\n",
      "\n",
      "DATE ['1854', 'old', '1932–45', '1650', 'MD', 'end', '1930s', '1990;20(3):19–22', 'Nineteenth', '1919', 'full', '3362', '47', '1885', 'generation', '2905', '46', 'era', 'wartime', 'Saturday']\n",
      "\n",
      "EVENT ['FREEDOM', 'Humanitarian', 'K.', 'Lost', 'Marine', 'IRAQI', 'Detainees', 'Succession', '2021', 'Uphold', 'Harris', 'Regulation', 'Agreement', '17', 'A', 'COMBAT', 'on', 'Emergency', 'JE', 'CURRENT']\n",
      "\n",
      "FAC ['Level', 'Zhong', 'Park', 'Criteria', 'Montefiore', 'Martens', 'The', 'Jiangxi', 'Dix', 'STS-4', 'Normal', 'Street', 'Commission', 'Bridge', 'NUREMBERG', 'Ft', 'O6', 'Perimeter', 'Chemical', 'TXA']\n",
      "\n",
      "GPE ['Kanter,20', 'Yersinia', 'PT', 'IV', 'Wallingford', 'Fairfield', 'People', 'TL', 'Tissington', 'Disaster', 'Kansas', 'UNITED', 'Republic', 'India', 'Agade', 'DF', 'Edmonton', 'Warm', 'Tampa', 'SW']\n",
      "\n",
      "LANGUAGE ['English', 'Arabic', 'Filipino', 'Hebrew', 'stress35', 'Chinese', 'Taiwanese', 'Spanish', 'French']\n",
      "\n",
      "LAW ['2(4', 'Hostile', 'chapter', '119a', 'Information', 'Powers', 'Committees', '\\n', 'Treaty', 'With', '10', 'III', 'Constitution', 'Epizootic', '........................................................................................................', 'MILITARY', '105', 'Second', '95', '46']\n",
      "\n",
      "LOC ['Red', 'Willis', 'District', 'individuals2', 'Surgeon', 'Job', 'Specific', 'Armee', 'that', 'Elton', 'IO', 'Incident', 'common16', 'Edelstein', '2', 'Alps', 'Health', 'Veatch', 'FB', 'Middle']\n",
      "\n",
      "MONEY ['Council', '90', '3,000', '300', 'mL', '106', '210', '3', 'Forces', 'D-238', '16', '276', '19', '15', '588', '24', '251', 'over', 'than', 'nearly']\n",
      "\n",
      "NORP ['investigations87', 'Nonissue', 'Laotian', 'Pentagonists', 'poles', 'Civilian', 'Soviets', 'Stakeholders', 'WAPS', 'Map', 'Crimean', 'PHD†', 'Semitic', 'Aaronic', 'stages13', 'Ricin', 'Reese', '-Establish', 'Libyan', 'Continental']\n",
      "\n",
      "ORDINAL ['47th', '9ml', '”2(p69', 'Third', 'First', '50ff', '50th', '5ml', 'Fourth', '11th', 'sixth', 'DoD', '4th', '84th', 'Secondary', 'third', 'Fifth', '8th', '73rd', '40th']\n",
      "\n",
      "ORG ['Kluckhohn', 'Levie', ']', 'Otis', 'Blast', 'Medicine,29', 'DeVito', 'Arlington', 'Commonwealth', 'Occupation', 'Indefinite', 'Marchand', 'Lindsey', 'Grivetti', 'Man', 'Documentation', 'Avery', 'Politics', 'MDA', 'ESP']\n",
      "\n",
      "PERCENT ['786', 'the', '70', '36', '79', '24', '55', 'Beecher', 'at', '28', '16', '80%–100', '15', '40', '03', '15%–20', '(', '97%–98', '100', 'F3d']\n",
      "\n",
      "PERSON ['Bruton', 'Mulrine', 'Penguin', 'Castro', 'Linn', 'Watts', 'Integrity', 'II', 'Beneson', 'Eastham', 'Hood', 'Ringer', 'Amitani', 'Occidente', 'Hooper', 'Nindl', 'Candidate', 'Keith', 'Margaret', 'E4.T2']\n",
      "\n",
      "PRODUCT ['Promise', 'Bikini', 'Threats', 'an', 'Triggered', 'Blackhawk', 'Demobilization', 'Sustainment', 'JF', 'nomogram', 'John', 'Bougie', 'Cushing', 'Protocol', 'Travax', '10', 'McVay', 'tularensis', 'S120', 'Survival']\n",
      "\n",
      "QUANTITY ['5mL', '760', 'minute', '45', '15,000', '2g', 'inch', '2nd', 'day', '163', 'nearly', '25', 'kilometers', 'up', '800', 'mph', 'between', '5', 'ten', 'concussions']\n",
      "\n",
      "TIME ['32:39', '21', '1991;1:96–104', 'AFJI', 'mm', 'rush', '58', 'Last', 'just', '2004;108:17–27', 'man', 'next', '7', '2010;24(1):48–59', '5', '4:28', 'only', 'last', 'This', 'hourly']\n",
      "\n",
      "WORK_OF_ART ['Certain', 'NO', 'Biol', 'Dental', 'Insert', 'Romano', 'Preparing', '“', 'MRAP', 'Executive', 'Sensitive', 'Forcible', 'GOVERNMENT', 'Hour', 'TACTICAL', 'Resources', 'Physician', 'Ceftriaxone', 'Sanctioning', 'Asahi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = sorted([str(w) for w in domain_doc_ners_df.nlp_type.unique()])\n",
    "words_list = random.sample(words_list, min(len(words_list), 20))\n",
    "print(words_list)\n",
    "mask_series = domain_doc_ners_df.nlp_type.isin(words_list)\n",
    "for type, type_df in domain_doc_ners_df[mask_series].groupby('nlp_type'):\n",
    "    mask_series = type_df.nlp_word.isnull()\n",
    "    texts_list = sorted(type_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    print(type, random.sample(texts_list, min(len(texts_list), 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4374d-c26b-48a4-85ac-4663ab97f480",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the tag and parts-of-speech columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34b89cb-4446-43c1-aeda-72d764a5dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan', 'RBS', 'RBR', '.', 'JJ', 'VBZ', 'NN', ':', '``', 'VB', 'WP', 'VBG', '-RRB-', 'UH', '_SP', 'JJR', 'WP$', \"''\", 'PRP', 'JJS']\n",
      "\n",
      "'' ['”3', '”13', '”19', '”13(p65', '”89', '”85', '”62(p82', \"'\", '”92', '”5', '”7(p78', '”12', '”42', '’', '”46', '”34', '”17(p47', '\"', '‘', '”54', '”36', '”37', '”59(p49', '”62', '”4', '”24', '”34(p32', '”88', '”1', '\\uf0a7', '“', '”7', '”64(p26', '”10', '”18,19', '”20(p19', '”71,72', '”26', '”2', '”', '”102(p149', '”64(p13', '”35', '”9', '”11', '”23,24', '”32', '’s', '”15(p43', '”8']\n",
      "\n",
      "-RRB- ['…', ']', '•', '”34(Art23e', '}', ')', '):', '46,47']\n",
      "\n",
      ". ['.....................................................................................................................', '”4(p448', '....................................................', '.........................................................................................', '................................................', '.....................................................................................................', '....................................................................................................................', '.....................................', '.........................................................................................................................', '...............................................', '...................................................................................', '.............................................................................................................................................................', 'd.', '.................................................................................................................', '”12(¶I', '........................................................................................', '...........................................................................................', '...................................................................................................................................................', '.”8(p67', '.....................................................................................................................................', '..................................................................................................', '..............................................................................................................................................', '...................................................................', '..............................................................................................................................................................', '............................................................................................', '...................................................................................................................', '..................................................................................................................................................', '..........................................................', '.............................................................................................................', '”36,37', '?', '................................................................................................', '............................................................', '.......................................................................', '\\uf0b7', '..........................................................................................', '...........................................................................................................................................', '..............................................................................................', '”53(p17', '............................................................................................................................', '”1', '..................................................................................................................', '.........................................................................................................', 'to31(p682', '........................................................................................................', '.............................................', '..........................................................................................................................', '”30', '................................................................................................................................', '..........................................................................................................................................']\n",
      "\n",
      ": ['.............................................................................', '-Any', '...............', '-4', '-If', '.......................................................................', '.....................................................................................................................', '=', '.................................', '..............................................................................', '.The', '[that', '#', '........................................................', '—', '...', '.........................................................................................................', '...............................................................................................', ':D', '.................', '--', '................................................................', '*', '…', '[then', '-5', 'recently,1–8', '):', '/', '×', '..........................................', '-', '...................................................................................................', '.Now', '.........', '........................................................................................', '-The', ':]', '............', '...................................................................', '.................................................................................', '............................................................................................', '-3', ';', '-O2', '............................', ']', '-One', '-Air', '..........']\n",
      "\n",
      "JJ ['actual', 'noisy', 'calm', 'minor', 'ndms', 'econdary', 'Suitable', 'uncommon', 'infirm', 'pneumatic', 'otherMilitary', 'Philippine', 'severe', 'reportable', '82nd', 'sedative', 'Cultural', 'shocked', 'centralized', 'electronic', '30th', 'determinative', 'expert', 'nutrient', 'lucrative', 'knowable', 'loose', 'shortened', 'motionless', 'resolvable', 'worldwide', 'variable', 'tight', 'republican', 'lay', 'legal', 'southwest', 'Assistive', 'advantaged', 'visceral', 'Exotic', 'analytical', 'Muscular', 'tender', 'platelet', 'ondansetron', 'supervisory', 'clandestine', 'corollary', 'neurosurgical']\n",
      "\n",
      "JJR ['healthier', 'thicker', 'electrolyte', 'richer', 'Better', 'closer', 'easier', 'Higher', 'freer', 'brancardier', 'lesser', 'wider', 'shallower', 'https://', 'further', 'cooler', 'hearsay', 'Greater', 'Lesser', 'harder', 'LESS', 'II),25', 'shorter', 'deeper', 'fsw', 'aider', 'colder', 'slower', 'worse', '20,23', 'https://en.wikipedia.org/w/index.php?title=Tactical_Combat_Casualty_Care&oldid=1181962148', 'unsrer', 'stronger', 'Longer', 'broader', 'sharper', 'safer', '71(1', 'better', 'Less', 'poorer', 'MORE', 'clearer', 'lower', 'Younger', 'younger', 'mL', 'lighter', 'greater', 'weaker']\n",
      "\n",
      "JJS ['oldest', 'freshest', 'latest', 'finest', 'highest', 'best', 'deepest', 'longest', 'http://www.fda.gov/', 'toughest', 'greatest', 'Most', 'earliest', 'closest', 'Best', 'strongest', 'fastest', 'least', 'healthiest', 'vest', 'Worst', 'lowest', 're247', 'smallest', 'simplest', 'heaviest', 'shortest', 'Highest', '•', 'largest', 'strictest', 'rarest', 'safest', 'youngest', 'brightest', 'newest', 'electrolyte', 'direst', 'purest', 'fullest', 'causative', 'clearest', 'biggest', 'gravest', 'worst', 'postarrest', 'fittest', 'syphilis', 'easiest', '):']\n",
      "\n",
      "NN ['ambulance', 'rigging', '45,46', 'road', 'retrograde', 'ravine', 'insurgency', 'scuba', 'conduction', 'https://jts.amedd.army.mil/assets/docs/cpgs/Traumatic_Brain_Injury_PFC_06_Dec_2017_ID63.pdf', 'contribute', 'drunkenness', '”66', 'Childbirth', 'lane', 'obsolescence', 'ultrasound', 'cath', 'swimmer', 'defense', 'stamp', 'lobbying', 'noncontrast', 'Part', 'integrating', 'arrayal', 'intent', 'Prohibition', '28th', 'Postexposure', 'skull', 'Removal', 'chill', 'facility', 'thrive', 'clearinghouse', 'hypovolemia', 'yoga', 'Masculinity', 'hand', 'life-', 'register', 'mobilization', 'engagement', 'weariness', 'migratory', 'tomography', 'medwatch.aspx', 'Observe', 'commendation']\n",
      "\n",
      "PRP ['himself', 'i', 'We', 'Me', 'Themselves', 'him', 'barotrauma', 'He', ']', 'us', 'herself', 'they', 'myself', 'tasks13(p136', 'ourselves', 'YOU', 'Itself', 'One', 'them', 'α', 'one', 'yourself', 'They', 'estimate7(p178,n5)—they', 'I', 'HAZARDS', 'mine', 'he', 'IT', 'we', 'em', 'actigraph', 'ours', 'PVOs', 'themselves', 'Himself', 'Him', 'oneself', 'itself', '=', 'her', 'it', 'she', 'me', 'She', 'You', 'you', 'Dax', 'n’t', '’s']\n",
      "\n",
      "RBR ['Earlier', 'More', 'closer', 'longer', 'harder', 'higher', 'further', 'worse', 'ipso', 'better', 'https://', 'later', 'more', 'farther', 'sooner', 'lower', 'Later', 'faster', 'Better', 'earlier', 'multiplier', 'less', 'Less']\n",
      "\n",
      "RBS ['Best', 'best', 'highest', 'most', 'https://www.defense.gov/News/Special-Reports/1012_biosurveillance/.', '•', 'least', 'Most']\n",
      "\n",
      "UH ['en', 'tag', 'Hg', 'well', 'Drs', 'os', 'Yes', 'Cc', '%', 'T]hose', 'Hey', 'right', '49,95–97', 'No', 'yes', 'quote', 'O', '-', 'n%20the%20Battlefield%20J%20Trauma%202012.pdf', 'Um', '-Chin', 'Right', 'Administer', 'please', 'Pike', 'doe', 'okay', 'Ay', 'mm', 'Oh', 'hey', 'UH', 'Well', 'say', 'no', 'ha', 'ah', 'Please', 'YES', 'like']\n",
      "\n",
      "VB ['honor', 'precipitate', 'inspire', 'sit', 'camouflage', 'bide', 'sponsor', 'fail11', 'Hisahiko', 'imbue', 'denigrate', 'exterminate', 'misuse', 'cherish', 'relax', 'recommend', 'grapple', 'ingest', 'escort', 'fare', 'defy', 'fusionism', 'Synchronize', 'account', 'elucidate', 'mandate', 'rot', 'lure', 'acquaint', 'relegate', 'complete', 'Integrate', 'return', 'urinate', 'curer', 'myriad', 'feel', 'curtail', 'plan', 'bear', 'count', 'mislead', 'lethal', 'wend', 'recall', 'alternate', 'execute', 'substantiate', 'postpone', 'regulate']\n",
      "\n",
      "VBG ['traversing', 'alizing', 'handling', 'Meeting', 'rollerblading', 'violating', 'Anticipating', 'managing', 'consuming', 'dividing', 'letting', 'knowing', 'abusing', 'saluting', 'including', 'rushing', 'Sharing', 'slackening', 'GATHERING', 'interacting', 'theorizing', 'contributing', 'being', 'FEIGNING', 'accumulating', 'Stepping', 'reproducing', 'Advancing', 'escalating', 'orchestrating', 'depolarizing', 'approving', 'Countering', 'addressing', 'casualtyproducing', 'baffling', 'caregiving', 'developing', 'paving', 'linking', 'inciting', 'Managing', 'attending', 'converting', 'characterizing', 'maiming', 'disrupting', 'insulating', 'dedicating', 'succeeding']\n",
      "\n",
      "VBZ ['specifies', 'fabricates', 'points', 'disposes', 'precedes', 'emphasizes', 'censures', 'blames', 'harms', 'dovetails', 'associates', 'CHALLENGES', 'skills', 'owns', 'environments', 'symptoms', 'revolves', 'physicians', 'APPLIES', 'costs', 'transmits', 'approves', 'turns', 'measures', 'overlooks', 'procures', 'accommodates', 'saves', 'becomes', 'hallways', 'exposures', 'normalizes', 'axioms', 'crosses', 'fractures', 'obliges', 'undergirds', 'noncombatants', 'vehicles', 'Plans', 'begins', 'wipes', 'briefs', 'realizes', 'USAF', 'conforms', 'declines', 'accomplishes', 'visits', 'activates']\n",
      "\n",
      "WP ['WHAT', 'WHO', 'whoever', 'Who', 'what', 'who', 'What', 'whom']\n",
      "\n",
      "WP$ ['whose', 'Whose']\n",
      "\n",
      "_SP ['\\n \\n \\n \\n \\n \\n \\n \\n ', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', 'vehicle.[2', '\\n \\n ', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n\\n \\n \\n', '\\xa0\\n ', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n\\n\\n\\n\\n', '\\n \\n \\n ', '\\n\\n', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n \\n \\n', '\\n ', '\\xa0', '\\xa0\\n', '\\n \\n \\n \\n', '\\n \\n \\n \\n \\n \\n \\n', '\\n \\n \\n \\n \\n \\n', '\\u2003\\n', '\\n \\n', '\\n \\n\\n\\n', '\\n \\n \\n \\n \\n', '\\n\\n \\n \\n\\n\\n', '\\u2002 ', '\\n', '\\n\\n\\n']\n",
      "\n",
      "`` ['‘', '“', 'http://www.biol.tsukuba.ac.jp/~macer/index.html', '”', '…', '\"']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = sorted([str(w) for w in domain_doc_ners_df.nlp_tag.unique()])\n",
    "words_list = random.sample(words_list, min(len(words_list), 20))\n",
    "print(words_list)\n",
    "mask_series = domain_doc_ners_df.nlp_tag.isin(words_list)\n",
    "for tag, tag_df in domain_doc_ners_df[mask_series].groupby('nlp_tag'):\n",
    "    mask_series = tag_df.nlp_word.isnull()\n",
    "    texts_list = sorted(tag_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    print(tag, random.sample(texts_list, min(len(texts_list), 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee497e3-2371-41a9-8e9f-897535af1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NUM', 'SCONJ', 'X', 'PART', 'VERB', 'NOUN', 'INTJ', 'ADV', 'nan', 'PUNCT', 'PROPN', 'AUX', 'SPACE', 'SYM', 'DET', 'ADJ', 'PRON', 'CCONJ', 'ADP']\n",
      "\n",
      "ADJ ['Discontinue', 'true', 'combatcapable', 'rocky', 'advanced', 'Russian', 'untenable', 'wholesome', 'painful', 'derive', 'Frequent', 'liquid', 'µg', 'ineradicable', 'Coalition', 'inconsistent', 'absolute', 'unsure', 'irreversible', 'succinct']\n",
      "\n",
      "ADP ['throughout', 'Up', 'https://', 'Around', 'After', 'notwithstanding', 'With', 'irregulâr', '1933–1945', 'Into', 'v.', 'out', 'DURING', 'vs.', 'for', 'volvulus', 'Between', 'underneath', 'ta', 'at']\n",
      "\n",
      "ADV ['beforehand', 'perbronchially', 'externally', 'Widely', 'than', 'easily', 'out', 'well', 'solemnly', 'comparably', 'unusually', '+', 'attitudes,25', 'Fundamentally', 'aptly', 'invariably', 'back', 'subsequently', 'closely', 'incredibly']\n",
      "\n",
      "AUX ['”41', 'got', 'uses', 'See', 'becoming', 'Has', 'Spurting', 'would', 'remain', 'had', '’ve', 'Ca', 'aches', 'Can', 'deploym', 'see', 'did', 'became', '69,70', 'EXHIBIT']\n",
      "\n",
      "CCONJ ['OR', 'nor', 'vs.', 'minus', 'Nor', 'plus', '”62', 'so', 'Neither', 'either', 'Yet', 'mg', 'or', 'But', 'gland', '&', 'Mid-', 'below)—but', 'AND', 'And']\n",
      "\n",
      "DET ['All', 'An', 'fl', 'right”—the', 'such', 'Those', 'Which', 'the', 'Either', 'each', 'many', 'Such', 'Some', 'THE', 'done?—the', 'which', 'https://jts.amedd.army.mil/assets/docs/forms/MASCAL_Form_Instructions.pdf', 'twice', 'principlism”—the', 'NO']\n",
      "\n",
      "INTJ ['Yes', 'Pike', 'please', 'yes', 'quote', 'like', 'T]hose', 'os', 'hey', 'Right', 'ha', 'O', '49,95–97', 'Well', 'YES', '-Chin', 'Ay', 'mm', 'tag', 'Administer']\n",
      "\n",
      "NOUN ['Socialization', 'Number', 'booby', 'Battalion', 'servicepersons', 'carve', 'extend', 'orbits', 'Source', 'sigh', 'steps', 'Bowers', 'Mechanism', 'bows', 'obligation', 'record', 'Yard', 'thereof', 'client', 'Assessments']\n",
      "\n",
      "NUM ['87', '793', '1813', '1881', '1890', '767', '1953;6:179', '2004;351:2471', '335', '10/10', '800mcg', '13', '2015;29(3):19–26', 'civilMilitary', '1000', 'Six', 'Thirty', '2014;179(3):324–328', '2007;172(9):907–911', '2001;138:410–418']\n",
      "\n",
      "PART ['s', 'to', 'To', \"'s\", 'na', '‘', 'Not', 'not', '’S', 'nt', 'µg', 'NOT', '=', \"'S\", '*', '’', 'TO', '’s', '\\uf0a7', 'n’t']\n",
      "\n",
      "PRON ['all', 'Her', 'whom', 'YOUR', 'his', 'THE', 'Both', 'him', 'everything', 'its', 'em', 'each', 'midazolam', 'MY', 'Anything', 'THEIR', 'Which', 'Those', 'Another', '”94(p56']\n",
      "\n",
      "PROPN ['Interviewing', 'Foster', 'Masami', 'hyperbaric', 'DCoE', 'DS', 'Q', 'Ginde', 'Arkansas', 'Blockers', 'http://emacweb.org/index.php/learn-about-emac/what-is-emac', 'Documentation', 'Ingraham', 'Vulnerability', 'Rechts-', 'Interdisciplinary', 'Populated', 'Kolmer', 'Wedmore', 'Presumptive']\n",
      "\n",
      "PUNCT ['”20(p20', 'd.', '”46', '”2', '–', '........................................................................................................................', '(', 'Health,', '3,45', '.........................................................................', '-%20June%202015%20Updated%20Dec%202016.pdf?ver=2016', '2014;17:366–371', '”30', '...............................................................................................................................................', '…', '”10', '.....................................................................................................', '”37', '.........', '.............................................................................................................']\n",
      "\n",
      "SCONJ ['Though', 'Because', 'Despite', 'whereby', 'because', 'why', 'HOW', 'IF', 'If', 'Since', 'How', 'when', 'where', 'a]lthough', 'since', 'Once', 'Within', 'after', 'till', 'albeit']\n",
      "\n",
      "SPACE ['\\n \\n \\n', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\xa0\\n ', '\\n\\n\\n', '\\n\\n \\n \\n\\n\\n', '\\n', '\\n \\n \\n \\n \\n \\n', '\\n ', '\\n\\n \\n \\n', '\\n \\n \\n \\n \\n \\n \\n', '\\u2003\\n', '\\n \\n \\n \\n \\n \\n \\n \\n ', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n\\n\\n\\n\\n', '\\u2002 ', '\\n \\n \\n \\n', '\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', '\\n \\n \\n \\n \\n', '\\xa0']\n",
      "\n",
      "SYM ['.', '$', 'mL', 'x', '?', '#', '…', '=', '-', 'www.militarychild.org', 'May–25', '/', '/reports', '•', '.[This', '\\uf0a7', '>', ':', '*', '/-']\n",
      "\n",
      "VERB ['Dubbed', 'bolstered', 'condensed', 'differentiated', 'intrigued', 'speak', 'divert', 'checking', 'Needed', 'Commanded', 'outlawing', 'peer', 'broke', 'postpone', 'accentuate', 'plug', 'minded', 'cover', 'itemized', 'suit']\n",
      "\n",
      "X ['34', '4(pp41–42', '74(pp162,180–181', 'DoD', '94(pp51–54', 'AtomicArchive.com', 'population,13,14', 'a]ny', '”6(pp7–8', '2005;3:53', 'v', '2000;35:82–87', '17(p21', '2(p724', '”27(p181', 'http://telemedicine.wramc.amedd.army.mil/', '18(p2', '15(p154', '40(pp17–18', '2007;12(1):34–47']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = sorted([str(w) for w in domain_doc_ners_df.nlp_pofs.unique()])\n",
    "words_list = random.sample(words_list, min(len(words_list), 20))\n",
    "print(words_list)\n",
    "mask_series = domain_doc_ners_df.nlp_pofs.isin(words_list)\n",
    "for pofs, pofs_df in domain_doc_ners_df[mask_series].groupby('nlp_pofs'):\n",
    "    mask_series = pofs_df.nlp_word.isnull()\n",
    "    texts_list = sorted(pofs_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    print(pofs, random.sample(texts_list, min(len(texts_list), 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006132dd-12de-438d-b0ad-792b98361f70",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the entity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f017890b-f346-4fe8-9072-ca002dc5e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-LOC', 'nan', 'I-ORG', 'I-PER', 'I-MISC']\n",
      "\n",
      "I-LOC ['t', 'Detrick', 'cChord', 'Towson', 'States', 'Iraq', 'Springs', 'China', 'Afghanistan', 'South', 'Geneva', 'Campanile', 'Center', 'Florida', 'Connecticut', 'Marine', 'Hurlburt', 'Asia', 'and', 'line', 'Chicago', 'Tennessee', 'Lackland', 'man', 'worth', 'S', 'Leyte', 'Andrews', 'Montefiore', 'Maryland', 'Einstein', 'Hebert', 'West', 'Haven', 'Durham', 'Vincent', 'EUTHANASIALE', 'Philippines', 'Mogadishu', 'Spring', 'Hospital', 'Glen', 'Delaware', 'Kingdom', 'pan', 'France', 'Germany', 'Arizona', 'Room', 'Silver']\n",
      "\n",
      "I-MISC ['Experiment', 'Armed', 'Sun', 'Human', 'Aristotle', 'Doctrine', 's', 'SICIGI', 'ETUMAN', '21st', 'alis', 'LA', 'Remembrance', 'Persian', 'L', 'Roman', 'Biological', 'lines', '19th', 'Occied', 'Da', 'Doc', 'North', 'Ethics', 'mmde', 'erilization', 'NAZI', 'RAUM', 'Immunode', 'LAN', 'LNL', 'The', 'Confederate', 'Ra', 'A', 'Geneva', 'mitatus', 'Medical', 'Decision', 'Desert', 'Great', 'Just', 'VILDID', 'NE', 'HUMAN', 'With', 'um', 'APAN', 'ATA', 'Islam']\n",
      "\n",
      "I-ORG ['oT', 'N', 'geon', 'TI', 'Ethics', 'Uniformed', 'stain', 'Science', 'enario', 'Collection', 'CP', 'C', 'Yale', 'Luke', 'Chemical', 'SCHWARTZNI', 'UNIVERS', ',', 'Fellowship', 'Ms', 'sight', 'Harris', 'uoyancy', 'Branch', 'Sciences', 'Warfare', 'Normandy', 'Disaster', 'LLSIUMR', 'L', 'NNY', 'PYLAX', 'R', 'the', 'UN', 'Uniform', 'Maryland', 'Wissenschaftsgeschichte', 'EHALUWASEYI', 'TCCC', 'WarfareUM', 'AMIE', 'TORRISI', 'Pentagon', 'JD', 'EXIBITL', 'Combat', 'Tumatic', 'eutral', 'Metor']\n",
      "\n",
      "I-PER ['MBS', 'MARK', 'HARS', 'Scott', 'rook', 'LUNASCOUMAN', 'THAS', 'Twillige', 'James', 'SCHEURING', 'NEIL', 'Drew', 'man', 'MULLNRAD', 'TOY', 'RICRD', 'Savell', 'Varisano', 'arisano', 'BEAM', 'Mosely', 'OR', 'CHETAN', 'Daniel', 'Hammond', 'MICHAEL', 'McBarron', 'BALKINLGI', 'ANGELA', 'Don', 'ARNELL', 'ARVEY', 'UPCHAK', 'Shelia', 'LANLOKUN', 'OWE', 'PATRICIA', 'OPRESTI', 'Early', 'GELA', 'SHACKELFD', 'Montgomery', 'rrey', 'ERESA', 'Robert', 'Franklin', 'RASCONA', 'Peter', 'ISAIAH', 'gle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = sorted([str(w) for w in domain_doc_ners_df.bert_entity.unique()])\n",
    "words_list = random.sample(words_list, min(len(words_list), 20))\n",
    "print(words_list)\n",
    "mask_series = domain_doc_ners_df.bert_entity.isin(words_list)\n",
    "for entity, entity_df in domain_doc_ners_df[mask_series].groupby('bert_entity'):\n",
    "    mask_series = entity_df.bert_word.isnull()\n",
    "    texts_list = sorted(entity_df[~mask_series].bert_word.unique())\n",
    "    print()\n",
    "    print(entity, random.sample(texts_list, min(len(texts_list), 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3e573-63f1-4473-9b97-0c5dc3139db5",
   "metadata": {},
   "source": [
    "\n",
    "## Explore column groupbys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05db7cd4-1b8f-437f-930c-31b11657e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Domain_Knowledge/Fundamentals of Military Medicine/Fund ch 1.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>89871</th>\n",
       "      <th>90227</th>\n",
       "      <th>95310</th>\n",
       "      <th>94584</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file_path</th>\n",
       "      <td>../data/Domain_Knowledge/Fundamentals of Milit...</td>\n",
       "      <td>../data/Domain_Knowledge/Fundamentals of Milit...</td>\n",
       "      <td>../data/Domain_Knowledge/Fundamentals of Milit...</td>\n",
       "      <td>../data/Domain_Knowledge/Fundamentals of Milit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_word</th>\n",
       "      <td>an</td>\n",
       "      <td>a</td>\n",
       "      <td>,</td>\n",
       "      <td>units</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_tag</th>\n",
       "      <td>DT</td>\n",
       "      <td>DT</td>\n",
       "      <td>,</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_type</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_pofs</th>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       89871  \\\n",
       "file_path  ../data/Domain_Knowledge/Fundamentals of Milit...   \n",
       "nlp_word                                                  an   \n",
       "nlp_tag                                                   DT   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                                 DET   \n",
       "\n",
       "                                                       90227  \\\n",
       "file_path  ../data/Domain_Knowledge/Fundamentals of Milit...   \n",
       "nlp_word                                                   a   \n",
       "nlp_tag                                                   DT   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                                 DET   \n",
       "\n",
       "                                                       95310  \\\n",
       "file_path  ../data/Domain_Knowledge/Fundamentals of Milit...   \n",
       "nlp_word                                                   ,   \n",
       "nlp_tag                                                    ,   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                               PUNCT   \n",
       "\n",
       "                                                       94584  \n",
       "file_path  ../data/Domain_Knowledge/Fundamentals of Milit...  \n",
       "nlp_word                                               units  \n",
       "nlp_tag                                                  NNS  \n",
       "nlp_type                                                      \n",
       "nlp_pofs                                                NOUN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for file_path, file_path_df in domain_doc_ners_df.groupby('file_path'):\n",
    "    print(file_path)\n",
    "    display(file_path_df.sample(4).dropna(axis='columns', how='all').T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cfa2c2-df23-4d0e-882d-724657e82c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1057944</th>\n",
       "      <th>1018594</th>\n",
       "      <th>1280591</th>\n",
       "      <th>1280584</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file_path</th>\n",
       "      <td>../data/Domain_Knowledge/Military Medical Ethi...</td>\n",
       "      <td>../data/Domain_Knowledge/Military Medical Ethi...</td>\n",
       "      <td>../data/Domain_Knowledge/Military Medical Ethi...</td>\n",
       "      <td>../data/Domain_Knowledge/Military Medical Ethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_word</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_tag</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_type</th>\n",
       "      <td></td>\n",
       "      <td>MONEY</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_pofs</th>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     1057944  \\\n",
       "file_path  ../data/Domain_Knowledge/Military Medical Ethi...   \n",
       "nlp_word                                                   $   \n",
       "nlp_tag                                                    $   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                                 SYM   \n",
       "\n",
       "                                                     1018594  \\\n",
       "file_path  ../data/Domain_Knowledge/Military Medical Ethi...   \n",
       "nlp_word                                                   $   \n",
       "nlp_tag                                                    $   \n",
       "nlp_type                                               MONEY   \n",
       "nlp_pofs                                                 SYM   \n",
       "\n",
       "                                                     1280591  \\\n",
       "file_path  ../data/Domain_Knowledge/Military Medical Ethi...   \n",
       "nlp_word                                                   •   \n",
       "nlp_tag                                                    $   \n",
       "nlp_type                                             PRODUCT   \n",
       "nlp_pofs                                                 SYM   \n",
       "\n",
       "                                                     1280584  \n",
       "file_path  ../data/Domain_Knowledge/Military Medical Ethi...  \n",
       "nlp_word                                                   •  \n",
       "nlp_tag                                                    $  \n",
       "nlp_type                                             PRODUCT  \n",
       "nlp_pofs                                                 SYM  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for nlp_tag, nlp_tag_df in domain_doc_ners_df.groupby('nlp_tag'):\n",
    "    print(nlp_tag)\n",
    "    display(nlp_tag_df.sample(4).dropna(axis='columns', how='all').T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17a34216-30de-48dc-b474-392799c5ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_entity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311771</th>\n",
       "      <td>MEESEL</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.565598</td>\n",
       "      <td>172.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>Fund ch 3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427113</th>\n",
       "      <td>OLYTRAUMA</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.824845</td>\n",
       "      <td>232.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>Fund ch 36.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828641</th>\n",
       "      <td>OMINICK</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.836177</td>\n",
       "      <td>717.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>Ethics-ch-11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542462</th>\n",
       "      <td>ONAT</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.579091</td>\n",
       "      <td>79.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>Fund ch 7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "311771     MEESEL       I-ORG    0.565598       172.0     371.0   \n",
       "427113  OLYTRAUMA       I-ORG    0.824845       232.0     241.0   \n",
       "828641    OMINICK       I-PER    0.836177       717.0     724.0   \n",
       "542462       ONAT       I-ORG    0.579091        79.0      83.0   \n",
       "\n",
       "               file_path  \n",
       "311771     Fund ch 3.txt  \n",
       "427113    Fund ch 36.txt  \n",
       "828641  Ethics-ch-11.txt  \n",
       "542462     Fund ch 7.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481847</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>476.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>Fund ch 4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294854</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.953635</td>\n",
       "      <td>882.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>Ethics-ch-26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807603</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.955297</td>\n",
       "      <td>244.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>Ethics-ch-10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213496</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.968651</td>\n",
       "      <td>699.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>Fund ch 21.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "481847          &       I-ORG    0.904070       476.0     487.0   \n",
       "1294854  Sciences       I-ORG    0.953635       882.0     902.0   \n",
       "807603   Sciences       I-ORG    0.955297       244.0     269.0   \n",
       "213496   Sciences       I-ORG    0.968651       699.0     713.0   \n",
       "\n",
       "                file_path  \n",
       "481847      Fund ch 4.txt  \n",
       "1294854  Ethics-ch-26.txt  \n",
       "807603   Ethics-ch-10.txt  \n",
       "213496     Fund ch 21.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481847</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>476.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>Fund ch 4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120606</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.625684</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>Fund ch 12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862723</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.898472</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>Ethics-ch-12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109083</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.952303</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>Fund ch 11.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "481847         &       I-ORG    0.904070       476.0     487.0   \n",
       "120606  Sciences       I-ORG    0.625684      1033.0    1047.0   \n",
       "862723  Sciences       I-ORG    0.898472      1068.0    1088.0   \n",
       "109083  Sciences       I-ORG    0.952303      1071.0    1085.0   \n",
       "\n",
       "               file_path  \n",
       "481847     Fund ch 4.txt  \n",
       "120606    Fund ch 12.txt  \n",
       "862723  Ethics-ch-12.txt  \n",
       "109083    Fund ch 11.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481847</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>476.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>Fund ch 4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311682</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.923410</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>Ethics-ch-27.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299962</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.860803</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>Fund ch 29.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120606</th>\n",
       "      <td>Sciences</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.625684</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>Fund ch 12.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "481847          &       I-ORG    0.904070       476.0     487.0   \n",
       "1311682  Sciences       I-ORG    0.923410      1016.0    1036.0   \n",
       "299962   Sciences       I-ORG    0.860803      1023.0    1037.0   \n",
       "120606   Sciences       I-ORG    0.625684      1033.0    1047.0   \n",
       "\n",
       "                file_path  \n",
       "481847      Fund ch 4.txt  \n",
       "1311682  Ethics-ch-27.txt  \n",
       "299962     Fund ch 29.txt  \n",
       "120606     Fund ch 12.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1294842</th>\n",
       "      <td>Room</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.374644</td>\n",
       "      <td>594.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>Ethics-ch-26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495184</th>\n",
       "      <td>H</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.970456</td>\n",
       "      <td>138.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>Fund ch 40.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573136</th>\n",
       "      <td>University</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.970490</td>\n",
       "      <td>466.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>Fund ch 9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223549</th>\n",
       "      <td>University</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.970550</td>\n",
       "      <td>842.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Fund ch 22.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "1294842        Room       I-LOC    0.374644       594.0     604.0   \n",
       "495184            H       I-ORG    0.970456       138.0     148.0   \n",
       "573136   University       I-ORG    0.970490       466.0     479.0   \n",
       "223549   University       I-ORG    0.970550       842.0     855.0   \n",
       "\n",
       "                file_path  \n",
       "1294842  Ethics-ch-26.txt  \n",
       "495184     Fund ch 40.txt  \n",
       "573136      Fund ch 9.txt  \n",
       "223549     Fund ch 22.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38723</th>\n",
       "      <td>Pass</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.767956</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>SchauerMedicBag.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262026</th>\n",
       "      <td>for</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.966432</td>\n",
       "      <td>956.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>Fund ch 25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284233</th>\n",
       "      <td>Emergency</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.990785</td>\n",
       "      <td>955.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>Fund ch 27.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324236</th>\n",
       "      <td>Department</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.998094</td>\n",
       "      <td>954.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>3-84-D11-LEGAL-ROE.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "38723          Pass       I-LOC    0.767956        52.0      59.0   \n",
       "262026          for       I-ORG    0.966432       956.0     968.0   \n",
       "284233    Emergency       I-ORG    0.990785       955.0     973.0   \n",
       "1324236  Department       I-ORG    0.998094       954.0     967.0   \n",
       "\n",
       "                      file_path  \n",
       "38723       SchauerMedicBag.txt  \n",
       "262026           Fund ch 25.txt  \n",
       "284233           Fund ch 27.txt  \n",
       "1324236  3-84-D11-LEGAL-ROE.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38723</th>\n",
       "      <td>Pass</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.767956</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>SchauerMedicBag.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223557</th>\n",
       "      <td>iformed</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.989640</td>\n",
       "      <td>979.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>Fund ch 22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275639</th>\n",
       "      <td>Program</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.825875</td>\n",
       "      <td>974.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>Fund ch 26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311679</th>\n",
       "      <td>iformed</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980799</td>\n",
       "      <td>974.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>Ethics-ch-27.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "38723        Pass       I-LOC    0.767956        52.0      59.0   \n",
       "223557    iformed       I-ORG    0.989640       979.0     986.0   \n",
       "275639    Program       I-ORG    0.825875       974.0     985.0   \n",
       "1311679   iformed       I-ORG    0.980799       974.0     981.0   \n",
       "\n",
       "                   file_path  \n",
       "38723    SchauerMedicBag.txt  \n",
       "223557        Fund ch 22.txt  \n",
       "275639        Fund ch 26.txt  \n",
       "1311679     Ethics-ch-27.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>862730</th>\n",
       "      <td>ggs</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.310521</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>Ethics-ch-12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153027</th>\n",
       "      <td>agency</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980444</td>\n",
       "      <td>587.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>Ethics-ch-20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542481</th>\n",
       "      <td>Medical</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980487</td>\n",
       "      <td>592.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>Fund ch 7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284234</th>\n",
       "      <td>Uniformed</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980573</td>\n",
       "      <td>975.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>Fund ch 27.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "862730         ggs       I-ORG    0.310521      1266.0    1382.0   \n",
       "1153027     agency       I-ORG    0.980444       587.0     593.0   \n",
       "542481     Medical       I-ORG    0.980487       592.0     610.0   \n",
       "284234   Uniformed       I-ORG    0.980573       975.0     984.0   \n",
       "\n",
       "                file_path  \n",
       "862730   Ethics-ch-12.txt  \n",
       "1153027  Ethics-ch-20.txt  \n",
       "542481      Fund ch 7.txt  \n",
       "284234     Fund ch 27.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>862730</th>\n",
       "      <td>ggs</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.310521</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>Ethics-ch-12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153027</th>\n",
       "      <td>agency</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980444</td>\n",
       "      <td>587.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>Ethics-ch-20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542481</th>\n",
       "      <td>Medical</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980487</td>\n",
       "      <td>592.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>Fund ch 7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284234</th>\n",
       "      <td>Uniformed</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.980573</td>\n",
       "      <td>975.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>Fund ch 27.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "862730         ggs       I-ORG    0.310521      1266.0    1382.0   \n",
       "1153027     agency       I-ORG    0.980444       587.0     593.0   \n",
       "542481     Medical       I-ORG    0.980487       592.0     610.0   \n",
       "284234   Uniformed       I-ORG    0.980573       975.0     984.0   \n",
       "\n",
       "                file_path  \n",
       "862730   Ethics-ch-12.txt  \n",
       "1153027  Ethics-ch-20.txt  \n",
       "542481      Fund ch 7.txt  \n",
       "284234     Fund ch 27.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_start and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43781</th>\n",
       "      <td>TCCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.975280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Tactical_Combat_Casualty_Care.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204181</th>\n",
       "      <td>Community</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.708247</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>Fund ch 20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83930</th>\n",
       "      <td>AMERICAN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.987863</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>Fund ch 1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99174</th>\n",
       "      <td>Military</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.786213</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>Fund ch 10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "43781        TCCC       I-ORG    0.975280         0.0       4.0   \n",
       "204181  Community       I-ORG    0.708247      1031.0    1049.0   \n",
       "83930    AMERICAN      I-MISC    0.987863      1032.0    1040.0   \n",
       "99174    Military      I-MISC    0.786213      1032.0    1049.0   \n",
       "\n",
       "                                file_path  \n",
       "43781   Tactical_Combat_Casualty_Care.txt  \n",
       "204181                     Fund ch 20.txt  \n",
       "83930                       Fund ch 1.txt  \n",
       "99174                      Fund ch 10.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "\n",
    "columns_list = [cn for cn in domain_doc_ners_df.columns if cn.startswith('bert_')]\n",
    "for groupby_columns in combinations(columns_list, 2):\n",
    "    groupby_columns = list(groupby_columns)\n",
    "    df = nu.get_minority_combinations(domain_doc_ners_df, groupby_columns).dropna(axis='columns', how='all')\n",
    "    if df.shape[0]:\n",
    "        print(nu.conjunctify_nouns(groupby_columns))\n",
    "        df.file_path = df.file_path.map(lambda x: str(x).split('/')[-1])\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3083085-8b20-4a66-8b2a-b2e8ba4a256f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex (Python 3.10.13)",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51ab3c6-9bc9-4299-86d6-64763b66e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "if (osp.join(os.pardir, 'py') not in sys.path): sys.path.insert(1, osp.join(os.pardir, 'py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c96aaf-34eb-4e78-b477-d2e6bd721e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import (nu, osp, re, display)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ffe8e-6573-470f-aef5-348522a0de15",
   "metadata": {},
   "source": [
    "\n",
    "# Search for Scenario and Probe Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e474dff1-2f02-4ab6-b5f6-61b8eda7af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for domain_doc_ners_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df.csv.\n",
      "(1529806, 16)\n",
      "['bert_word', 'bert_entity', 'bert_score', 'bert_start', 'bert_end', 'file_path', 'nlp_word', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_phrase', 'ent_type', 'ent_start', 'ent_end', 'is_probe', 'is_probe_probability']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the NER entities from a CSV\n",
    "if nu.csv_exists('domain_doc_ners_df'):\n",
    "    domain_doc_ners_df = nu.load_data_frames(domain_doc_ners_df='domain_doc_ners_df')['domain_doc_ners_df']\n",
    "    print(domain_doc_ners_df.shape)\n",
    "    print(domain_doc_ners_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c495b3-e3f3-43c1-8195-52504fa12e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/frvrs_logs_df.pkl.\n",
      "Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)\n",
      "No pickle exists for frvrs_logs_df - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/frvrs_logs_df.csv.\n",
      "(829116, 114)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Attempt to load the logs data frame\n",
    "frvrs_logs_df = nu.load_data_frames(frvrs_logs_df='frvrs_logs_df')['frvrs_logs_df']\n",
    "print(frvrs_logs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779ac14-dba1-49c8-9833-24c12d459a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of starting phrases to label for model training\n",
    "df = nu.load_csv(csv_name='domain_doc_ners_df')\n",
    "print(df.columns.tolist())\n",
    "base_mask_series = df.is_probe\n",
    "domain_doc_ners_df['is_probe'] = False\n",
    "\n",
    "# Add the BERT words\n",
    "mask_series = base_mask_series & ~df.bert_word.isnull()\n",
    "bert_words_list = df[mask_series].bert_word.unique().tolist()\n",
    "mask_series = domain_doc_ners_df.bert_word.isin(bert_words_list)\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "\n",
    "# Add the SpaCy words\n",
    "mask_series = base_mask_series & ~df.nlp_word.isnull()\n",
    "nlp_words_list = df[mask_series].nlp_word.unique().tolist()\n",
    "mask_series = domain_doc_ners_df.nlp_word.isin(nlp_words_list)\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "\n",
    "# Add the SpaCy entities\n",
    "mask_series = base_mask_series & ~df.ent_phrase.isnull()\n",
    "ent_phrases_list = df[mask_series].ent_phrase.unique().tolist()\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isin(ent_phrases_list)\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "\n",
    "# Create a set of unique relevant words and save the data frame\n",
    "canonical_phrases = list(set(bert_words_list + nlp_words_list + ent_phrases_list))\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77ccb8-371d-4694-aecd-4a53638292a0",
   "metadata": {},
   "source": [
    "\n",
    "## Clean up ent_phrase column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a95c3e-2be1-4498-9019-4f1d40c5e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True, ngram_range=(1, 3)\n",
    ")\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    norm='l1', smooth_idf=True, sublinear_tf=False, use_idf=True\n",
    ")\n",
    "classifier = SGDClassifier(loss='log_loss', warm_start=True)\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "columns_list = ['ent_phrase', 'is_probe']\n",
    "df = domain_doc_ners_df[~mask_series][columns_list]\n",
    "df.is_probe = df.is_probe.map(\n",
    "    lambda x: {True: 1, False: 0}.get(x, x)\n",
    ")\n",
    "train_data_list = df.ent_phrase.tolist()\n",
    "train_labels_list = df.is_probe.values\n",
    "X_train_counts = vectorizer.fit_transform(train_data_list)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train on initial data\n",
    "classifier.fit(X_train_tfidf, train_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613b233-ffe6-42cb-8e06-a11258ba8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "sample_ent_phrase = domain_doc_ners_df[~mask_series].sample(1).ent_phrase.squeeze()\n",
    "display(sample_ent_phrase)\n",
    "X_test = tfidf_transformer.transform(vectorizer.transform([sample_ent_phrase])).toarray()\n",
    "display(classifier.predict_proba(X_test)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb2b18-a5cf-46a4-97c0-4221c6d3ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'is_probe_probability' not in domain_doc_ners_df.columns: domain_doc_ners_df['is_probe_probability'] = 0.0\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "domain_doc_ners_df.loc[~mask_series, 'is_probe_probability'] = domain_doc_ners_df[~mask_series].ent_phrase.map(\n",
    "    lambda x: classifier.predict_proba(tfidf_transformer.transform(vectorizer.transform([x])).toarray())[0][1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a332e87-a2b0-4e1e-b395-4ebcba2833e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = ~domain_doc_ners_df.is_probe & (domain_doc_ners_df.is_probe_probability > 0.0)\n",
    "analysis_columns = [\n",
    "    'bert_word', 'bert_entity', 'bert_score', 'nlp_word', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_phrase', 'ent_type',\n",
    "    'is_probe', 'is_probe_probability'\n",
    "]\n",
    "print(domain_doc_ners_df.columns.tolist())\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).head(60).tail(20).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddc378-ffc1-46b1-a5ee-1ce9f2d14067",
   "metadata": {},
   "source": [
    "\n",
    "### ent_phrase Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b49595-5b8e-49b5-8ba1-ca8f82a23622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_words_list = ' '.join([str(x).lower() for x in domain_doc_ners_df[domain_doc_ners_df.is_probe].ent_phrase.tolist()])\n",
    "# print(sorted(set(re.compile(r'[\\s/◻&‡:†®−\\.•,-]+').split(test_words_list))))\n",
    "test_words_list = [\n",
    "    'airway', 'celox', 'chest', 'chin', 'chito', 'circulation', 'compression', 'decompress', 'dressings', 'gauze', 'hemorrhage',\n",
    "    'hemostatic',\n",
    "    'jaw', 'junctional', 'kaolin', 'keenan', 'lifesaver', 'limb', 'nasopharyngeal', 'needle', 'pneumatic', 'quickclot', 'tourniquet',\n",
    "    'trauma', 'triage',\n",
    "    'unconscious', 'wounds'\n",
    "]\n",
    "word_analysis_columns = ['file_path'] + analysis_columns\n",
    "print(sorted(set(test_words_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd6f45-4e2b-4afe-8a80-2dfa75001f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    word_str = test_words_list.pop()\n",
    "    mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: word_str.lower() in str(x).lower()) & domain_doc_ners_df.is_probe\n",
    "    df = domain_doc_ners_df[mask_series][word_analysis_columns]\n",
    "    if df.shape[0]:\n",
    "        display(word_str)\n",
    "        df.file_path = df.file_path.map(lambda x: str(x).replace('../data/Domain_Knowledge/', ''))\n",
    "        display(df.dropna(axis='columns', how='all'))\n",
    "except Exception as e: print(str(e).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e3572-4a3f-4af3-82ea-cf9da22c9a55",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecc9e5-901f-4666-8062-9500bd69eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'cpg' in str(x).lower())# & (domain_doc_ners_df.ent_type == 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = False\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "df = domain_doc_ners_df[mask_series][word_analysis_columns].sort_values(['is_probe', 'is_probe_probability'], ascending=[True, False])\n",
    "df.file_path = df.file_path.map(lambda x: str(x).replace('../data/Domain_Knowledge/', ''))\n",
    "display(df.dropna(axis='columns', how='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88196ec1-5ca5-43fd-925b-ded8758a43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: str(x).startswith('Massive Hemorrhage\\nAssess'))\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e717f04-7300-4a7f-b2ba-974c2aea77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: str(x).startswith('Massive External \\nHemorrhage'))\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c8a1-c5f6-456f-ae70-4036f578c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: str(x).startswith('CONTINUE TACTICAL FIELD CARE\\nHemorrhage Contr'))\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f36b73-1635-43aa-8bb0-a6cd07659813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'junctional' in str(x).lower())# & (domain_doc_ners_df.ent_type != 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7c2c8-2ed4-409c-af40-1a851e300c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'nasopharyngeal' in str(x).lower())# & (domain_doc_ners_df.ent_type == 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].sort_values('is_probe_probability', ascending=False).dropna(\n",
    "    axis='columns', how='all'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe074aff-a9fb-4656-aea3-8ef545341ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'tourniquet' in str(x).lower()) & (domain_doc_ners_df.ent_type == 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = False\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'limb tourniquet' in str(x).lower()) & (domain_doc_ners_df.ent_type == 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].dropna(axis='columns', how='all').sort_values(\n",
    "    'is_probe_probability', ascending=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef474d5-da39-48c6-8f34-706cc414380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'triage' in str(x).lower()) & (domain_doc_ners_df.ent_type != 'PERSON')\n",
    "domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "display(domain_doc_ners_df[mask_series][analysis_columns].dropna(axis='columns', how='all').sort_values(\n",
    "    'is_probe_probability', ascending=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec589c-42cc-46c3-9495-cc03e6c21b43",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2571213-2168-44cb-8260-2338658b99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.map(lambda x: 'Xxxxxxxx' in str(x).lower())# & (domain_doc_ners_df.ent_type == 'PERSON')\n",
    "# domain_doc_ners_df.loc[mask_series, 'is_probe'] = True\n",
    "# nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)\n",
    "df = domain_doc_ners_df[mask_series][word_analysis_columns].sort_values(['is_probe', 'is_probe_probability'], ascending=[True, False])\n",
    "df.file_path = df.file_path.map(lambda x: str(x).replace('../data/Domain_Knowledge/', ''))\n",
    "display(df.dropna(axis='columns', how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add653f-09da-4f36-968b-b5d35ac8a744",
   "metadata": {},
   "source": [
    "\n",
    "## Break up dataset into the word columns and save them that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d0eab-eae4-4c78-9da8-73010be5e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the BERT words\n",
    "mask_series = ~domain_doc_ners_df.bert_word.isnull()\n",
    "nu.save_data_frames(**{'domain_doc_bert_words_df': domain_doc_ners_df[mask_series].dropna(axis='columns', how='all')})\n",
    "\n",
    "# Save the SpaCy words\n",
    "mask_series = ~domain_doc_ners_df.nlp_word.isnull()\n",
    "nu.save_data_frames(**{'domain_doc_nlp_words_df': domain_doc_ners_df[mask_series].dropna(axis='columns', how='all')})\n",
    "\n",
    "# Save the SpaCy phrases\n",
    "mask_series = ~domain_doc_ners_df.ent_phrase.isnull()\n",
    "nu.save_data_frames(**{'domain_doc_ent_phrases_df': domain_doc_ners_df[mask_series].dropna(axis='columns', how='all')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc7d9d-112d-495d-9121-dc257e1b6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = ~domain_doc_ners_df.nlp_word.isnull()\n",
    "df = domain_doc_ners_df[mask_series].dropna(axis='columns', how='all')\n",
    "columns_list = [cn for cn in df.columns if cn.startswith('nlp_')]\n",
    "for cn in columns_list:\n",
    "    print(cn, df[cn].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62a244-8d06-40c4-9a8e-5f166a3a4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for nlp_pofs, nlp_pofs_df in df.groupby('nlp_pofs', dropna=False):\n",
    "    nu.save_data_frames(**{f'domain_doc_nlp_word_{str(nlp_pofs).lower()}_df': nlp_pofs_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22435f52-fc54-4763-97b2-b36064679ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex (Python 3.10.13)",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

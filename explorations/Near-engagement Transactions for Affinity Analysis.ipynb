{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f303819c-dc03-46ab-a5fc-d1de4fe42714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "if (osp.join(os.pardir, 'py') not in sys.path): sys.path.insert(1, osp.join(os.pardir, 'py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3d321f-6382-4ab7-823c-6c7ab0243a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import (fu, nu, DataFrame, display)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc37e1-1f10-499c-a2e0-cfd6cbf5b652",
   "metadata": {},
   "source": [
    "\n",
    "# Post-engagement transactions for Affinity Analysis and Association Rule Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6971d12-57c5-48d0-97a4-1fd8abeb97c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(829116, 114)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all CSVs out of one data frame\n",
    "if nu.pickle_exists('frvrs_logs_df'):\n",
    "    frvrs_logs_df = nu.load_object('frvrs_logs_df')\n",
    "    print(frvrs_logs_df.shape)\n",
    "    # df = frvrs_logs_df.sample(4).dropna(axis='columns', how='all')\n",
    "    # display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c6319-80f7-47a4-b9b5-a93497bde849",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3da9ea-1dc2-4fc3-b35e-71ba5a06b458",
   "metadata": {},
   "source": [
    "<p>\n",
    "    OSU and Big Bear: With the IRB approval (for the previous OSU dataset) in place, I would like to get working on the dataset asap. That means:\n",
    "    <ul>\n",
    "        <li>Getting it deidentified and sharable</li>\n",
    "        <li>If there is general demographic information like gender or level of expertise that can be associated with the VR performance, that would be useful.</li>\n",
    "        <li>Big Bear: once it is available to you, I want an exploratory analysis on decision points such that I can see the variance in responses to the same situation.</li>\n",
    "        <li>For example, if there are 3 waving patients and the participant chooses to move toward one and assess them, I want to know the sim state (visual info. on each patient, distance, etc.) and then a breakdown of how many participants chose to move toward each patient.</li>\n",
    "        <li>Similar for treatment options, for each patient present in the simulated environment, what is the variance in which treatment is applied</li>\n",
    "         <li>Does it depend on order?</li>\n",
    "          <li>Timing?</li>\n",
    "           <li>Be creative and ask all the questions. The goal is to provide TA1 with this dataset if there is anything useful to be found in analyzing the decisions (not necessarily that we find results but that the data is in shape to ask these questions). Let me know if you need additional clarification.</li>\n",
    "/</p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e283308-fa69-420e-8455-1752fe0db17c",
   "metadata": {},
   "source": [
    "\n",
    "### Patient Engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12799fc-7c14-4d71-9421-cdc018c7a9a4",
   "metadata": {},
   "source": [
    "\n",
    "These action types mean that the DM has made a decision: INJURY_TREATED, PATIENT_ENGAGED, PULSE_TAKEN, TAG_APPLIED, and TOOL_APPLIED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d4de80-1138-4d50-a0e3-91b9ae1a10ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>148879</th>\n",
       "      <th>450086</th>\n",
       "      <th>55389</th>\n",
       "      <th>271973</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>action_type</th>\n",
       "      <td>TOOL_HOVER</td>\n",
       "      <td>S_A_L_T_WALKED</td>\n",
       "      <td>TOOL_HOVER</td>\n",
       "      <td>S_A_L_T_WALK_IF_CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action_tick</th>\n",
       "      <td>245034</td>\n",
       "      <td>686716</td>\n",
       "      <td>318344</td>\n",
       "      <td>310231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <td>2022-12-07 12:45:50</td>\n",
       "      <td>2022-03-15 09:40:16</td>\n",
       "      <td>2023-03-15 10:48:40</td>\n",
       "      <td>2022-03-15 10:13:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_uuid</th>\n",
       "      <td>331f875e-eba1-4033-a502-6a888aee4e9c</td>\n",
       "      <td>090d0988-3f81-4603-87e2-477538a6750c</td>\n",
       "      <td>71197277-ba36-4a82-9ae0-0016e7756665</td>\n",
       "      <td>aec5d448-c4e6-4af7-8e36-d258c7bb6f96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_name</th>\n",
       "      <td>All CSV files renamed by date/12.07.22.1249.csv</td>\n",
       "      <td>Disaster Day 2022/MT_0950.csv</td>\n",
       "      <td>All CSV files renamed by date/03.15.23.1058.csv</td>\n",
       "      <td>Disaster Day 2022/JS_1016.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       148879  \\\n",
       "action_type                                        TOOL_HOVER   \n",
       "action_tick                                            245034   \n",
       "event_time                                2022-12-07 12:45:50   \n",
       "session_uuid             331f875e-eba1-4033-a502-6a888aee4e9c   \n",
       "file_name     All CSV files renamed by date/12.07.22.1249.csv   \n",
       "\n",
       "                                            450086  \\\n",
       "action_type                         S_A_L_T_WALKED   \n",
       "action_tick                                 686716   \n",
       "event_time                     2022-03-15 09:40:16   \n",
       "session_uuid  090d0988-3f81-4603-87e2-477538a6750c   \n",
       "file_name            Disaster Day 2022/MT_0950.csv   \n",
       "\n",
       "                                                       55389   \\\n",
       "action_type                                        TOOL_HOVER   \n",
       "action_tick                                            318344   \n",
       "event_time                                2023-03-15 10:48:40   \n",
       "session_uuid             71197277-ba36-4a82-9ae0-0016e7756665   \n",
       "file_name     All CSV files renamed by date/03.15.23.1058.csv   \n",
       "\n",
       "                                            271973  \n",
       "action_type                    S_A_L_T_WALK_IF_CAN  \n",
       "action_tick                                 310231  \n",
       "event_time                     2022-03-15 10:13:03  \n",
       "session_uuid  aec5d448-c4e6-4af7-8e36-d258c7bb6f96  \n",
       "file_name            Disaster Day 2022/JS_1016.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = frvrs_logs_df.sample(4).dropna(axis='columns', how='all').T\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ea553-ed7a-436a-aef7-6ba268effd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the actions in the same or previous minute to the end of the session\n",
    "actions_list = []\n",
    "for (session_uuid, scene_id), scene_df in fu.get_session_groupby(frvrs_logs_df, mask_series=None, extra_column='scene_id'):\n",
    "    mask_series = scene_df.action_type.isin(['SESSION_END'])\n",
    "    end_minutes_list = sorted(scene_df[mask_series].event_time.map(lambda x: x.replace(second=0, microsecond=0)).tolist())\n",
    "    all_minutes_list = sorted(scene_df.event_time.map(lambda x: x.replace(second=0, microsecond=0)).unique().tolist())\n",
    "    \n",
    "    previous_minutes_list = []\n",
    "    for event_time in end_minutes_list:\n",
    "        if event_time in all_minutes_list:\n",
    "            previous_element = all_minutes_list[all_minutes_list.index(event_time) - 1]\n",
    "            previous_minutes_list.append(previous_element)\n",
    "    \n",
    "    for (previous_minute, end_minute) in zip(previous_minutes_list, end_minutes_list):\n",
    "        mask_series = scene_df.event_time.map(lambda x: x.replace(second=0, microsecond=0)).isin([previous_minute, end_minute])\n",
    "        action_types_list = []\n",
    "        if scene_df[mask_series].shape[0] > 1: action_types_list = scene_df[mask_series].action_type.tolist()\n",
    "        if action_types_list: actions_list.append(action_types_list)\n",
    "len(actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1268a60-9f46-4ff1-a5f4-ce0b538832bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the actions in the same minute as the end of the session\n",
    "actions_list = []\n",
    "for (session_uuid, scene_id), scene_df in fu.get_session_groupby(frvrs_logs_df, mask_series=None, extra_column='scene_id'):\n",
    "    mask_series = scene_df.action_type.isin(['SESSION_END'])\n",
    "    end_minutes_list = sorted(scene_df[mask_series].event_time.map(lambda x: x.replace(second=0, microsecond=0)).tolist())\n",
    "    all_minutes_list = sorted(scene_df.event_time.map(lambda x: x.replace(second=0, microsecond=0)).unique().tolist())\n",
    "    \n",
    "    for end_minute in end_minutes_list:\n",
    "        mask_series = scene_df.event_time.map(lambda x: x.replace(second=0, microsecond=0)).isin([end_minute])\n",
    "        action_types_list = []\n",
    "        if scene_df[mask_series].shape[0] > 1: action_types_list = scene_df[mask_series].action_type.tolist()\n",
    "        if action_types_list: actions_list.append(action_types_list)\n",
    "len(actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860b183-2b2a-4ad9-8c88-5131f8097f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(actions_list).transform(actions_list)\n",
    "actions_one_hot_df = DataFrame(te_ary, columns=te.columns_)\n",
    "print(actions_one_hot_df.shape)\n",
    "print(actions_one_hot_df.columns.tolist())\n",
    "df = actions_one_hot_df.sample(min(4, actions_one_hot_df.shape[0])).dropna(axis='columns', how='all').T\n",
    "df.sample(min(4, df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290221ab-fbfe-405c-bee2-50d36c82ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "actions_itemsets_df = apriori(\n",
    "    actions_one_hot_df, min_support=0.01, use_colnames=True, max_len=3\n",
    ")\n",
    "print(actions_itemsets_df.shape)\n",
    "actions_itemsets_df.sample(min(4, actions_itemsets_df.shape[0])).dropna(axis='columns', how='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc962e96-6248-4b67-a8fa-c993deee860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if nu.pickle_exists('actions_rules_df'):\n",
    "    actions_rules_df = nu.load_object('actions_rules_df')\n",
    "else:\n",
    "    from mlxtend.frequent_patterns import association_rules\n",
    "    actions_rules_df = association_rules(actions_itemsets_df, metric='lift', min_threshold=0.75)\n",
    "    nu.store_objects(actions_rules_df=actions_rules_df)\n",
    "print(actions_rules_df.shape)\n",
    "display(actions_rules_df.sample(min(4, actions_rules_df.shape[0])).dropna(axis='columns', how='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e1fe1-c19c-41f6-a822-9782c95d0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = (actions_rules_df.consequents == frozenset({'SESSION_END'}))\n",
    "df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "if df.head(5).shape[0]:\n",
    "    print(f'Get the antecedent with the highest support for session end')\n",
    "    antecedents_list = [str(tuple(x)).replace(\"('\", '').replace(\"',)\", '') for x in df.head(5).antecedents.tolist()]\n",
    "    if antecedents_list:\n",
    "        print('The antecedents with highest lift and support to SESSION_END,', end='')\n",
    "        print(f' and probably therefore to PATIENT_ENGAGEMENT_END, are {nu.conjunctify_nouns(antecedents_list)}.')\n",
    "    display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7423cc-c446-48a2-a245-86e5b79a8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = (actions_rules_df.antecedents == frozenset({'VOICE_CAPTURE'}))\n",
    "df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "if df.head(5).shape[0]:\n",
    "    print(f'Get the consequent with the highest support for voice capture')\n",
    "    consequents_list = [str(tuple(x)) for x in df.head(5).consequents.tolist()]\n",
    "    if consequents_list:\n",
    "        print('The consequents with highest lift and support to VOICE_CAPTURE,', end='')\n",
    "        print(f' are {nu.conjunctify_nouns(consequents_list)}.')\n",
    "    display(df.head(5).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0c8e1-1fee-4fb4-bb50-e176682c0623",
   "metadata": {},
   "source": [
    "\n",
    "### Affinity Analysis and Association Rule Learning using the Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf567622-ae11-498c-bf95-ebd2e63e4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Record as transactions a more complex example of a patient engagement\n",
    "if nu.pickle_exists('all_transactions'):\n",
    "    all_transactions = nu.load_object('all_transactions')\n",
    "else:\n",
    "    all_transactions = []\n",
    "    for (session_uuid, scene_id), scene_df in fu.get_session_groupby(\n",
    "        frvrs_logs_df, mask_series=None, extra_column='scene_id'\n",
    "    ):\n",
    "        mask_series = (frvrs_logs_df.session_uuid == session_uuid) & (frvrs_logs_df.scene_id == scene_id)\n",
    "        action_types_list = frvrs_logs_df[mask_series].action_type.tolist()\n",
    "        all_transactions.append(action_types_list)\n",
    "    nu.store_objects(all_transactions=all_transactions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8fd9c62-3a67-4f27-a794-5630211b22fa",
   "metadata": {},
   "source": [
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Convert the list of lists to one hot\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(all_transactions).transform(all_transactions)\n",
    "df = DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Build the model\n",
    "itemsets_df = apriori(df, use_colnames=True)\n",
    "print(f'{itemsets_df.shape[0]:,} rules')\n",
    "itemsets_df.sort_values('support', ascending=False).head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ee5e8fe-1845-4659-84af-c4309166a688",
   "metadata": {},
   "source": [
    "\n",
    "# Collect the inferred rules in a data frame\n",
    "rules_df = association_rules(itemsets_df, metric='lift', min_threshold=1)\n",
    "rules_df = rules_df.sort_values(['confidence', 'lift'], ascending =[False, False])\n",
    "display(rules_df.head().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ac925-a641-4658-b831-6298230f20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_affinity_analysis(actions_type, verbose=False):\n",
    "    if verbose:\n",
    "        print(f'{actions_type.title()} Actions for Affinity Analysis and Association Rule Learning', flush=True)\n",
    "    \n",
    "    if verbose: print('Collect the inferred rules in a data frame', flush=True)\n",
    "    if nu.pickle_exists(f'{actions_type}_actions_rules_df'):\n",
    "        actions_rules_df = nu.load_object(f'{actions_type}_actions_rules_df')\n",
    "    else:\n",
    "        if verbose: print('Build the model', flush=True)\n",
    "        if nu.pickle_exists(f'{actions_type}_actions_itemsets_df'):\n",
    "            actions_itemsets_df = nu.load_object(f'{actions_type}_actions_itemsets_df')\n",
    "        else:\n",
    "            if verbose: print('Convert the list of lists to one hot', flush=True)\n",
    "            if nu.pickle_exists(f'{actions_type}_actions_one_hot_df'):\n",
    "                actions_one_hot_df = nu.load_object(f'{actions_type}_actions_one_hot_df')\n",
    "                print(f'{actions_type}_actions_one_hot_df.shape: {actions_one_hot_df.shape}', flush=True)\n",
    "            else:\n",
    "                from mlxtend.preprocessing import TransactionEncoder\n",
    "                te = TransactionEncoder()\n",
    "                if verbose: print('Record as actions a more complex example of a patient engagement', flush=True)\n",
    "                if nu.pickle_exists(f'{actions_type}_actions_list'):\n",
    "                    actions_list = nu.load_object(f'{actions_type}_actions_list')\n",
    "                else:\n",
    "                    actions_list = []\n",
    "                    if verbose: print('Get all CSVs out of one data frame', flush=True)\n",
    "                    if nu.pickle_exists('frvrs_logs_df'):\n",
    "                        frvrs_logs_df = nu.load_object('frvrs_logs_df')\n",
    "                    print(f'frvrs_logs_df.shape: {frvrs_logs_df.shape}', flush=True)\n",
    "                    for (session_uuid, action_tick), df in fu.get_session_groupby(frvrs_logs_df, mask_series=None, extra_column='action_tick'):\n",
    "                        action_types_list = []\n",
    "                        if df.shape[0] > 1: action_types_list = df.action_type.tolist()\n",
    "                        if action_types_list: actions_list.append(action_types_list)\n",
    "                    nu.store_objects(**{f'{actions_type}_actions_list': actions_list})\n",
    "                if verbose:\n",
    "                    list_length = len(actions_list)\n",
    "                    print(\n",
    "                        f'We have {list_length:,} simultaneous actions in our list. Here are some examples:',\n",
    "                        flush=True\n",
    "                    )\n",
    "                    max_length = 0\n",
    "                    max_list = []\n",
    "                    counter = 0\n",
    "                    import random\n",
    "                    for action_list in actions_list:\n",
    "                        if (counter < 5) and (random.random() <= 10/list_length):\n",
    "                            print(action_list)\n",
    "                            counter += 1\n",
    "                        if max_length < len(action_list):\n",
    "                            max_length = len(action_list)\n",
    "                            max_list = action_list\n",
    "                    print(\n",
    "                        f'\\nIn one sim log, these {max_length} actions occur simultaneously:'\n",
    "                        f'{nu.conjunctify_nouns(max_list)}.'\n",
    "                    )\n",
    "                te_ary = te.fit(actions_list).transform(actions_list)\n",
    "                actions_one_hot_df = DataFrame(te_ary, columns=te.columns_)\n",
    "                nu.store_objects(**{f'{actions_type}_actions_one_hot_df': actions_one_hot_df})\n",
    "            print(f'{actions_type}_actions_one_hot_df.shape: {actions_one_hot_df.shape}', flush=True)\n",
    "            from mlxtend.frequent_patterns import apriori\n",
    "            actions_itemsets_df = apriori(\n",
    "                actions_one_hot_df, min_support=0.01, use_colnames=True, max_len=50\n",
    "            )\n",
    "            actions_itemsets_df['itemsets_size'] = actions_itemsets_df.itemsets.map(lambda x: len(eval(str(x))))\n",
    "            nu.store_objects(**{f'{actions_type}_actions_itemsets_df': actions_itemsets_df})\n",
    "        print(f'{actions_type}_actions_itemsets_df.shape: {actions_itemsets_df.shape}', flush=True)\n",
    "        display(actions_itemsets_df.sort_values('itemsets_size', ascending=False).head(5))\n",
    "        from mlxtend.frequent_patterns import association_rules\n",
    "        actions_rules_df = association_rules(actions_itemsets_df, metric='lift', min_threshold=0.75)\n",
    "        nu.store_objects(**{f'{actions_type}_actions_rules_df': actions_rules_df})\n",
    "    print(f'{actions_type}_actions_rules_df.shape: {actions_rules_df.shape}', flush=True)\n",
    "    if verbose:\n",
    "        display(actions_rules_df.head().T)\n",
    "        metric_dict = {}\n",
    "        for metric in [\n",
    "            'antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction',\n",
    "            'zhangs_metric'\n",
    "        ]:\n",
    "            max_metric = actions_rules_df[metric].max()\n",
    "            print(metric)\n",
    "            metric_dict[metric] = max_metric\n",
    "            mask_series = (actions_rules_df[metric] == max_metric)\n",
    "            display(actions_rules_df[mask_series].head())\n",
    "        metrics_list = sorted(\n",
    "            [(metric, max_metric) for metric, max_metric in metric_dict.items()], key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        display(actions_rules_df.sort_values([x[0] for x in metrics_list], ascending=[False]*len(metrics_list)))\n",
    "    \n",
    "    mask_series = (actions_rules_df.antecedents == frozenset({'PATIENT_ENGAGED'}))\n",
    "    df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "    if df.head(5).shape[0]:\n",
    "        print(f'Get the consequent with the highest support for patient engaged')\n",
    "        display(df.head(5))\n",
    "    \n",
    "    mask_series = (actions_rules_df.consequents == frozenset({'PATIENT_ENGAGED'}))\n",
    "    df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "    if df.head(5).shape[0]:\n",
    "        print(f'Get the antecedent with the highest support for patient engaged')\n",
    "        display(df.head(5))\n",
    "    \n",
    "    mask_series = (actions_rules_df.consequents == frozenset({'SESSION_END'}))\n",
    "    df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "    if df.head(5).shape[0]:\n",
    "        print(f'Get the antecedent with the highest support for session end')\n",
    "        antecedents_list = [str(tuple(x)) for x in df.head(5).antecedents.tolist()]\n",
    "        if antecedents_list:\n",
    "            print('The antecedents with highest lift and support to SESSION_END,', end='')\n",
    "            print(f' and probably therefore to PATIENT_ENGAGEMENT_END, are {nu.conjunctify_nouns(antecedents_list)}.')\n",
    "        display(df.head(5))\n",
    "    \n",
    "    mask_series = (actions_rules_df.antecedents == frozenset({'SESSION_START'}))\n",
    "    df = actions_rules_df[mask_series].sort_values(['lift', 'antecedent support'], ascending=[False, False])\n",
    "    if df.head(5).shape[0]:\n",
    "        print(f'Get the consequent with the highest support for session start')\n",
    "        display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875de84-3475-4e93-a526-f3708565ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "do_affinity_analysis(actions_type='next', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b2bed-e558-4ffc-957b-ba2d55c0e51e",
   "metadata": {},
   "source": [
    "\n",
    "## Do Affinity analysis on the actions that are logged at the exact same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8433d-64cd-4a0d-b795-893226fd2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "do_affinity_analysis(actions_type='simultaneous', verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

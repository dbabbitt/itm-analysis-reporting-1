{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51ab3c6-9bc9-4299-86d6-64763b66e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "if (osp.join('..', 'py') not in sys.path): sys.path.insert(1, osp.join('..', 'py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c96aaf-34eb-4e78-b477-d2e6bd721e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import (nu, DataFrame, osp, re, walk, Series, DataFrame, display, concat)\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=nu,\n",
    "    secrets_json_path=osp.abspath(osp.join(nu.data_folder, 'secrets', 'itm_secrets.json'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ffe8e-6573-470f-aef5-348522a0de15",
   "metadata": {},
   "source": [
    "\n",
    "# Parse Domain Documents for Entities\n",
    "\n",
    "Downloaded all documents from https://nextcentury.atlassian.net/wiki/spaces/ITMC/pages/2991849482/Domain+Documents and converted them all to PDF files and stored them in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea43e5c-ac11-4453-accc-78c7f84e4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "pdf_folder = '../data/Domain_Knowledge/OSU Additions'\n",
    "black_list = ['.ipynb_checkpoints', '$Recycle.Bin', '.git']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053cef1-4571-4abd-9868-2b0de4bf37de",
   "metadata": {},
   "source": [
    "\n",
    "## Option 1: Use a Hugging Face NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da6aee2-cb75-4121-9285-7dd8fa91887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9988381, 'index': 1, 'word': 'Barack', 'start': 0, 'end': 6}, {'entity': 'I-PER', 'score': 0.9994398, 'index': 2, 'word': 'Obama', 'start': 7, 'end': 12}, {'entity': 'I-LOC', 'score': 0.9983613, 'index': 10, 'word': 'United', 'start': 43, 'end': 49}, {'entity': 'I-LOC', 'score': 0.9920671, 'index': 11, 'word': 'States', 'start': 50, 'end': 56}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Named entity recognition pipeline, passing in a specific model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "token_classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "sentence = 'Barack Obama was the 44th President of the United States.'\n",
    "tokens = token_classifier(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2547e-6a16-4772-b317-eacac8a443c9",
   "metadata": {},
   "source": [
    "\n",
    "## Option 2: Use SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "003f66f0-f67f-47f2-88eb-b2bbb6d33791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Barack', 'tag_': 'NNP', 'ent_type_': 'PERSON', 'pos_': 'PROPN'}, {'text': 'Obama', 'tag_': 'NNP', 'ent_type_': 'PERSON', 'pos_': 'PROPN'}, {'text': 'was', 'tag_': 'VBD', 'ent_type_': '', 'pos_': 'AUX'}, {'text': 'the', 'tag_': 'DT', 'ent_type_': '', 'pos_': 'DET'}, {'text': '44th', 'tag_': 'JJ', 'ent_type_': 'ORDINAL', 'pos_': 'ADJ'}, {'text': 'President', 'tag_': 'NNP', 'ent_type_': '', 'pos_': 'PROPN'}, {'text': 'of', 'tag_': 'IN', 'ent_type_': '', 'pos_': 'ADP'}, {'text': 'the', 'tag_': 'DT', 'ent_type_': 'GPE', 'pos_': 'DET'}, {'text': 'United', 'tag_': 'NNP', 'ent_type_': 'GPE', 'pos_': 'PROPN'}, {'text': 'States', 'tag_': 'NNP', 'ent_type_': 'GPE', 'pos_': 'PROPN'}, {'text': '.', 'tag_': '.', 'ent_type_': '', 'pos_': 'PUNCT'}]\n",
      "[{'text': 'Barack Obama', 'label_': 'PERSON'}, {'text': '44th', 'label_': 'ORDINAL'}, {'text': 'the United States', 'label_': 'GPE'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "try: nlp = spacy.load('en_core_web_sm')\n",
    "except OSError as e:\n",
    "    print(str(e).strip())\n",
    "    command_str = f'{sys.executable} -m spacy download en_core_web_sm --quiet'\n",
    "    print(command_str)\n",
    "    !{command_str}\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Example usage\n",
    "sentence = 'Barack Obama was the 44th President of the United States.'\n",
    "doc = nlp(sentence)\n",
    "print([{'text': word.text, 'tag_': word.tag_, 'ent_type_': word.ent_type_, 'pos_': word.pos_} for word in doc])\n",
    "print([{'text': ent.text, 'label_': ent.label_} for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba76ba4-1188-4912-80b4-c9cb20ec01f0",
   "metadata": {},
   "source": [
    "\n",
    "## Extract the text from PDFs and load it into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b51a88a-6ed0-4b50-9011-0bdefd9b5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def convert(file_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Convert PDF, return its text content as a string\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text += page.extract_text()\n",
    "    if verbose: print(f'Text length for {file_path} is {len(text):,} characters.')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa20875-045c-4579-8150-1c5e406fb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentences_dictionary(pdf_folder, verbose=False):\n",
    "    domain_knowledge_sentences_dict = {}\n",
    "    for sub_directory, directories_list, files_list in walk(pdf_folder):\n",
    "        if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "            if any(map(lambda x: x.endswith('.txt'), files_list)):\n",
    "                for file_name in files_list:\n",
    "                    if file_name.endswith('.txt'):\n",
    "                        file_path = osp.join(sub_directory, file_name)\n",
    "                        with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                            text = f.read()\n",
    "                            domain_knowledge_sentences_dict[file_path] = text\n",
    "            else:\n",
    "                for file_name in files_list:\n",
    "                    if file_name.endswith('.pdf'):\n",
    "                        file_path = osp.join(sub_directory, file_name)\n",
    "                        text = convert(file_path, verbose=True)\n",
    "                        domain_knowledge_sentences_dict[file_path] = text\n",
    "    \n",
    "    return domain_knowledge_sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3655159d-289f-4306-b6ac-6c70ee1d02c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/domain_knowledge_sentences_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get text from PDFs\n",
    "if nu.pickle_exists('domain_knowledge_sentences_dict'): domain_knowledge_sentences_dict = nu.load_object('domain_knowledge_sentences_dict')\n",
    "else:\n",
    "    domain_knowledge_sentences_dict = get_sentences_dictionary(pdf_folder, verbose=False)\n",
    "    nu.store_objects(domain_knowledge_sentences_dict=domain_knowledge_sentences_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64e3819a-c3e1-4fc2-a4b7-4f9daeae85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assert that you got all the hyphenated word wrappings out\n",
    "for file_path, text in domain_knowledge_sentences_dict.items():\n",
    "    assert not ('effec-' in text), f'{file_path} still has hyphenated word wrappings.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e474dff1-2f02-4ab6-b5f6-61b8eda7af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load documents\n",
    "if nu.csv_exists('domain_doc_ners_df'): domain_doc_ners_df = nu.load_data_frames(domain_doc_ners_df='domain_doc_ners_df')['domain_doc_ners_df']\n",
    "else:\n",
    "    entities = []\n",
    "    for file_path, text in domain_knowledge_sentences_dict.items():\n",
    "        text_length = len(text)\n",
    "        # print(f'Text length for {file_path} is {text_length:,} characters.')\n",
    "        \n",
    "        # Prepare to join subword tokens back together and keep track of entity and score\n",
    "        output_words = []\n",
    "        current_word = ''\n",
    "        current_entities = []\n",
    "        current_scores = []\n",
    "        current_starts = []\n",
    "        current_ends = []\n",
    "        \n",
    "        # Extract metadata from entity recognition pipeline and add it as a row dictionary to the entities rows list\n",
    "        tokens = token_classifier(text)\n",
    "        for metadata_dict in tokens:\n",
    "            current_entities.append(metadata_dict['entity'])\n",
    "            current_scores.append(metadata_dict['score'])\n",
    "            current_starts.append(metadata_dict['start'])\n",
    "            current_ends.append(metadata_dict['end'])\n",
    "            if metadata_dict['word'].startswith('##'): current_word += metadata_dict['word'][2:]\n",
    "            else:\n",
    "                \n",
    "                # Take the mode of entities and average of scores for the current_word\n",
    "                if current_word:\n",
    "                    if len(current_entities) > 1: current_entities = current_entities[:-1]\n",
    "                    if len(current_scores) > 1: current_scores = current_scores[:-1]\n",
    "                    if len(current_ends) > 2: current_ends = current_ends[:-1]\n",
    "                    mode_entity = Series(current_entities).mode().tolist()[-1]\n",
    "                    mean_score = Series(current_scores).mean()\n",
    "                    start_idx = current_starts[0]\n",
    "                    end_idx = current_ends[-1]\n",
    "                    entity_tuple = (current_word, mode_entity, mean_score, start_idx, end_idx)\n",
    "                    output_words.append(entity_tuple)\n",
    "                    current_word = ''\n",
    "                    current_entities = []\n",
    "                    current_scores = []\n",
    "                    current_starts = []\n",
    "                    current_ends = []\n",
    "                else:\n",
    "                    current_word = metadata_dict['word']\n",
    "                    current_entities = [metadata_dict['entity']]\n",
    "                    current_scores = [metadata_dict['score']]\n",
    "                    current_starts = [metadata_dict['start']]\n",
    "                    current_ends = [metadata_dict['end']]\n",
    "        \n",
    "        # Take the mode of entities for the last current_word\n",
    "        if current_word:\n",
    "            mode_entity = Series(current_entities).mode().tolist()[-1]\n",
    "            mean_score = Series(current_scores).mean()\n",
    "            start_idx = current_starts[0]\n",
    "            end_idx = current_ends[-1]\n",
    "            entity_tuple = (current_word, mode_entity, mean_score, start_idx, end_idx)\n",
    "            output_words.append(entity_tuple)\n",
    "        \n",
    "        for word, entity, score, start, end in output_words:\n",
    "            metadata_dict = {'bert_word': word, 'bert_entity': entity, 'bert_score': score, 'bert_start': start, 'bert_end': end, 'file_path': file_path}\n",
    "            entities.append(metadata_dict)\n",
    "\n",
    "        # Extract SpaCy named entities and add them as a row dictionary to the entities rows list\n",
    "        if text_length <= nlp.max_length:\n",
    "            doc = nlp(text)\n",
    "            entities.extend([\n",
    "                {'file_path': file_path, 'nlp_word': word.text, 'nlp_tag': word.tag_, 'nlp_type': word.ent_type_, 'nlp_pofs': word.pos_}\n",
    "                for word in doc\n",
    "            ])\n",
    "            entities.extend([\n",
    "                {'file_path': file_path, 'ent_phrase': ent.text, 'ent_type': ent.label_, 'ent_start': ent.start_char, 'ent_end': ent.end_char}\n",
    "                for ent in doc.ents\n",
    "            ])\n",
    "    domain_doc_ners_df = DataFrame(entities)\n",
    "    nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5af4fbc1-a318-44b5-a730-a4d977276b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert_word', 'bert_entity', 'bert_score', 'bert_start', 'bert_end', 'file_path', 'nlp_word', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_phrase', 'ent_type', 'ent_start', 'ent_end']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assert that all the subword tokens are gone\n",
    "print(domain_doc_ners_df.columns.tolist())\n",
    "mask_series = domain_doc_ners_df.bert_word.map(lambda x: str(x).startswith('##'))\n",
    "df = domain_doc_ners_df[mask_series]\n",
    "assert (df.shape[0] == 0), 'There still exist subword tokens.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fc27787-e268-4c03-b810-8e9eacbdc812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check that you indeed have only strings among the BERT entities\n",
    "mask_series = domain_doc_ners_df.bert_entity.isnull()\n",
    "sorted(domain_doc_ners_df[~mask_series].bert_entity.tolist(), key=lambda x: len(str(x)), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b311d7-0d52-4ccd-bb6e-07d5dcc69257",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the entity type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67525ee5-93a2-45ca-870d-5581fa9fafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORG', 'LAW', 'PERSON', 'MONEY', 'EVENT', 'LOC', 'PRODUCT', 'CARDINAL', 'QUANTITY', 'WORK_OF_ART']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CARDINAL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['9', '400', '3', '102', '20', '93', '1510.25', '4:1', '1-31', '1:1']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EVENT'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Operation Enduring Freedom', 'Operation Iraqi Freedom (OIF', 'Generation I and II IFAKs', 'The Generation II IFAK']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LAW'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Chapter 3', 'Option 2', '• •Check', 'Chapter 4\\nMARCH/PAWS Treatment Algorithms', 'the Warrior Aid', 'Basic Management Plan for Tactical Field Care', 'Basic Management Plan for Care Under Fire/Threat', 'Chapter 2', 'Universal Splint 2', 'Nasal Trumpet 3\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LOC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['TCCC', '• Extraglottic', 'the Battlefield Part II', 'Black Talon', '• •Cut', 'analgesia', 'Luer', 'Elastic', 'Joint Publication', 'http://www.naemt.org/education/TCCC/guidelines_curriculum']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MONEY'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['#', '90  \\n• Hypothermia  \\n• End', '500 mL bolus', '100 ml']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['AHS', 'Expeditionary Medical Support', 'OTFC', '− Direct', 'Hypothermia Prevention and Management Kit', 'Special Forces', 'C. Extraction', 'the Rapid Fielding Initiative', 'GlideScope', '• •Medical Simulation and Training Centers']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Flurbiprofen', '•••*NPA', '•Any', 'Andre Cap', 'Russ Kotwal', 'James Czarnik', '− Cover', 'MD CAPT', 'Zofran', '− Check']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PRODUCT'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['• Chin', 'CRTS', 'a TCCC Card', 'c. Estimate', 'Task Evaluation', 'Tourniquet Conversion', '• •Assess', 'Fourth Edition', '•', '• Nasopharyngeal']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'QUANTITY'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['2 inches', '90-degree', '1 mg/2 ml', '4 1/2 inch', '8 cm', '3 inches', '6 mm', '90 degrees', '10 kg ABOVE', '30 degrees']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WORK_OF_ART'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Intraosseous Access with the Sternal EZ-IO Needle Set\\nThe Sternal EZ-IO', '− Reassess', 'Request for Publications', '− Minimize', '•− News From the Front\\n•− Training Lessons', 'Publications by Type', 'Aeromedical Evacuation', 'Fundamentals of Combat Casualty Care', 'Tactical Evacuation', 'A = Panels\\nB = Pyrotechnic']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print a sample of the ent_type column\n",
    "categories_list = sorted([str(w) for w in domain_doc_ners_df.ent_type.unique()])\n",
    "categories_list = random.sample(categories_list, min(len(categories_list), 10))\n",
    "print(categories_list)\n",
    "mask_series = domain_doc_ners_df.ent_type.isin(categories_list)\n",
    "for type, type_df in domain_doc_ners_df[mask_series].groupby('ent_type'):\n",
    "    mask_series = type_df.ent_phrase.isnull()\n",
    "    texts_list = sorted(type_df[~mask_series].ent_phrase.unique())\n",
    "    print()\n",
    "    display(type, random.sample(texts_list, min(len(texts_list), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "303fbcc3-6738-4029-899c-82865d46a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WORK_OF_ART', 'PERCENT', 'LOC', 'QUANTITY', 'DATE', 'PERSON', 'NORP', 'GPE', 'FAC', 'ORDINAL']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DATE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['242018Z', '5163120', '519', 'mid', '527', '1016', '1350', '524', '3204', '532']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Palpated', '•', 'Casualty', 'Blizzard', 'Gram', 'Reflective', 'Key', 'Blanket', '•Move', 'Airway']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['tibia', '−', 'mmHG', 'MD', 'Chicago', 'IO', '.', 'NS', 'U.S.', 'Afghanistan']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LOC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Black', 'Publication', 'analgesia', 'II', '•Cut', 'TCCC', 'Elastic', 'http://www.naemt.org/education/TCCC/guidelines_curriculum', 'Luer', 'Extraglottic']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NORP'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['D.', 'Line', 'anesthetist', 'Heimlich', 'Melker', 'Soldier', 'Combat', 'Litter', 'q45min', 'R.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ORDINAL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['6th', 'fifth', 'fourth', 'third', '5th', '3rd', 'second', '2nd', 'first', 'First']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERCENT'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['than', 'or', '33', 'greater', '10', '87', 'to', '20', 'percent', 'More']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Landing', 'Cover', 'Newsletters', 'Jay', 'Maitha', '\\n', 'Lubricated', 'Donovan', 'Add', 'Expert']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'QUANTITY'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['2', 'milliliter', '6', 'ABOVE', 'centimeter', 'one', '/', '400', 'over', 'at']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WORK_OF_ART'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Management', 'Evacuation', 'Type', 'Shock', ':', 'Care', 'for', 'Request', 'EZ', 'A']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print a sample of the nlp_type column\n",
    "categories_list = sorted([str(w) for w in domain_doc_ners_df.nlp_type.unique()])\n",
    "categories_list = random.sample(categories_list, min(len(categories_list), 10))\n",
    "print(categories_list)\n",
    "mask_series = domain_doc_ners_df.nlp_type.isin(categories_list)\n",
    "for type, type_df in domain_doc_ners_df[mask_series].groupby('nlp_type'):\n",
    "    mask_series = type_df.nlp_word.isnull()\n",
    "    texts_list = sorted(type_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    display(type, random.sample(texts_list, min(len(texts_list), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4374d-c26b-48a4-85ac-4663ab97f480",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the tag and parts-of-speech columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f34b89cb-4446-43c1-aeda-72d764a5dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan', 'WP', 'UH', 'DT', 'VBD', '_SP', 'TO', 'VBG', 'POS', 'VBZ']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['all', 'this', 'any', 'both', 'either', 'The', 'these', 'some', 'an', 'half']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['s', \"'s\", '’s']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TO'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['to', 'To']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'UH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['O', 'Please', 'please']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VBD'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['inserted', 'swung', 'Inserted', 'fasten', 'Applied', 'uncontrolled', 'were', '•Assess', 'cut', 'was']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VBG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['•Using', 'Folding', 'executing', 'aiming', 'becoming', 'spurting', 'speaking', 'marking', '•Secure', 'performing']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VBZ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['conducts', 'increases', 'has', 'is', 'consists', 'develops', 'helps', 'feels', 'scars', 'writes']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WP'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['•Cover', 'whom', 'who', 'what']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_SP'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['  \\n', '  ', '\\n \\n \\n', ' \\n   ', ' \\n \\n', ' \\n \\n \\n', '      ', '\\n\\n\\n', ' ', '\\n   ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print a sample of the nlp_tag column\n",
    "categories_list = sorted([str(w) for w in domain_doc_ners_df.nlp_tag.unique()])\n",
    "categories_list = random.sample(categories_list, min(len(categories_list), 10))\n",
    "print(categories_list)\n",
    "mask_series = domain_doc_ners_df.nlp_tag.isin(categories_list)\n",
    "for tag, tag_df in domain_doc_ners_df[mask_series].groupby('nlp_tag'):\n",
    "    mask_series = tag_df.nlp_word.isnull()\n",
    "    texts_list = sorted(tag_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    display(tag, random.sample(texts_list, min(len(texts_list), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee497e3-2371-41a9-8e9f-897535af1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCONJ', 'nan', 'PROPN', 'AUX', 'CCONJ', 'X', 'DET', 'NUM', 'ADV', 'SPACE']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ADV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Therefore', 'regardless', 'Later', 'over', 'scientifically', 'least', 'longer', 'now', 'Continuously', 'readily']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AUX'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['is', 'been', 'May', 'can', '•Did', 'Do', 'must', 'may', 'has', 'were']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CCONJ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['and', 'or', 'but', 'Either', 'nor', '•Place', 'OR', 'either', 'so', 'AND']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DET'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['A', 'half', 'a', 'whose', 'the', 'no', 'No', 'that', 'Each', 'another']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NUM'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['68', '119', '89', '1:1', '106', '4.5', '9', '8-', '5,000', 'six']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Pharyngeal', 'SOCIAL', 'Biological', 'Nored', 'Ankle', 'Arms', 'D.', 'MEDIA', 'TEC', 'Travis']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SCONJ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['for', 'After', 'so', 'While', 'whether', 'while', 'unless', 'As', 'after', 'when']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SPACE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['\\n \\n', ' \\n \\n                   ', ' \\n \\n \\n', '\\n ', ' \\n  \\n    ', '\\n   ', ' ', ' \\n   \\n \\n \\n ', '\\n \\n ', ' \\n ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['15', '•Begin', '9', '•', '•Check', '.', '•••*Replace', 'TRAC2ES', 'MEDPROS', '•••*Combat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print a sample of the nlp_pofs column\n",
    "categories_list = sorted([str(w) for w in domain_doc_ners_df.nlp_pofs.unique()])\n",
    "categories_list = random.sample(categories_list, min(len(categories_list), 10))\n",
    "print(categories_list)\n",
    "mask_series = domain_doc_ners_df.nlp_pofs.isin(categories_list)\n",
    "for pofs, pofs_df in domain_doc_ners_df[mask_series].groupby('nlp_pofs'):\n",
    "    mask_series = pofs_df.nlp_word.isnull()\n",
    "    texts_list = sorted(pofs_df[~mask_series].nlp_word.unique())\n",
    "    print()\n",
    "    display(pofs, random.sample(texts_list, min(len(texts_list), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006132dd-12de-438d-b0ad-792b98361f70",
   "metadata": {},
   "source": [
    "\n",
    "## Explore the entity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f017890b-f346-4fe8-9072-ca002dc5e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-LOC', 'I-ORG', 'I-MISC', 'nan']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I-LOC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Afghanistan', 'S']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I-MISC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['US', 'Cas', 'DIA', 'Care', 'CareCCC', 'ian', 'Handbook', 'Tactical', 'as', 'lines']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I-ORG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['TCCC', 'CCC', 'All', 'CoTCCC', 'on', 'Co', 'Combat', 'S']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print a sample of the bert_entity column\n",
    "categories_list = sorted([str(w) for w in domain_doc_ners_df.bert_entity.unique()])\n",
    "categories_list = random.sample(categories_list, min(len(categories_list), 10))\n",
    "print(categories_list)\n",
    "mask_series = domain_doc_ners_df.bert_entity.isin(categories_list)\n",
    "for entity, entity_df in domain_doc_ners_df[mask_series].groupby('bert_entity'):\n",
    "    mask_series = entity_df.bert_word.isnull()\n",
    "    texts_list = sorted(entity_df[~mask_series].bert_word.unique())\n",
    "    print()\n",
    "    display(entity, random.sample(texts_list, min(len(texts_list), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3e573-63f1-4473-9b97-0c5dc3139db5",
   "metadata": {},
   "source": [
    "\n",
    "## Explore column groupbys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05db7cd4-1b8f-437f-930c-31b11657e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Domain_Knowledge/OSU Additions/17-13-tactical-casualty-combat-care-handbook-v5-may-17-distro-a (1).txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7220</th>\n",
       "      <th>7748</th>\n",
       "      <th>22189</th>\n",
       "      <th>4312</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file_path</th>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_word</th>\n",
       "      <td>axillary</td>\n",
       "      <td>.</td>\n",
       "      <td>easily</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_tag</th>\n",
       "      <td>JJ</td>\n",
       "      <td>.</td>\n",
       "      <td>RB</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_type</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_pofs</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>ADV</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       7220   \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                            axillary   \n",
       "nlp_tag                                                   JJ   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                                 ADJ   \n",
       "\n",
       "                                                       7748   \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                                   .   \n",
       "nlp_tag                                                    .   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                               PUNCT   \n",
       "\n",
       "                                                       22189  \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                              easily   \n",
       "nlp_tag                                                   RB   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                                 ADV   \n",
       "\n",
       "                                                       4312   \n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...  \n",
       "nlp_word                                                   .  \n",
       "nlp_tag                                                    .  \n",
       "nlp_type                                                      \n",
       "nlp_pofs                                               PUNCT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for file_path, file_path_df in domain_doc_ners_df.groupby('file_path'):\n",
    "    print(file_path)\n",
    "    display(file_path_df.sample(4).dropna(axis='columns', how='all').T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5cfa2c2-df23-4d0e-882d-724657e82c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26745</th>\n",
       "      <th>26810</th>\n",
       "      <th>3478</th>\n",
       "      <th>23674</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file_path</th>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "      <td>../data/Domain_Knowledge/OSU Additions/17-13-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_word</th>\n",
       "      <td>”</td>\n",
       "      <td>”</td>\n",
       "      <td>’</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_tag</th>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_type</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp_pofs</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       26745  \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                                   ”   \n",
       "nlp_tag                                                   ''   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                               PUNCT   \n",
       "\n",
       "                                                       26810  \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                                   ”   \n",
       "nlp_tag                                                   ''   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                               PUNCT   \n",
       "\n",
       "                                                       3478   \\\n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...   \n",
       "nlp_word                                                   ’   \n",
       "nlp_tag                                                   ''   \n",
       "nlp_type                                                       \n",
       "nlp_pofs                                               PUNCT   \n",
       "\n",
       "                                                       23674  \n",
       "file_path  ../data/Domain_Knowledge/OSU Additions/17-13-t...  \n",
       "nlp_word                                                   ”  \n",
       "nlp_tag                                                   ''  \n",
       "nlp_type                                                      \n",
       "nlp_pofs                                               PUNCT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for nlp_tag, nlp_tag_df in domain_doc_ners_df.groupby('nlp_tag'):\n",
    "    if (nlp_tag_df.shape[0] >= 4):\n",
    "        print(nlp_tag)\n",
    "        display(nlp_tag_df.sample(4).dropna(axis='columns', how='all').T)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d5fc8-c5ea-4aa2-9397-95726eed6015",
   "metadata": {},
   "source": [
    "\n",
    "## Explore combinations of BERT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17a34216-30de-48dc-b474-392799c5ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_entity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>ian</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.550099</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tactical</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.952839</td>\n",
       "      <td>290.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "30463          ian      I-MISC    0.550099      1132.0    1143.0   \n",
       "11              as      I-MISC    0.485629      1505.0    1517.0   \n",
       "0         Tactical      I-MISC    0.952839       290.0     305.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30463                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "11     17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "0      17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30883</th>\n",
       "      <td>lines</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.576725</td>\n",
       "      <td>46.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>ian</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.550099</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "30883        lines      I-MISC    0.576725        46.0     159.0   \n",
       "30463          ian      I-MISC    0.550099      1132.0    1143.0   \n",
       "11              as      I-MISC    0.485629      1505.0    1517.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30883      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "30463                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "11     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30883</th>\n",
       "      <td>lines</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.576725</td>\n",
       "      <td>46.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>ian</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.550099</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "30883        lines      I-MISC    0.576725        46.0     159.0   \n",
       "30463          ian      I-MISC    0.550099      1132.0    1143.0   \n",
       "11              as      I-MISC    0.485629      1505.0    1517.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30883      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "30463                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "11     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_word and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30883</th>\n",
       "      <td>lines</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.576725</td>\n",
       "      <td>46.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>ian</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.550099</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "30883        lines      I-MISC    0.576725        46.0     159.0   \n",
       "30463          ian      I-MISC    0.550099      1132.0    1143.0   \n",
       "11              as      I-MISC    0.485629      1505.0    1517.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30883      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "30463                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "11     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.974557</td>\n",
       "      <td>988.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.954088</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Combat</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.898376</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.742123</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "7          S       I-LOC    0.974557       988.0    1233.0   \n",
       "9         on       I-ORG    0.954088      1485.0    1496.0   \n",
       "10    Combat       I-ORG    0.898376      1497.0    1505.0   \n",
       "14         S       I-ORG    0.742123      1954.0    2068.0   \n",
       "\n",
       "                                            file_path  \n",
       "7   17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "9   17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "10  17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "14  17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>All</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.559561</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30887</th>\n",
       "      <td>CCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.520630</td>\n",
       "      <td>1531.0</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoTCCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.990001</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "13             All       I-ORG    0.559561      1944.0    1953.0   \n",
       "30887          CCC       I-ORG    0.520630      1531.0    1534.0   \n",
       "12          CoTCCC       I-ORG    0.990001      1519.0    1525.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "13     17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30887      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "12     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_entity and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>611.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>All</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.559561</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30887</th>\n",
       "      <td>CCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.520630</td>\n",
       "      <td>1531.0</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoTCCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.990001</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "6      Afghanistan       I-LOC    0.999079       611.0     987.0   \n",
       "13             All       I-ORG    0.559561      1944.0    1953.0   \n",
       "30887          CCC       I-ORG    0.520630      1531.0    1534.0   \n",
       "12          CoTCCC       I-ORG    0.990001      1519.0    1525.0   \n",
       "\n",
       "                                               file_path  \n",
       "6      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "13     17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30887      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "12     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score and bert_start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DIA</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.462606</td>\n",
       "      <td>364.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30465</th>\n",
       "      <td>US</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.984484</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.974557</td>\n",
       "      <td>988.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30464</th>\n",
       "      <td>US</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "3           DIA      I-MISC    0.462606       364.0     367.0   \n",
       "30465        US      I-MISC    0.984484      1164.0    1166.0   \n",
       "7             S       I-LOC    0.974557       988.0    1233.0   \n",
       "30464        US      I-MISC    0.968811      1144.0    1163.0   \n",
       "\n",
       "                                               file_path  \n",
       "3      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30465                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "7      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30464                   9-LINE MEDEVAC AND MIST PREP.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DIA</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.462606</td>\n",
       "      <td>364.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30465</th>\n",
       "      <td>US</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.984484</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.974557</td>\n",
       "      <td>988.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30464</th>\n",
       "      <td>US</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>9-LINE MEDEVAC AND MIST PREP.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "3           DIA      I-MISC    0.462606       364.0     367.0   \n",
       "30465        US      I-MISC    0.984484      1164.0    1166.0   \n",
       "7             S       I-LOC    0.974557       988.0    1233.0   \n",
       "30464        US      I-MISC    0.968811      1144.0    1163.0   \n",
       "\n",
       "                                               file_path  \n",
       "3      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30465                   9-LINE MEDEVAC AND MIST PREP.txt  \n",
       "7      17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30464                   9-LINE MEDEVAC AND MIST PREP.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_start and bert_end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_word</th>\n",
       "      <th>bert_entity</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bert_start</th>\n",
       "      <th>bert_end</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30880</th>\n",
       "      <td>Combat</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>All</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.559561</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30887</th>\n",
       "      <td>CCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.520630</td>\n",
       "      <td>1531.0</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>Tactical Combat Casualty Care (TCCC) 2021.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoTCCC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.990001</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>17-13-tactical-casualty-combat-care-handbook-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bert_word bert_entity  bert_score  bert_start  bert_end  \\\n",
       "30880    Combat      I-MISC    0.575667        12.0      20.0   \n",
       "13          All       I-ORG    0.559561      1944.0    1953.0   \n",
       "30887       CCC       I-ORG    0.520630      1531.0    1534.0   \n",
       "12       CoTCCC       I-ORG    0.990001      1519.0    1525.0   \n",
       "\n",
       "                                               file_path  \n",
       "30880      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "13     17-13-tactical-casualty-combat-care-handbook-v...  \n",
       "30887      Tactical Combat Casualty Care (TCCC) 2021.txt  \n",
       "12     17-13-tactical-casualty-combat-care-handbook-v...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "\n",
    "columns_list = [cn for cn in domain_doc_ners_df.columns if cn.startswith('bert_')]\n",
    "for groupby_columns in combinations(columns_list, 2):\n",
    "    groupby_columns = list(groupby_columns)\n",
    "    df = nu.get_minority_combinations(domain_doc_ners_df, groupby_columns).dropna(axis='columns', how='all')\n",
    "    if df.shape[0]:\n",
    "        print(nu.conjunctify_nouns(groupby_columns))\n",
    "        df.file_path = df.file_path.map(lambda x: str(x).split('/')[-1])\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34727f60-9420-4e75-bb7c-776519f2fd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "nu.delete_ipynb_checkpoint_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5372f221-b963-413c-9a5a-bf09ec3acd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists for domain_doc_ners_df_old - attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df_old.csv.\n",
      "['bert_word', 'bert_entity', 'bert_score', 'bert_start', 'bert_end', 'file_path', 'nlp_word', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_phrase', 'ent_type', 'ent_start', 'ent_end', 'is_probe', 'is_probe_probability']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the NER entities from a CSV\n",
    "if nu.csv_exists('domain_doc_ners_df_old'):\n",
    "    domain_doc_ners_old_df = nu.load_data_frames(domain_doc_ners_df_old='domain_doc_ners_df_old')['domain_doc_ners_df_old']\n",
    "    print(domain_doc_ners_old_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3083085-8b20-4a66-8b2a-b2e8ba4a256f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01', '07002', '2018', '2nd', 'a', 'adjuncts', 'airway', 'airways', 'amedd', 'and', 'application', 'applied', 'area', 'army', 'assess', 'assessment', 'assets', 'background', 'based', 'battlefield', 'care', 'casualty', 'celox', 'chest', 'chin', 'chitogauze', 'chitosan', 'circulation', 'collection', 'com', 'combat', 'compression', 'concept', 'continue', 'control', 'controlled', 'convert', 'cpgs', 'current', 'decompress', 'decompressing', 'decompression', 'devices', 'docs', 'dressings', 'drugs', 'emergency', 'establishing', 'evacuation', 'external', 'extraction', 'field', 'for', 'forces', 'fulltext', 'gauze', 'guidelines', 'guiding', 'hemorrhage', 'hemostatic', 'https', 'immediate', 'impending', 'impregnated', 'in', 'individual', 'insert', 'interventions', 'intranasal', 'intranasally', 'jaw', 'journals', 'jtrauma', 'jts', 'junctional', 'kaolin', 'kit', 'learned', 'lessons', 'life', 'lifesaver', 'lift', 'likely', 'limb', 'lww', 'maintaining', 'management', 'maneuver', 'massive', 'medical', 'mil', 'military', 'models', 'morgue', 'nasal', 'nasopharyngeal', 'needle', 'needles', 'needless', 'needlessly', 'obstruction', 'of', 'operations', 'performe', 'performing', 'pernasal', 'pneumatic', 'point', 'position', 'prehospital', 'previous', 'principles', 'prioritization', 'procedures', 'prolonged', 'protocol', 'quickclot', 'recovery', 'resources', 'respirations', 'sam', 'saving', 'single', 'sof', 'special', 'successful', 'tactical', 'tccc', 'the', 'thrust', 'time', 'tool', 'tourniquet', 'tourniquets', 'transportation', 'trauma', 'treatment', 'triage', 'unconscious', 'vehicle', 'without', 'wounds', 'yes']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "regexed_columns_list = ['bert_word', 'nlp_word', 'ent_phrase']\n",
    "df = domain_doc_ners_old_df[domain_doc_ners_old_df.is_probe][regexed_columns_list]\n",
    "sorted(set(\n",
    "    [word_str for phrase_str in nu.modalize_columns(df, regexed_columns_list, 'phrase_str').phrase_str.unique() for word_str in re.split(\n",
    "        r'[\\s&./_•−◻®,†:‡\\-]+', phrase_str.lower(), 0\n",
    "    ) if word_str]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fbcb795a-c957-4318-b858-4e48e8f39aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "canonical_phrases = [\n",
    "    'airway', 'celox', 'chest', 'chin', 'chitogauze', 'chitosan', 'compression', 'cpgs', 'decompress', 'dressing', 'gauze', 'hemorrhage',\n",
    "    'hemostatic',\n",
    "    'intranasal', 'jaw', 'junctional', 'kaolin', 'lifesaver', 'limb', 'nasopharyngeal', 'needle', 'pernasal', 'pneumatic', 'prehospital',\n",
    "    'quickclot',\n",
    "    'respirations', 'tourniquet', 'trauma', 'triage'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80920a7f-bd02-47d9-a93f-25eeb0b666ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build a data frame of just the phrases and words that were picked up and their various measures\n",
    "rows_list = []\n",
    "indices_list = []\n",
    "cns_list = []\n",
    "word_strs_list = []\n",
    "word_columns_list = [\n",
    "    'bert_word', 'nlp_word', 'ent_phrase',\n",
    "    'bert_entity', 'nlp_tag', 'nlp_type', 'nlp_pofs', 'ent_type'\n",
    "]\n",
    "if 'is_probe' not in domain_doc_ners_df.columns: domain_doc_ners_df['is_probe'] = False\n",
    "for cn in set(regexed_columns_list):\n",
    "    for word_str in set(canonical_phrases):\n",
    "        mask_series = domain_doc_ners_df[cn].map(lambda x: word_str in str(x).lower())\n",
    "        df = domain_doc_ners_df[mask_series][word_columns_list].dropna(axis='columns', how='all')\n",
    "        if df.shape[0]: domain_doc_ners_df.loc[mask_series, 'is_probe'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5091643-627d-4695-b200-f1d2cb9e60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', warm_start=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True, ngram_range=(1, 3)\n",
    ")\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    norm='l1', smooth_idf=True, sublinear_tf=False, use_idf=True\n",
    ")\n",
    "classifier = SGDClassifier(loss='log_loss', warm_start=True)\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "columns_list = ['ent_phrase', 'is_probe']\n",
    "df = domain_doc_ners_df[~mask_series][columns_list]\n",
    "df.is_probe = df.is_probe.map(\n",
    "    lambda x: {True: 1, False: 0}.get(x, x)\n",
    ")\n",
    "train_data_list = df.ent_phrase.tolist()\n",
    "train_labels_list = df.is_probe.values\n",
    "X_train_counts = vectorizer.fit_transform(train_data_list)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train on initial data\n",
    "classifier.fit(X_train_tfidf, train_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fecdf478-3b73-4884-96ce-715401e630c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2006'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.016255309990146363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "sample_ent_phrase = domain_doc_ners_df[~mask_series].sample(1).ent_phrase.squeeze()\n",
    "display(sample_ent_phrase)\n",
    "X_test = tfidf_transformer.transform(vectorizer.transform([sample_ent_phrase])).toarray()\n",
    "display(classifier.predict_proba(X_test)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d01c87d2-3440-4d6f-b59a-77f806124361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'is_probe_probability' not in domain_doc_ners_df.columns: domain_doc_ners_df['is_probe_probability'] = 0.0\n",
    "mask_series = domain_doc_ners_df.ent_phrase.isnull()\n",
    "domain_doc_ners_df.loc[~mask_series, 'is_probe_probability'] = domain_doc_ners_df[~mask_series].ent_phrase.map(\n",
    "    lambda x: classifier.predict_proba(tfidf_transformer.transform(vectorizer.transform([x])).toarray())[0][1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8733ef4c-788d-4e5f-abfa-cbe942d60599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nu.save_data_frames(domain_doc_ners_df=domain_doc_ners_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eede81-6a3b-4c26-82a8-99b140d3bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db080f3e-4c9e-4ee4-8fac-9916336d6b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/domain_doc_ners_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nu.save_data_frames(domain_doc_ners_df=concat([domain_doc_ners_df, domain_doc_ners_old_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b2761-8e4f-4afb-af98-1fa0623bf81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex (Python 3.10.13)",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adee83-86d4-442f-b1e5-4f9748fe489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the notebook\n",
    "%pprint\n",
    "import sys\n",
    "if (osp.join(os.pardir, 'py') not in sys.path): sys.path.insert(1, osp.join(os.pardir, 'py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945cd35-7f46-41db-a1c8-5577d4598ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FRVRS import (osp, np, nu, to_datetime, listdir, read_csv, csv, DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ae0a53-e147-4efe-87ea-907fd518701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11)\n",
      "/mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/data/logs/v.1.3/d9d58da9-9bdc-41ea-90fe-4c84db4635d9.csv\n",
      "Doug,\n",
      "\n",
      "There is a file called v.1.3/d9d58da9-9bdc-41ea-90fe-4c84db4635d9.csv that was dated 2023-09-11 that only has Bob_1 Root, Bob_10 Root, Bob_12 Root, Bob_7 Root, Gary_16 Root, Gary_2 Root, Helga_14 Root, Helga_17 Root, Helga_6 Root, Helga_9 Root, Lily_0 Root, Lily_11 Root, Lily_15 Root, Lily_5 Root, Mike_13 Root, Mike_3 Root, Mike_4 Root, and Mike_8 Root in it - no military. Was that session also in the jungle environment?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find out if any files dated the same are actually jungle\n",
    "from datetime import date\n",
    "\n",
    "mask_series = file_stats_df.session_file_date.isin([date(2023, 9, 7), date(2023, 9, 11)]) & (file_stats_df.scenario_environment != 'jungle')\n",
    "df = file_stats_df[mask_series]\n",
    "session_uuid = df.session_uuid.squeeze()\n",
    "print(df.shape) # (1, 11)\n",
    "file_name = df.file_name.squeeze()\n",
    "print(osp.abspath(osp.join('../data/logs', file_name)))\n",
    "mask_series = (frvrs_logs_df.session_uuid == session_uuid) & ~frvrs_logs_df.patient_id.isnull()\n",
    "patients_list = sorted(frvrs_logs_df[mask_series].patient_id.unique())\n",
    "session_file_date = df.session_file_date.squeeze()\n",
    "print(\n",
    "    f'Doug,\\n\\nThere is a file called {file_name} that was dated {session_file_date.date()}'\n",
    "    f' that only has {nu.conjunctify_nouns(patients_list)} in it - no military.'\n",
    "    ' Was that session also in the jungle environment?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd4588a4-6665-4341-98ef-0fa11611d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave and Doug,\n",
      "\n",
      "We need you to fill the missing (as of 2/15) data in the responder_type (Paramedic, EMT-I, etc.), site_name (Union County, Plain Twp, etc.), and encounter_layout (First 11, DCEMS 11, etc.) columns of the attached spreadsheet.\n",
      "\n",
      "There are 159 rows of missing information:\n",
      "53 out of 53 from 03/15 to 03/16/2022\n",
      "23 out of 23 from 11/30 to 12/02/2022\n",
      "7 out of 7 from 12/06\n",
      "10 out of 26 from 03/07 to 03/08/2023\n",
      "12 out of 47 from 03/14 to 03/15/2023\n",
      "3 out of 51 from 04/19 to 04/20/2023\n",
      "11 out of 11 from 04/25\n",
      "2 out of 2 from 05/01\n",
      "7 out of 35 from 05/09 to 05/12/2023\n",
      "8 out of 8 from 05/16\n",
      "2 out of 2 from 05/22 to 05/23/2023\n",
      "2 out of 11 from 06/08\n",
      "2 out of 2 from 06/12\n",
      "5 out of 11 from 06/23\n",
      "2 out of 25 from 07/27\n",
      "1 out of 26 from 08/01\n",
      "4 out of 26 from 08/09 to 08/10/2023\n",
      "4 out of 5 from 08/16\n",
      "1 out of 6 from 09/11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a missing demographics Excel file, delete the jungle entries and the responder name column\n",
    "# and then send it to OSU with BigBear.ai and CACI CCâ€™d\n",
    "missing_columns_list = ['responder_type', 'site_name', 'encounter_layout']\n",
    "mask_series = False\n",
    "for cn in missing_columns_list: mask_series |= file_stats_df[cn].isnull()\n",
    "mask_series &= (file_stats_df.scenario_environment != 'jungle')\n",
    "columns_list = ['is_scene_aborted']\n",
    "for cn in file_stats_df.columns:\n",
    "    if (file_stats_df[mask_series][cn].nunique() > 1) and (cn not in missing_columns_list): columns_list.append(cn)\n",
    "df = file_stats_df[mask_series][columns_list+missing_columns_list].drop(columns=['responder_name'])\n",
    "df.is_scene_aborted = ''\n",
    "df.to_excel('../saves/xlsx/missing_demographics.xlsx', index=False)\n",
    "stamps_list = [ts.timestamp() for ts in df.sort_values('session_file_date').session_file_date]\n",
    "\n",
    "# It would also be good to include in the text of your email the row sum total and grouped by date range \"out of\" total by date:\n",
    "missing_columns_str = []\n",
    "for cn in missing_columns_list:\n",
    "    mask_series = ~file_stats_df[cn].isnull() & (file_stats_df[cn] != 'Unknown')\n",
    "    values_list = np.random.choice(file_stats_df[mask_series][cn].unique(), size=2, replace=False).tolist()\n",
    "    missing_columns_str.append(f'{cn} (' + ', '.join(values_list) + ', etc.)')\n",
    "missing_columns_str = nu.conjunctify_nouns(missing_columns_str)\n",
    "print(\n",
    "    f'Guys,\\n\\nWe need you to fill the missing (as of 2/15) data in the {missing_columns_str}'\n",
    "    f' columns of the attached spreadsheet.\\n\\nThere are {df.shape[0]} rows of missing information:'\n",
    ")\n",
    "\n",
    "# 53 from 3/15-3/16/2022\n",
    "# 30 from 11/30-12/6/2022\n",
    "# 23 from 3/7-3/15/2023\n",
    "# 14 from 4/19-4/25/2023\n",
    "# 19 from 5/1-5/23/2023\n",
    "# 9 from 6/1-6/23/2023\n",
    "# 2 from 7/27/2023\n",
    "# 9 from 8/1-8/16/2023\n",
    "file_count = 0\n",
    "full_splits_list = nu.split_list_by_gap(\n",
    "    [ts.timestamp() for ts in file_stats_df.sort_values('session_file_date').session_file_date], value_difference=24*60*60\n",
    ")\n",
    "for grouping in nu.split_list_by_gap(stamps_list, value_difference=24*60*60):\n",
    "    from_date_obj = to_datetime(grouping[0], unit='s')\n",
    "    from_date = from_date_obj.strftime('%m/%d')\n",
    "    to_date_obj = to_datetime(grouping[-1], unit='s')\n",
    "    to_date = to_date_obj.strftime('%m/%d/%Y')\n",
    "    full_group = []\n",
    "    for subgroup in full_splits_list:\n",
    "        if grouping[0] in subgroup:\n",
    "            full_group = subgroup\n",
    "            break\n",
    "    y = 'y'\n",
    "    if full_group:\n",
    "        y = len(full_group)\n",
    "        file_count += y\n",
    "    date_range = f'{from_date} to {to_date}'\n",
    "    if (from_date_obj == to_date_obj): date_range = from_date\n",
    "    print(f'{len(grouping)} out of {y} from {date_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83bb18f3-94de-4307-9ad3-1972fe3a7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 385 files in the \"CSV Files Renamed by Date.zip\" file that Doug uploaded to Data and Videos (on Sep 23, 2023) contain 385 unique UUIDs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The 159 rows with missing data for: responder_type; site_name; encounter_layout\n",
    "# Are those 159 rows part of the 389 CSV files Doug provided to you?\n",
    "# Or do you have 389 and you are looking for an additional 159?\n",
    "# I noticed that your lists of \"x out of y\" for date clusters; the ys add up to 377\n",
    "# Is it possible that 389 - z = 377?\n",
    "# where z = the jungle scenarios that shouldn't be in there\n",
    "# Does that fully define z?\n",
    "# I believe this was a miscommunication in the 11:30 meeting and I will like to make sure I understand the status.\n",
    "import os\n",
    "\n",
    "sub_directory = '../data/temp'\n",
    "file_names_list = listdir(sub_directory)\n",
    "print(\n",
    "    f'The {len(file_names_list)} files in the \"CSV Files Renamed by Date.zip\" file that Doug uploaded to Data and'\n",
    "    ' Videos (on Sep 23, 2023) contain ', end=''\n",
    ")\n",
    "session_uuids_list = []\n",
    "for file_name in file_names_list:\n",
    "    \n",
    "    # Construct the full path to the file\n",
    "    file_path = osp.join(sub_directory, file_name)\n",
    "    \n",
    "    # Attempt to read CSV file using pandas\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        version_number = '1.0'\n",
    "        file_df = read_csv(file_path, header=None, index_col=False)\n",
    "    \n",
    "    # If unsuccessful, try using a reader\n",
    "    except:\n",
    "        version_number = '1.3'\n",
    "        rows_list = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "            for values_list in reader:\n",
    "                if (values_list[-1] == ''): values_list.pop(-1)\n",
    "                rows_list.append({i: v for i, v in enumerate(values_list)})\n",
    "        file_df = DataFrame(rows_list)\n",
    "    \n",
    "    for session_uuid in file_df[3].unique():\n",
    "        session_uuids_list.append(session_uuid)\n",
    "print(f'{len(set(session_uuids_list))} unique UUIDs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bfe9cf9e-46f6-4b30-b9cc-8f6cc778c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave,\n",
      "\n",
      "Of the 385 files in the \"CSV Files Renamed by Date.zip\" file that Doug uploaded to Data and Videos (on Sep 23, 2023) attached are the ones that have the missing data in the responder_type (EMT-Basic, EM-RES1, etc.), site_name (DCEMS-RND 1, Madison Twp, etc.), and encounter_layout (First 11, DCEMS 11, etc.) columns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a missing demographics Excel file for the is_in_registry data\n",
    "missing_columns_list = ['responder_type', 'site_name', 'encounter_layout']\n",
    "mask_series = False\n",
    "for cn in missing_columns_list: mask_series |= file_stats_df[cn].isnull()\n",
    "mask_series &= file_stats_df.session_uuid.isin(session_uuids_list)\n",
    "columns_list = []\n",
    "for cn in file_stats_df.columns:\n",
    "    if (file_stats_df[mask_series][cn].nunique() > 1) and (cn not in missing_columns_list): columns_list.append(cn)\n",
    "columns_list += ['is_scene_aborted']\n",
    "df = file_stats_df[mask_series][columns_list+missing_columns_list]\n",
    "df.is_scene_aborted = ''\n",
    "df.to_excel('../saves/xlsx/missing_demographics.xlsx', index=False)\n",
    "stamps_list = [ts.timestamp() for ts in df.sort_values('session_file_date').session_file_date]\n",
    "\n",
    "# It would also be good to include in the text of your email the row sum total and grouped by date range \"out of\" total by date:\n",
    "missing_columns_str = []\n",
    "for cn in missing_columns_list:\n",
    "    mask_series = ~file_stats_df[cn].isnull() & (file_stats_df[cn] != 'Unknown')\n",
    "    values_list = np.random.choice(file_stats_df[mask_series][cn].unique(), size=2, replace=False).tolist()\n",
    "    missing_columns_str.append(f'{cn} (' + ', '.join(values_list) + ', etc.)')\n",
    "missing_columns_str = nu.conjunctify_nouns(missing_columns_str)\n",
    "print(\n",
    "    f'Dave,\\n\\nOf the {len(file_names_list)} files in the \"CSV Files Renamed by Date.zip\" file that Doug uploaded to Data and'\n",
    "    ' Videos (on Sep 23, 2023)'\n",
    "    f' attached are the ones that have the missing data in the {missing_columns_str} columns.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa7a530a-964a-4d45-9cc0-52e0daff8b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 of those 385 unique UUIDs are part of the 159 rows with missing data for responder_type, site_name, and encounter_layout.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The 159 rows with missing data for: responder_type; site_name; encounter_layout\n",
    "# Are those 159 rows part of the 389 CSV files Doug provided to you?\n",
    "missing_columns_list = ['responder_type', 'site_name', 'encounter_layout']\n",
    "mask_series = False\n",
    "for cn in missing_columns_list: mask_series |= file_stats_df[cn].isnull()\n",
    "mask_series &= file_stats_df.session_uuid.isin(session_uuids_list)\n",
    "columns_list = ['responder_type', 'site_name', 'encounter_layout']\n",
    "df = file_stats_df[mask_series]\n",
    "print(\n",
    "    f'{df.shape[0]} of those {len(set(session_uuids_list))} unique UUIDs are part of the 159 rows with missing data for'\n",
    "    ' responder_type, site_name, and encounter_layout.'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a137994-b9ea-49dc-b1d0-5e27f8057322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925d4f87-da88-4f95-99e1-e314bcc00713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "from FRVRS import fu, nu\n",
    "from datetime import date, timedelta\n",
    "from pandas import DataFrame, to_datetime, Series, concat\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from IPython.display import HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d2930-a28d-42dc-a6e0-ec6b7b0c6959",
   "metadata": {},
   "source": [
    "\n",
    "# Find the Whereabouts of the 389 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8af8db5f-5d7b-48be-bfc3-8917158b50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_name column is in the file_stats_df DataFrame.\n",
      "The scene_type column is in the scene_stats_df DataFrame.\n",
      "The is_scene_aborted column is in the file_stats_df DataFrame.\n",
      "The is_scene_aborted column is in the scene_stats_df DataFrame.\n",
      "The logger_version column is in the file_stats_df DataFrame.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_frames_list = nu.load_data_frames(\n",
    "    frvrs_logs_df='', file_stats_df='', scene_stats_df='',\n",
    "    verbose=False\n",
    ")\n",
    "ldf = data_frames_list['frvrs_logs_df']\n",
    "fdf = data_frames_list['file_stats_df']\n",
    "sdf = data_frames_list['scene_stats_df']\n",
    "\n",
    "# Get the column sets\n",
    "triage_columns = ['file_name', 'scene_type', 'is_scene_aborted', 'logger_version']\n",
    "for cn in triage_columns:\n",
    "    for (df_name, df) in [('frvrs_logs_df', ldf), ('file_stats_df', fdf), ('scene_stats_df', sdf)]:\n",
    "        if cn in df.columns: print(f'The {cn} column is in the {df_name} DataFrame.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd534c8b-c4c4-4a72-ac84-adf6bba9eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the redundant file stats columns\n",
    "file_columns_set = set(fdf.columns)\n",
    "drop_columns = list(set(['is_scene_aborted']).intersection(file_columns_set))\n",
    "if drop_columns: fdf = fdf.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5886085-d2bc-4a7b-8fa5-f4b1fab01b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257704 357 122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge the latest FRVRS logs to get the original columns\n",
    "patient_count_filter_fn = lambda scene_df: True\n",
    "merge_df = fu.get_elevens_data_frame(\n",
    "    ldf, fdf, sdf,\n",
    "    needed_columns=triage_columns,\n",
    "    patient_count_filter_fn=patient_count_filter_fn\n",
    ")\n",
    "mask_series = ~merge_df.file_name.isnull()\n",
    "one_triage_file_filter_fn = lambda file_name_df: fu.get_is_a_one_triage_file(file_name_df)\n",
    "df = merge_df[mask_series].groupby('file_name').filter(one_triage_file_filter_fn)\n",
    "print(df.shape[0], df.session_uuid.nunique(), df.shape[1])\n",
    "frvrs_session_uuids_list = sorted(df.session_uuid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7645a-48c6-4436-8778-5fc00eb4f31d",
   "metadata": {},
   "source": [
    "\n",
    "## Trying summing up CSV folder-counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3b035e6-3f96-49e3-affa-2e6a36de017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There exists a combination of subfolders that sums to 389.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logs_folder = '../data/logs'\n",
    "\n",
    "# Iterate through subfolders using os.walk\n",
    "target_sum = 389\n",
    "subfolder_counts = []\n",
    "for root, _, files in os.walk(logs_folder):\n",
    "    \n",
    "    # Count CSV files\n",
    "    csv_count = sum(1 for f in files if f.endswith(\".csv\"))\n",
    "    \n",
    "    # Display subfolder name and CSV count (if any)\n",
    "    if csv_count > 0:\n",
    "        # print(f\"Subfolder: {root.replace(logs_folder, '')}, CSV count: {csv_count}\")\n",
    "        subfolder_tuple = (root.replace(logs_folder, ''), csv_count)\n",
    "        subfolder_counts.append(subfolder_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a564f83d-1499-4824-b8d7-1f3408595df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No combination of subfolders sums to 389.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subfolder_counts =  [\n",
    "    ('/All CSV files renamed by date', 144), ('/DCEMS Round 2 only triage sessions', 81), ('/Disaster Day 2022', 29), ('/Double runs removed', 28),\n",
    "    ('/v.1.0', 128), ('/v.1.3', 17)\n",
    "]\n",
    "\n",
    "# Function to check if a combination of subfolders sums to the target\n",
    "def check_combination(counts, target, used=[]):\n",
    "    if target == 0:\n",
    "        print(used)\n",
    "        return True\n",
    "    if target < 0 or not counts: return False\n",
    "    for i, (subfolder_name, count) in enumerate(counts):\n",
    "        if count not in used:\n",
    "            if check_combination(counts[:i] + counts[i+1:], target - count, used + [count]): return True\n",
    "    return False\n",
    "\n",
    "# Check if a combination of subfolders sums to the target\n",
    "if check_combination(subfolder_counts, target_sum): print(f\"There exists a combination of subfolders that sums to {target_sum}.\")\n",
    "else: print(f\"No combination of subfolders sums to {target_sum}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530c728-1ceb-4c05-8a45-9718d567cf31",
   "metadata": {},
   "source": [
    "\n",
    "## Get a dataset of all file names and session UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b558176-a91f-4ed0-88e7-ece3e48887f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867 518 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Walk through the logs, getting only file names and session_uuids\n",
    "important_columns_list = ['file_name', 'session_uuid']\n",
    "logs_df = DataFrame([], columns=important_columns_list)\n",
    "\n",
    "# Iterate over the subdirectories, directories, and files in the logs folder\n",
    "logs_folder = '../data/logs'\n",
    "for sub_directory, directories_list, files_list in os.walk(logs_folder):\n",
    "    \n",
    "    # Create a data frame to store the data for the current subdirectory\n",
    "    sub_directory_df = DataFrame([], columns=important_columns_list)\n",
    "    \n",
    "    # Iterate over the files in the current subdirectory\n",
    "    for file_name in files_list:\n",
    "        \n",
    "        # If the file is a CSV file, merge it into the subdirectory data frame\n",
    "        if file_name.endswith('.csv'):\n",
    "            # sub_directory_df = fu.process_files(sub_directory_df, sub_directory, file_name, verbose=verbose)\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = osp.join(sub_directory, file_name)\n",
    "            \n",
    "            # Attempt to read CSV file using pandas\n",
    "            try: file_df = pd.read_csv(file_path, header=None, index_col=False)\n",
    "            \n",
    "            # If unsuccessful, try using a reader\n",
    "            except:\n",
    "                rows_list = []\n",
    "                with open(file_path, 'r') as f:\n",
    "                    import csv\n",
    "                    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "                    for values_list in reader:\n",
    "                        if (values_list[-1] == ''): values_list.pop(-1)\n",
    "                        rows_list.append({i: v for i, v in enumerate(values_list)})\n",
    "                file_df = DataFrame(rows_list)\n",
    "            \n",
    "            # Ignore small files and return the subdirectory data frame unharmed\n",
    "            if (file_df.shape[1] >= 16):\n",
    "                \n",
    "                # Add file name  to the data frame\n",
    "                file_dir_suffix = osp.abspath(sub_directory).replace(osp.abspath(logs_folder) + os.sep, '')\n",
    "                file_df['file_name'] = '/'.join(file_dir_suffix.split(os.sep)) + '/' + file_name\n",
    "                \n",
    "                # Name the global columns\n",
    "                columns_list = ['action_type', 'action_tick', 'event_time', 'session_uuid']\n",
    "                file_df.columns = columns_list + file_df.columns.tolist()[len(columns_list):]\n",
    "\n",
    "                # Remove all but the file name and session columns\n",
    "                file_df = file_df[important_columns_list].drop_duplicates()\n",
    "                \n",
    "                # Append the data frame for the current file to the data frame for the current subdirectory\n",
    "                sub_directory_df = concat([sub_directory_df, file_df], axis='index')\n",
    "    \n",
    "    # Append the data frame for the current subdirectory to the main data frame\n",
    "    logs_df = pd.concat([logs_df, sub_directory_df], axis='index')\n",
    "    \n",
    "logs_df = logs_df.reset_index()\n",
    "print(logs_df.shape[0], logs_df.session_uuid.nunique(), logs_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c738fe3d-201b-4f91-b99c-a5ffef039371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401177 332 111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load triage paper data frame and get the session UUIDs\n",
    "data_frames_list = nu.load_data_frames(\n",
    "    first_responder_master_registry_df='',\n",
    "    verbose=False\n",
    ")\n",
    "triage_paper_df = data_frames_list['first_responder_master_registry_df']\n",
    "print(triage_paper_df.shape[0], triage_paper_df.session_uuid.nunique(), triage_paper_df.shape[1])\n",
    "mask_series = ~triage_paper_df.session_uuid.isnull()\n",
    "registry_session_uuids_list = sorted(triage_paper_df[mask_series].session_uuid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92a581d9-7bf9-4c2a-ad66-45907e36d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the session UUIDs not in the registry or subsequent file additions\n",
    "logs_df['sub_folder'] = logs_df.file_name.map(lambda x: x.split('/')[0])\n",
    "mask_series = ~logs_df.session_uuid.isin(registry_session_uuids_list) & ~logs_df.sub_folder.isin([\n",
    "    'Disaster Day 3.6.2024 ITM Files 405E', 'Disaster day 3.6.2024 ITM files 405F', 'Metrics Evaluation Open World'\n",
    "]) & ~logs_df.session_uuid.isnull()\n",
    "df = logs_df[mask_series]\n",
    "extra_session_uuids_list = sorted(df.session_uuid.unique())\n",
    "extra_session_uuids_count = len(extra_session_uuids_list)\n",
    "extra_session_uuids_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "366d0e7d-01e2-420e-82b3-bd14adc360a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Do the extra sessions add up?\n",
    "len(registry_session_uuids_list) + extra_session_uuids_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "852fcc17-4043-4c92-b490-ab510afa1ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# How many session UUIDs are missing from the registry?\n",
    "389 - len(registry_session_uuids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0ad8a-8266-42e8-9070-7cc5c677bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = merge_df.session_uuid.isin(extra_session_uuids_list)\n",
    "frvrs_logs_df = merge_df[mask_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44a4e1-e696-469b-86b1-1e7130b41fa1",
   "metadata": {},
   "source": [
    "\n",
    "# Identify any Anomalous Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4a11181-7117-4f1f-8709-67bfa19a5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anomalous_files_set = set()\n",
    "anomalous_files_str = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb39e28a-d48f-4c55-83e8-4936980b8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If all the patients in a file are all named Mike, that is a training simulation\n",
    "files_list = []\n",
    "\n",
    "# Group the DataFrame by 'file_name'\n",
    "for file_name, file_name_df in frvrs_logs_df.groupby('file_name'):\n",
    "\n",
    "    # Get a list of the unique patient IDs in the file\n",
    "    patient_ids = file_name_df.patient_id.unique().tolist()\n",
    "\n",
    "    # Check if all the patient IDs in the file contain the string \"mike\" in lowercase\n",
    "    if all(map(lambda x: 'mike' in str(x).lower(), patient_ids)):\n",
    "\n",
    "        # Add the file name to the files_list list\n",
    "        files_list.append(file_name)\n",
    "\n",
    "# If there are files with all patients named \"Mike,\" print and update the results\n",
    "if files_list:\n",
    "\n",
    "    # Create a string with the list of files having \"Mike\" patients\n",
    "    print_str = f'\\n\\nThese files have patients that are all named \"Mike\":\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "\n",
    "    # Add carriage returns for better readability\n",
    "    print_str = print_str.replace(' and ', ', and\\n\\t')\n",
    "    \n",
    "    # Print the results\n",
    "    print(print_str)\n",
    "\n",
    "    # Update the anomalous files string\n",
    "    anomalous_files_str += print_str\n",
    "\n",
    "    # Update the anomalous files set\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93a5e86d-eaaa-41ea-9b2a-e590517025dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These files have no user action taken:\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store rows\n",
    "rows_list = []\n",
    "\n",
    "# Loop through sessions and scenes\n",
    "for (session_uuid, scene_id), scene_df in frvrs_logs_df.groupby(fu.scene_groupby_columns):\n",
    "\n",
    "    # Create a dictionary to store the results for the current group\n",
    "    row_dict = {}\n",
    "\n",
    "    # Get the logger version for the current group\n",
    "    logger_version = fu.get_logger_version(scene_df)\n",
    "\n",
    "    # Add the logger version, session UUID, and scene to the dictionary\n",
    "    for cn in fu.scene_groupby_columns: row_dict[cn] = eval(cn)\n",
    "    row_dict['logger_version'] = logger_version\n",
    "\n",
    "    # Count the number of user actions for the current group\n",
    "    row_dict['total_actions'] = fu.get_total_actions(scene_df)\n",
    "\n",
    "    # Append the row to the list\n",
    "    rows_list.append(row_dict)\n",
    "\n",
    "# Create a data frame from the list of rows\n",
    "total_actions_df = DataFrame(rows_list)\n",
    "\n",
    "# Group by session UUID and sum total actions\n",
    "df = total_actions_df.groupby('session_uuid').sum()\n",
    "\n",
    "# Create a mask for sessions with zero total actions\n",
    "mask_series = (df.total_actions == 0)\n",
    "\n",
    "# Get a list of session UUIDs with no user actions\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Create a mask to filter for UUIDs with no user actions\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "\n",
    "# Get unique file names associated with sessions with zero total actions\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Check if there are files with zero actions\n",
    "if files_list:\n",
    "    \n",
    "    # Create a formatted string of anomalous files\n",
    "    print_str = f'\\n\\nThese files have no user action taken:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    \n",
    "    # Add carriage returns for readability\n",
    "    print_str = print_str.replace(' and ', ', and\\n\\t')\n",
    "\n",
    "    # Print and update the anomalous files string and set\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7975bdab-3c39-4f12-8af2-08e721466d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These files have no teleportation being done:\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store dictionaries for each session for Total Number of Teleports\n",
    "rows_list = []\n",
    "\n",
    "# Iterate through the scenes\n",
    "for (session_uuid, scene_id), scene_df in frvrs_logs_df.groupby(fu.scene_groupby_columns):\n",
    "    \n",
    "    # Create a dictionary to store information for each scene\n",
    "    row_dict = {}\n",
    "    \n",
    "    # Get the logger version for the current session\n",
    "    logger_version = fu.get_logger_version(scene_df)\n",
    "\n",
    "    # Add the logger version, session_uuid, and scene_id to the row dictionary\n",
    "    for cn in fu.scene_groupby_columns: row_dict[cn] = eval(cn)\n",
    "    row_dict['logger_version'] = logger_version\n",
    "\n",
    "    # Get the number of teleports for the current scene and add it to the row dictionary\n",
    "    row_dict['teleport_count'] = fu.get_teleport_count(scene_df)\n",
    "    \n",
    "    # Append the dictionary to the list\n",
    "    rows_list.append(row_dict)\n",
    "\n",
    "# Create a data frame from the list of dictionaries\n",
    "teleport_count_df = DataFrame(rows_list)\n",
    "\n",
    "# Group the data frame by UUID and sum the teleport counts\n",
    "df = teleport_count_df.groupby('session_uuid').sum()\n",
    "\n",
    "# Create a mask to filter sessions with teleport count equal to 0\n",
    "mask_series = (df.teleport_count == 0)\n",
    "\n",
    "# Get a list of session UUIDs with no teleportation\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Create a mask to filter files associated with the identified session UUIDs\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "\n",
    "# Get the file names with no teleportation being done\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Check if there are files with no teleportation\n",
    "if files_list:\n",
    "\n",
    "    # Create a printable string of the file names\n",
    "    print_str = f'\\n\\nThese files have no teleportation being done:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    \n",
    "    # Format the printed string for readability\n",
    "    print_str = print_str.replace(', ', ',\\n\\t').replace(',\\n\\tand ', ', and\\n\\t')\n",
    "    \n",
    "    # Print the list of files and update the anomalous_files_str and set\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0a56567-3ebf-42dc-a30c-ce4ca3a3b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These files have no injury treatment being done:\n",
      "\tAll CSV files renamed by date/03.14.23.0919.csv, All CSV files renamed by date/11.30.20.0828.csv, DCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv, DCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,, and\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv\n",
      "\n",
      "\n",
      "These files have no pulses being taken:\n",
      "\tAll CSV files renamed by date/03.14.23.1219.csv,\n",
      "\tAll CSV files renamed by date/03.15.23.0944.csv,\n",
      "\tv.1.0/Clean 5fa79a8e-a2df-4bb9-b614-f3ce36a5edb0.csv, and\n",
      "\tv.1.0/Clean a6b7ff70-3b20-48c6-86e8-744bad19f7d7.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a mask to filter rows where action_type is PULSE_TAKEN or INJURY_TREATED\n",
    "mask_series = frvrs_logs_df.action_type.isin(['PULSE_TAKEN', 'INJURY_TREATED'])\n",
    "\n",
    "# Define columns to group by and initialize lists\n",
    "\n",
    "\n",
    "rows_list = []\n",
    "indices_list = []\n",
    "\n",
    "# Iterate over groups based on the specified columns\n",
    "for (session_uuid, scene_id, patient_id), patient_df in frvrs_logs_df[mask_series].groupby(fu.patient_groupby_columns):\n",
    "\n",
    "    # Create a tuple to store the index of the current row\n",
    "    index_tuple = (session_uuid, scene_id, patient_id)\n",
    "    indices_list.append(index_tuple)\n",
    "\n",
    "    # Create a dictionary to store the results for the current row\n",
    "    row_dict = {}\n",
    "\n",
    "    # Add the logger_version for the current row\n",
    "    row_dict['logger_version'] = fu.get_logger_version(patient_df)\n",
    "    \n",
    "    # Count the number of 'PULSE_TAKEN' actions in this group\n",
    "    row_dict['pulses_count'] = fu.get_pulse_taken_count(patient_df)\n",
    "    \n",
    "    # Count the number of 'INJURY_TREATED' actions in this group\n",
    "    row_dict['treated_count'] = fu.get_injury_treatments_count(patient_df)\n",
    "    \n",
    "    # Try to calculate the number of pulses taken per injury treated\n",
    "    try: row_dict['pulses_by_treated'] = row_dict['pulses_count'] / row_dict['treated_count']\n",
    "\n",
    "    # Handle the case where 'treated_count' is zero to avoid division by zero\n",
    "    except ZeroDivisionError: row_dict['pulses_by_treated'] = np.nan\n",
    "    \n",
    "    # Add the row_dict to the rows_list\n",
    "    rows_list.append(row_dict)\n",
    "\n",
    "# Create a data frame from the rows and set a multi index based on the grouped columns\n",
    "pulses_count_df = DataFrame(rows_list, index=pd.MultiIndex.from_tuples(tuples=indices_list, names=fu.patient_groupby_columns))\n",
    "\n",
    "# Group the pulses count data frame by session_uuid and sum the values\n",
    "df = pulses_count_df.groupby('session_uuid').sum()\n",
    "\n",
    "# Check for sessions with 'treated_count' equal to zero\n",
    "mask_series = (df.treated_count == 0)\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Filter logs for sessions with no 'INJURY_TREATED' actions\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Print files with no injury treatment being done\n",
    "if files_list:\n",
    "    print_str = f'\\n\\nThese files have no injury treatment being done:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    print_str = print_str.replace(' and ', ', and\\n\\t')\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "    anomalous_files_set.update(files_list)\n",
    "\n",
    "# Check for sessions with 'pulses_count' equal to zero\n",
    "mask_series = (df.pulses_count == 0)\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Filter logs for sessions with no 'PULSE_TAKEN' actions\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Print files with no pulses being taken\n",
    "if files_list:\n",
    "    print_str = f'\\n\\nThese files have no pulses being taken:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    print_str = print_str.replace(', ', ',\\n\\t').replace(',\\n\\tand ', ', and\\n\\t')\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f81cfa03-5dcd-4380-9c1a-5c9f6116687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These files have no patient accuracy rate above zero:\n",
      "\tAll CSV files renamed by date/03.14.23.0919.csv,\n",
      "\tAll CSV files renamed by date/11.30.20.0828.csv,\n",
      "\tAll CSV files renamed by date/12.06.22.1331.csv,\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tDCEMS Round 2 only triage sessions/8ec8afba-8533-4915-898f-5769c1258c61.csv,\n",
      "\tDCEMS Round 2 only triage sessions/91f31664-43ad-4405-a763-0d58a8afc36a.csv,\n",
      "\tv.1.0/Clean 158e6365-673b-4030-8b36-6704be5996a2.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean 845d87c5-7b8b-4bf3-bfc6-91c74e285243.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv,\n",
      "\tv.1.0/Clean d90f2d85-e91c-4f12-b070-5929d95be1c5.csv,\n",
      "\tv.1.0/Clean db948ce1-783d-4dff-a1f8-2be49570f327.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Patient accuracy rate (how many patients correct / number of patients treated)\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "rows_list = []\n",
    "\n",
    "# Iterate over the scenes\n",
    "for (session_uuid, scene_id), scene_df in frvrs_logs_df.groupby(fu.scene_groupby_columns):\n",
    "\n",
    "    # Get the logger version for the group\n",
    "    logger_version = fu.get_logger_version(scene_df)\n",
    "    \n",
    "    # Create a dictionary to store the results for the group\n",
    "    row_dict = {}\n",
    "    for cn in fu.scene_groupby_columns: row_dict[cn] = eval(cn)\n",
    "    row_dict['logger_version'] = logger_version\n",
    "\n",
    "    # Get the total number of patients treated\n",
    "    total_treated = fu.get_injury_treatments_count(scene_df)\n",
    "\n",
    "    # If there were any patients treated, calculate the patient accuracy rate\n",
    "    if total_treated:\n",
    "        \n",
    "        # Filter the dataframe to only include rows where the injury was treated correctly\n",
    "        correctly_treated = fu.get_injury_correctly_treated_count(scene_df)\n",
    "\n",
    "        # Calculate the patient accuracy rate\n",
    "        row_dict['injury_treated_total_treated'] = total_treated\n",
    "        row_dict['injury_treated_correctly_treated'] = correctly_treated\n",
    "        row_dict['injury_treated_patient_accuracy_rate'] = correctly_treated / total_treated\n",
    "    \n",
    "    # Filter the data frame to only include rows where the injury record was treated.\n",
    "    total_mask = (scene_df.injury_record_injury_treated == True)\n",
    "    df2 = scene_df[total_mask]\n",
    "\n",
    "    # Get the total number of patients treated\n",
    "    total_treated = df2.shape[0]\n",
    "\n",
    "    # If there were any patients treated, calculate the patient accuracy rate\n",
    "    if total_treated:\n",
    "        \n",
    "        # Filter the dataframe to only include rows where the injury record was treated correctly\n",
    "        correct_mask = (df2.injury_record_injury_treated_with_wrong_treatment == False)\n",
    "        correctly_treated = df2[correct_mask].shape[0]\n",
    "\n",
    "        # Calculate the patient accuracy rate\n",
    "        row_dict['injury_record_total_treated'] = total_treated\n",
    "        row_dict['injury_record_correctly_treated'] = correctly_treated\n",
    "        row_dict['injury_record_patient_accuracy_rate'] = correctly_treated / total_treated\n",
    "\n",
    "    # Add the row dictionary to the list of results\n",
    "    rows_list.append(row_dict)\n",
    "\n",
    "# Create a data frame from the list of dictionaries\n",
    "patient_accuracy_rate_df = DataFrame(rows_list)\n",
    "\n",
    "# Modalize into one patient accuracy rate column if possible\n",
    "if 'injury_record_patient_accuracy_rate' in patient_accuracy_rate_df.columns:\n",
    "\n",
    "    # Get a list of columns that contain the patient accuracy rate\n",
    "    columns_list = [\n",
    "        'injury_treated_patient_accuracy_rate', 'injury_record_patient_accuracy_rate'\n",
    "    ]\n",
    "    \n",
    "    # Check if there's only one unique value across the specified columns\n",
    "    mask_series = (patient_accuracy_rate_df[columns_list].apply(pd.Series.nunique, axis='columns') == 1)\n",
    "    \n",
    "    # Set the patient accuracy rate column for the rows identified by the mask series to the non-null value in one of the patient accuracy rate columns\n",
    "    patient_accuracy_rate_df.loc[~mask_series, 'patient_accuracy_rate'] = np.nan\n",
    "    \n",
    "    # Define a function to select the first valid value\n",
    "    def f(srs):\n",
    "        cn = srs.first_valid_index()\n",
    "        \n",
    "        return srs[cn]\n",
    "    \n",
    "    # Modalize the patient accuracy rate columns to get a single column\n",
    "    patient_accuracy_rate = patient_accuracy_rate_df[mask_series][columns_list].apply(f, axis='columns')\n",
    "    \n",
    "    # Set the patient accuracy rate column for the rows identified by the mask series to the single patient accuracy rate column\n",
    "    patient_accuracy_rate_df.loc[mask_series, 'patient_accuracy_rate'] = patient_accuracy_rate\n",
    "    \n",
    "    # Group by 'session_uuid' and sum the patient accuracy rates\n",
    "    df = patient_accuracy_rate_df.groupby('session_uuid').sum()\n",
    "    mask_series = (df.patient_accuracy_rate == 0)\n",
    "else:\n",
    "    \n",
    "    # Group by 'session_uuid' and sum the patient accuracy rates for 'injury_treated' data\n",
    "    df = patient_accuracy_rate_df.groupby('session_uuid').sum()\n",
    "    mask_series = (df.injury_treated_patient_accuracy_rate == 0)\n",
    "\n",
    "# Get a list of session_uuids with no patient accuracy rate above zero\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Create a mask to filter rows in 'frvrs_logs_df' based on session_uuids\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "\n",
    "# Get a list of unique file names from the filtered rows\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Print the list of files with no patient accuracy rate above zero\n",
    "if files_list:\n",
    "    print_str = f'\\n\\nThese files have no patient accuracy rate above zero:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    print_str = print_str.replace(', ', ',\\n\\t').replace(',\\n\\tand ', ', and\\n\\t')\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7273314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These files have no patients being engaged:\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tv.1.0/Clean 158e6365-673b-4030-8b36-6704be5996a2.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv,\n",
      "\tv.1.0/Clean d90f2d85-e91c-4f12-b070-5929d95be1c5.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of patients engaged\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "rows_list = []\n",
    "\n",
    "# Iterate over the sessions, grouped by scene\n",
    "for (session_uuid, scene_id), scene_df in frvrs_logs_df.groupby(fu.scene_groupby_columns):\n",
    "\n",
    "    # Get the logger version\n",
    "    logger_version = fu.get_logger_version(scene_df)\n",
    "\n",
    "    # Get the number of patients in the session\n",
    "    patients_count = fu.get_patient_count(scene_df)\n",
    "\n",
    "    # Initialize the number of patients engaged\n",
    "    patients_engaged = 0\n",
    "    \n",
    "    # Loop over patients within this scene\n",
    "    for patient_id, patient_df in scene_df.groupby('patient_id'):\n",
    "        \n",
    "        # Create a mask to filter rows where action_type is 'PATIENT_ENGAGED'\n",
    "        mask_series = (patient_df.action_type == 'PATIENT_ENGAGED')\n",
    "        \n",
    "        # If there are any rows that match the mask, increment the number of patients engaged\n",
    "        if mask_series.any(): patients_engaged += 1\n",
    "\n",
    "    # Check if there are patients in this scene\n",
    "    if patients_count:\n",
    "        # Create a dictionary to store row data\n",
    "        row_dict = {}\n",
    "\n",
    "        # Add the session UUID and the scene to the dictionary\n",
    "        for cn in fu.scene_groupby_columns: row_dict[cn] = eval(cn)\n",
    "\n",
    "        # Add the logger version to the dictionary\n",
    "        row_dict['logger_version'] = logger_version\n",
    "\n",
    "        # Add the number of patients to the dictionary\n",
    "        row_dict['patients_count'] = patients_count\n",
    "\n",
    "        # Add the number of patients engaged to the dictionary\n",
    "        row_dict['patients_engaged'] = patients_engaged\n",
    "\n",
    "        # Calculate the percentage of patients engaged\n",
    "        row_dict['percentage_engaged'] = patients_engaged / patients_count\n",
    "\n",
    "        # Add the row dictionary to the list of rows\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "# Create a data frame from the list of rows\n",
    "percentage_engaged_df = DataFrame(rows_list)\n",
    "\n",
    "# Group the DataFrame by UUID and sum the number of patients engaged\n",
    "df = percentage_engaged_df.groupby('session_uuid').sum()\n",
    "\n",
    "# Create a mask to identify UUIDs with no engaged patients\n",
    "mask_series = (df.patients_engaged == 0)\n",
    "\n",
    "# List the UUIDs that meet the condition\n",
    "session_uuids_list = df[mask_series].index.tolist()\n",
    "\n",
    "# Create a mask to filter rows in the main data frame based on UUID\n",
    "mask_series = frvrs_logs_df.session_uuid.isin(session_uuids_list)\n",
    "\n",
    "# Get unique file_names that match the UUIDs\n",
    "files_list = frvrs_logs_df[mask_series].file_name.unique().tolist()\n",
    "\n",
    "# Check if there are any files with no patients engaged\n",
    "if files_list:\n",
    "\n",
    "    # Create a formatted string to list files with no engaged patients\n",
    "    print_str = f'\\n\\nThese files have no patients being engaged:\\n\\t{nu.conjunctify_nouns(files_list)}'\n",
    "    \n",
    "    # Format the string to have line breaks after commas\n",
    "    print_str = print_str.replace(', ', ',\\n\\t').replace(',\\n\\tand ', ', and\\n\\t')\n",
    "\n",
    "    # Print the results and add the results to the anomalous_files_str string\n",
    "    print(print_str); anomalous_files_str += print_str\n",
    "\n",
    "    # Add the files to the anomalous_files_set set\n",
    "    anomalous_files_set.update(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a37c02f6-7848-40e4-8aff-158bda03bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doug,\n",
      "\n",
      "Here is the set of anomalous files I'm concerned about:\n",
      "\tAll CSV files renamed by date/03.14.23.0919.csv\n",
      "\tAll CSV files renamed by date/03.14.23.1219.csv\n",
      "\tAll CSV files renamed by date/03.15.23.0944.csv\n",
      "\tAll CSV files renamed by date/11.30.20.0828.csv\n",
      "\tAll CSV files renamed by date/12.06.22.1331.csv\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv\n",
      "\tDCEMS Round 2 only triage sessions/8ec8afba-8533-4915-898f-5769c1258c61.csv\n",
      "\tDCEMS Round 2 only triage sessions/91f31664-43ad-4405-a763-0d58a8afc36a.csv\n",
      "\tv.1.0/Clean 158e6365-673b-4030-8b36-6704be5996a2.csv\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv\n",
      "\tv.1.0/Clean 5fa79a8e-a2df-4bb9-b614-f3ce36a5edb0.csv\n",
      "\tv.1.0/Clean 845d87c5-7b8b-4bf3-bfc6-91c74e285243.csv\n",
      "\tv.1.0/Clean a6b7ff70-3b20-48c6-86e8-744bad19f7d7.csv\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv\n",
      "\tv.1.0/Clean d90f2d85-e91c-4f12-b070-5929d95be1c5.csv\n",
      "\tv.1.0/Clean db948ce1-783d-4dff-a1f8-2be49570f327.csv\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n",
      "\n",
      "\n",
      "These files have no user action taken:\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n",
      "\n",
      "These files have no teleportation being done:\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n",
      "\n",
      "These files have no injury treatment being done:\n",
      "\tAll CSV files renamed by date/03.14.23.0919.csv, All CSV files renamed by date/11.30.20.0828.csv, DCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv, DCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,, and\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv\n",
      "\n",
      "These files have no pulses being taken:\n",
      "\tAll CSV files renamed by date/03.14.23.1219.csv,\n",
      "\tAll CSV files renamed by date/03.15.23.0944.csv,\n",
      "\tv.1.0/Clean 5fa79a8e-a2df-4bb9-b614-f3ce36a5edb0.csv, and\n",
      "\tv.1.0/Clean a6b7ff70-3b20-48c6-86e8-744bad19f7d7.csv\n",
      "\n",
      "These files have no patient accuracy rate above zero:\n",
      "\tAll CSV files renamed by date/03.14.23.0919.csv,\n",
      "\tAll CSV files renamed by date/11.30.20.0828.csv,\n",
      "\tAll CSV files renamed by date/12.06.22.1331.csv,\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tDCEMS Round 2 only triage sessions/8ec8afba-8533-4915-898f-5769c1258c61.csv,\n",
      "\tDCEMS Round 2 only triage sessions/91f31664-43ad-4405-a763-0d58a8afc36a.csv,\n",
      "\tv.1.0/Clean 158e6365-673b-4030-8b36-6704be5996a2.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean 845d87c5-7b8b-4bf3-bfc6-91c74e285243.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv,\n",
      "\tv.1.0/Clean d90f2d85-e91c-4f12-b070-5929d95be1c5.csv,\n",
      "\tv.1.0/Clean db948ce1-783d-4dff-a1f8-2be49570f327.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n",
      "\n",
      "These files have no patients being engaged:\n",
      "\tDCEMS Round 2 only triage sessions/1066671d-2a1d-4744-b66f-e4b48548701f.csv,\n",
      "\tDCEMS Round 2 only triage sessions/54aaf31a-22bc-46f2-a810-8564161bf8d0.csv,\n",
      "\tDCEMS Round 2 only triage sessions/7c2549d4-97a4-4389-bd03-029396714f59.csv,\n",
      "\tv.1.0/Clean 158e6365-673b-4030-8b36-6704be5996a2.csv,\n",
      "\tv.1.0/Clean 2310f107-d9d2-418e-a2d7-dd7a17924544.csv,\n",
      "\tv.1.0/Clean c6a48228-d864-4b20-93dd-8ad0d78d59c0.csv,\n",
      "\tv.1.0/Clean d90f2d85-e91c-4f12-b070-5929d95be1c5.csv, and\n",
      "\tv.1.0/Clean e78faf41-7bbd-410b-8750-e4e72b951216.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nDoug,\\n\\nHere is the set of anomalous files I'm concerned about:\")\n",
    "print('\\t' + '\\n\\t'.join(sorted(anomalous_files_set)))\n",
    "print(anomalous_files_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f65f715-9c29-4fd4-82ff-437fb72d2a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# How many session UUIDs are missing from the registry?\n",
    "389 - len(registry_session_uuids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7fdba40-8c9a-4d64-9f5d-a6d661ac3218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(anomalous_files_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfe249-8001-4e01-8b0b-c37841da7302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
